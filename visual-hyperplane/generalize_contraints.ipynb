{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We verify that MLP can generalize symbolic constraints\n",
    "- Sum of 25\n",
    "- Increasing\n",
    "- Symmetric\n",
    "- Odd/Even (weird?) or Dividable by something\n",
    "\n",
    "Real is much easier.\n",
    "For symbolic, it would not be much harder if internally it learns embeddings for each number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "3% of training data is too low -> generalization reaches 90% accuracy max (with embedding scheme). but 100% with proper normalization.\n",
    "10% of training data is enough -> generalization reaches 100% accuracy.\n",
    "\n",
    "Weirdly, real representation is much slower to learn with than onehot representation\n",
    "-> was a normalization issue, rescaling all reals to [0;1] solves the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives 200\n",
      "Negatives 9799\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from coins import generate_combinations\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def split(iterable, train_ratio=0.1, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(iterable)\n",
    "    n = int(len(iterable)*train_ratio)\n",
    "    train = iterable[:n]\n",
    "    test = iterable[n:]\n",
    "    return train, test\n",
    "\n",
    "def to_onehot(x):\n",
    "    x_flat = x.flatten()\n",
    "    x_onehot = np.zeros((len(x_flat), 10), dtype=np.float32)\n",
    "    x_onehot[range(len(x_onehot)), x_flat] = 1.\n",
    "    return x_onehot.reshape(x.shape[0], x.shape[1], 10)\n",
    "\n",
    "#ONSTRAINT = 'sum_25'\n",
    "CONSTRAINT = 'increasing'\n",
    "#NCODING_MODE = 'onehot'\n",
    "#ENCODING_MODE = 'embedding'\n",
    "ENCODING_MODE = 'real'\n",
    "\n",
    "uniform = np.asarray(list(product(range(10),range(10),range(10),range(10),range(10))))\n",
    "if CONSTRAINT == 'sum_25':\n",
    "    combinations = np.asarray(generate_combinations(25, range(10), 5))\n",
    "    positive = combinations\n",
    "    combinations_set = set(map(tuple, combinations))\n",
    "    non_combinations = np.asarray([c for c in uniform if tuple(c) not in combinations_set])\n",
    "    negative = non_combinations\n",
    "elif CONSTRAINT == 'increasing':\n",
    "    mask = np.all(np.diff(uniform, 1) >= 0, 1)\n",
    "    positive = uniform[mask]\n",
    "    negative = uniform[~mask]\n",
    "if ENCODING_MODE == 'real':\n",
    "    positive = positive.astype(np.float32) / 10. # respect range\n",
    "    negative = negative.astype(np.float32) / 10.\n",
    "else: # positive, negative must be Long before that\n",
    "    positive = to_onehot(positive)\n",
    "    negative = to_onehot(negative)\n",
    "    \n",
    "train_positive, test_positive = split(positive)\n",
    "train_negative, test_negative = split(negative)\n",
    "\n",
    "BATCH = 32\n",
    "\n",
    "def make_infinite(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "            \n",
    "            \n",
    "train_positive_iter = make_infinite(DataLoader(train_positive, batch_size=BATCH, shuffle=True))\n",
    "test_positive_iter = make_infinite(DataLoader(test_positive, batch_size=BATCH, shuffle=True))\n",
    "train_negative_iter = make_infinite(DataLoader(train_negative, batch_size=BATCH, shuffle=True))\n",
    "test_negative_iter = make_infinite(DataLoader(test_negative, batch_size=BATCH, shuffle=True))\n",
    "\n",
    "print 'Positives', len(train_positive)\n",
    "print 'Negatives', len(train_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Train 0.6857330799102783\n",
      "Test  0.6738572120666504\n",
      "Test Accuracy  0.796875\n",
      "Iteration 500\n",
      "Train 0.08287315875291824\n",
      "Test  0.09380870126187801\n",
      "Test Accuracy  0.9708749997615814\n",
      "Iteration 1000\n",
      "Train 0.05011832640506327\n",
      "Test  0.07946214739233255\n",
      "Test Accuracy  0.9721875\n",
      "Iteration 1500\n",
      "Train 0.036228066058829424\n",
      "Test  0.06890604943037033\n",
      "Test Accuracy  0.975\n",
      "Iteration 2000\n",
      "Train 0.03201420086901635\n",
      "Test  0.052420592317357656\n",
      "Test Accuracy  0.9815625\n",
      "Iteration 2500\n",
      "Train 0.020497090311255305\n",
      "Test  0.05273683045059443\n",
      "Test Accuracy  0.9821875\n",
      "Iteration 3000\n",
      "Train 0.018519691927358507\n",
      "Test  0.053652171096764505\n",
      "Test Accuracy  0.980625\n",
      "Iteration 3500\n",
      "Train 0.01349942522123456\n",
      "Test  0.036183242960833015\n",
      "Test Accuracy  0.9896875\n",
      "Iteration 4000\n",
      "Train 0.011604854034958407\n",
      "Test  0.04919256038032472\n",
      "Test Accuracy  0.9840625\n",
      "Iteration 4500\n",
      "Train 0.01013363469421165\n",
      "Test  0.055939925937709634\n",
      "Test Accuracy  0.981875\n",
      "Iteration 5000\n",
      "Train 0.005791885050712153\n",
      "Test  0.042234854553826154\n",
      "Test Accuracy  0.986875\n",
      "Iteration 5500\n",
      "Train 0.0046226125856628645\n",
      "Test  0.03745337927946821\n",
      "Test Accuracy  0.9896875\n",
      "Iteration 6000\n",
      "Train 0.0027399133509607054\n",
      "Test  0.024600340895121916\n",
      "Test Accuracy  0.9903125\n",
      "Iteration 6500\n",
      "Train 0.0012316065370396245\n",
      "Test  0.024826850838726388\n",
      "Test Accuracy  0.9934375\n",
      "Iteration 7000\n",
      "Train 0.0064021408001281085\n",
      "Test  0.02429637732260744\n",
      "Test Accuracy  0.9934375\n",
      "Iteration 7500\n",
      "Train 0.004411242134865461\n",
      "Test  0.03301452056723065\n",
      "Test Accuracy  0.991875\n",
      "Iteration 8000\n",
      "Train 0.00040313136028999\n",
      "Test  0.016117073358618655\n",
      "Test Accuracy  0.995\n",
      "Iteration 8500\n",
      "Train 0.00045962088232045064\n",
      "Test  0.020351451655733398\n",
      "Test Accuracy  0.99375\n",
      "Iteration 9000\n",
      "Train 0.0006524816314959026\n",
      "Test  0.020053252821817295\n",
      "Test Accuracy  0.9946875\n",
      "Iteration 9500\n",
      "Train 0.0001581967168021947\n",
      "Test  0.01640464225638425\n",
      "Test Accuracy  0.994375\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    EMBEDDING = 'embedding' # share embeddings between same numbers\n",
    "    ONEHOT = 'onehot'  # one hot without embedding sharing\n",
    "    REAL = 'real'\n",
    "    def __init__(self, mode, dims=100):\n",
    "        nn.Module.__init__(self)\n",
    "        self.mode = mode\n",
    "        if mode == self.ONEHOT:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(5*10, dims),  # 50 embeddings of dimension dims (digits*position) -> are all added together\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "        elif mode == self.REAL:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(5, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "        elif mode == self.EMBEDDING:\n",
    "            self.embedder = nn.Linear(10, dims) # 10 embeddings of dimension dims (position) -> concatenated together\n",
    "            self.main = nn.Sequential(  # ReLU is not needed\n",
    "                nn.Linear(5*dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if self.mode == self.REAL:\n",
    "            return self.main(x)\n",
    "        elif self.mode == self.ONEHOT:\n",
    "            x = x.view(len(x), -1)\n",
    "            return self.main(x)\n",
    "        elif self.mode == self.EMBEDDING:\n",
    "            out = x.view(-1, 10)\n",
    "            # embed\n",
    "            out = self.embedder(out)\n",
    "            out = out.view(len(x), -1)\n",
    "            return self.main(out)\n",
    "     \n",
    "model = Model(mode=ENCODING_MODE)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "def get_loss(model, positive, negative):\n",
    "    positive_pred = model(positive)\n",
    "    negative_pred = model(negative)    \n",
    "    positive_target = torch.ones(len(positive), dtype=torch.long)\n",
    "    negative_target = torch.zeros(len(negative), dtype=torch.long)\n",
    "    \n",
    "    positive_loss = criterion(positive_pred, positive_target)\n",
    "    negative_loss = criterion(negative_pred, negative_target)\n",
    "    \n",
    "    positive_accuracy = (positive_pred.argmax(1) == positive_target).float().mean()\n",
    "    negative_accuracy = (negative_pred.argmax(1) == negative_target).float().mean()\n",
    "    \n",
    "    loss = 0.5 * (positive_loss + negative_loss)\n",
    "    accuracy = 0.5 * (positive_accuracy + negative_accuracy)\n",
    "    \n",
    "    # Accuracy\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "for iteration in xrange(10000):\n",
    "    # train\n",
    "    positive = train_positive_iter.next()\n",
    "    negative = train_negative_iter.next()\n",
    "    loss, accuracy = get_loss(model, positive, negative)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # test\n",
    "    test_positive = test_positive_iter.next().float()\n",
    "    test_negative = test_negative_iter.next().float()    \n",
    "    test_loss, test_accuracy = get_loss(model, test_positive, test_negative)\n",
    "  \n",
    "    losses.append(loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    test_accuracies.append(test_accuracy.item())\n",
    "    if iteration % 500 == 0:\n",
    "        print 'Iteration', iteration\n",
    "        print 'Train', np.mean(losses[-min(len(losses), 50):])\n",
    "        print 'Test ', np.mean(test_losses[-min(len(losses), 50):])\n",
    "        print 'Test Accuracy ', np.mean(test_accuracies[-min(len(losses), 50):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 10])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
