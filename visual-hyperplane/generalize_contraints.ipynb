{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We verify that MLP can generalize symbolic constraints\n",
    "- Sum of 25\n",
    "- Increasing\n",
    "- Symmetric\n",
    "- Odd/Even (weird?) or Dividable by something\n",
    "\n",
    "Real is much easier.\n",
    "For symbolic, it would not be much harder if internally it learns embeddings for each number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "3% of training data is too low -> generalization reaches 90% accuracy max (with embedding scheme). but 100% with proper normalization.\n",
    "10% of training data is enough -> generalization reaches 100% accuracy.\n",
    "\n",
    "Weirdly, real representation is much slower to learn with than onehot representation\n",
    "-> was a normalization issue, rescaling all reals to [0;1] solves the problem\n",
    "\n",
    "Ask expected \"EVEN\" XOR function is very hard to learn with real representation.\n",
    "Surprisingly easy to learn with one-hot and embedding representations.\n",
    "\n",
    "An interesting question is whether a form of curriculum could help a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from coins import generate_combinations\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def split(iterable, train_ratio=0.1, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(iterable)\n",
    "    n = int(len(iterable)*train_ratio)\n",
    "    train = iterable[:n]\n",
    "    test = iterable[n:]\n",
    "    return train, test\n",
    "\n",
    "def to_onehot(x):\n",
    "    x_flat = x.flatten()\n",
    "    x_onehot = np.zeros((len(x_flat), 10), dtype=np.float32)\n",
    "    x_onehot[range(len(x_onehot)), x_flat] = 1.\n",
    "    return x_onehot.reshape(x.shape[0], x.shape[1], 10)\n",
    "\n",
    "CONSTRAINTS = [\n",
    "    'sum_25',\n",
    "    'increasing',\n",
    "    'symmetric',\n",
    "    'even'\n",
    "]\n",
    "ENCODING_MODES = [\n",
    "    'onehot',\n",
    "    'embedding',\n",
    "    'real'    \n",
    "]\n",
    "#CONSTRAINT = 'sum_25'\n",
    "#CONSTRAINT = 'increasing'\n",
    "#CONSTRAINT = 'symmetric'\n",
    "CONSTRAINT = 'even'\n",
    "#ENCODING_MODE = 'onehot'\n",
    "#ENCODING_MODE = 'embedding'\n",
    "ENCODING_MODE = 'real'\n",
    "BATCH = 32\n",
    "\n",
    "class Problem(object): \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "         'Problem \"{}\":[{}]'.format(self.constraint, self.encoding_mode),\n",
    "         '  train+: {}'.format(len(self.train_positive)),\n",
    "         '  train-: {}'.format(len(self.train_negative)),\n",
    "         '  test+: {}'.format(len(self.test_positive)),\n",
    "         '  test-: {}'.format(len(self.test_negative))])\n",
    "        \n",
    "class Summary(object): pass\n",
    "\n",
    "def make_infinite(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "            \n",
    "def get_problem(CONSTRAINT, ENCODING_MODE):\n",
    "    p = Problem()\n",
    "\n",
    "    uniform = np.asarray(list(product(range(10),range(10),range(10),range(10),range(10))))\n",
    "    if CONSTRAINT == 'sum_25':\n",
    "        combinations = np.asarray(generate_combinations(25, range(10), 5))\n",
    "        positive = combinations\n",
    "        combinations_set = set(map(tuple, combinations))\n",
    "        non_combinations = np.asarray([c for c in uniform if tuple(c) not in combinations_set])\n",
    "        negative = non_combinations\n",
    "    elif CONSTRAINT == 'increasing':\n",
    "        mask = np.all(np.diff(uniform, 1) >= 0, 1)\n",
    "        positive = uniform[mask]\n",
    "        negative = uniform[~mask]\n",
    "    elif CONSTRAINT == 'symmetric':\n",
    "        mask = np.all(uniform[:,:2] == uniform[:,:2:-1], 1)\n",
    "        positive = uniform[mask]\n",
    "        negative = uniform[~mask]\n",
    "    elif CONSTRAINT == 'even':\n",
    "        mask = (uniform.sum(1)%2==0)\n",
    "        positive = uniform[mask]\n",
    "        negative = uniform[~mask]\n",
    "    if ENCODING_MODE == 'real':\n",
    "        positive = positive.astype(np.float32) / 10. # respect range\n",
    "        negative = negative.astype(np.float32) / 10.\n",
    "    else: # positive, negative must be Long before that\n",
    "        positive = to_onehot(positive)\n",
    "        negative = to_onehot(negative)\n",
    "        \n",
    "    p.constraint = CONSTRAINT\n",
    "    p.encoding_mode = ENCODING_MODE\n",
    "        \n",
    "    p.positive = positive\n",
    "    p.negative = negative\n",
    "\n",
    "    p.train_positive, p.test_positive = split(positive)\n",
    "    p.train_negative, p.test_negative = split(negative)\n",
    "\n",
    "    p.train_positive_iter = make_infinite(DataLoader(p.train_positive, batch_size=BATCH, shuffle=True))\n",
    "    p.test_positive_iter = make_infinite(DataLoader(p.test_positive, batch_size=BATCH, shuffle=True))\n",
    "    p.train_negative_iter = make_infinite(DataLoader(p.train_negative, batch_size=BATCH, shuffle=True))\n",
    "    p.test_negative_iter = make_infinite(DataLoader(p.test_negative, batch_size=BATCH, shuffle=True))\n",
    "\n",
    "        \n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    EMBEDDING = 'embedding' # share embeddings between same numbers\n",
    "    ONEHOT = 'onehot'  # one hot without embedding sharing\n",
    "    REAL = 'real'\n",
    "    def __init__(self, mode, dims=100):\n",
    "        nn.Module.__init__(self)\n",
    "        self.mode = mode\n",
    "        if mode == self.ONEHOT:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(5*10, dims),  # 50 embeddings of dimension dims (digits*position) -> are all added together\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "        elif mode == self.REAL:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(5, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "        elif mode == self.EMBEDDING:\n",
    "            self.embedder = nn.Linear(10, dims) # 10 embeddings of dimension dims (position) -> concatenated together\n",
    "            self.main = nn.Sequential(  # ReLU is not needed\n",
    "                nn.Linear(5*dims, dims),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(dims, 2),\n",
    "                nn.LogSoftmax()\n",
    "            )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if self.mode == self.REAL:\n",
    "            return self.main(x)\n",
    "        elif self.mode == self.ONEHOT:\n",
    "            x = x.view(len(x), -1)\n",
    "            return self.main(x)\n",
    "        elif self.mode == self.EMBEDDING:\n",
    "            out = x.view(-1, 10)\n",
    "            # embed\n",
    "            out = self.embedder(out)\n",
    "            out = out.view(len(x), -1)\n",
    "            return self.main(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_loss(model, positive, negative):\n",
    "    positive_pred = model(positive)\n",
    "    negative_pred = model(negative)    \n",
    "    positive_target = torch.ones(len(positive), dtype=torch.long)\n",
    "    negative_target = torch.zeros(len(negative), dtype=torch.long)\n",
    "    \n",
    "    positive_loss = criterion(positive_pred, positive_target)\n",
    "    negative_loss = criterion(negative_pred, negative_target)\n",
    "    \n",
    "    positive_accuracy = (positive_pred.argmax(1) == positive_target).float().mean()\n",
    "    negative_accuracy = (negative_pred.argmax(1) == negative_target).float().mean()\n",
    "    \n",
    "    loss = 0.5 * (positive_loss + negative_loss)\n",
    "    accuracy = 0.5 * (positive_accuracy + negative_accuracy)\n",
    "    \n",
    "    # Accuracy\n",
    "    return loss, accuracy\n",
    "\n",
    "def evaluate_model(model, \n",
    "                   problem,\n",
    "                   iterations,\n",
    "                   log_every=500):\n",
    "    p = problem\n",
    "    s = Summary()\n",
    "    s.problem = problem\n",
    "    s.losses = []\n",
    "    s.test_losses = []\n",
    "    s.test_accuracies = []\n",
    "\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for iteration in xrange(iterations):\n",
    "        # train\n",
    "        positive = p.train_positive_iter.next()\n",
    "        negative = p.train_negative_iter.next()\n",
    "        loss, accuracy = get_loss(model, positive, negative)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # test\n",
    "        test_positive = p.test_positive_iter.next().float()\n",
    "        test_negative = p.test_negative_iter.next().float()    \n",
    "        test_loss, test_accuracy = get_loss(model, test_positive, test_negative)\n",
    "\n",
    "        s.losses.append(loss.item())\n",
    "        s.test_losses.append(test_loss.item())\n",
    "        s.test_accuracies.append(test_accuracy.item())\n",
    "        if log_every and iteration % log_every == 0:\n",
    "            print 'Iteration', iteration\n",
    "            print 'Train', np.mean(s.losses[-min(len(losses), 50):])\n",
    "            print 'Test ', np.mean(s.test_losses[-min(len(s.test_losses), 50):])\n",
    "            print 'Test Accuracy ', np.mean(s.test_accuracies[-min(len(s.test_accuracies), 50):])\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_accuracies = {}\n",
    "for constraint in CONSTRAINTS:\n",
    "    for encoding_mode in ENCODING_MODES:\n",
    "        problem = get_problem(constraint, encoding_mode)\n",
    "        model = Model(mode=encoding_mode)\n",
    "        print '\\nEvaluating problem', problem\n",
    "        s = evaluate_model(model, problem, iterations=4000, log_every=0)\n",
    "        test_accuracies.setdefault(constraint, {})\n",
    "        test_accuracies[constraint].setdefault(encoding_mode, {})\n",
    "        test_accuracies[constraint][encoding_mode] = np.mean(s.test_accuracies[-min(len(s.test_accuracies), 50):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'even': {'embedding': 0.48875, 'onehot': 0.6546875, 'real': 0.504375},\n",
       " 'increasing': {'embedding': 0.9509375, 'onehot': 0.92625, 'real': 0.97625},\n",
       " 'sum_25': {'embedding': 0.9984375, 'onehot': 0.998125, 'real': 0.958125},\n",
       " 'symmetric': {'embedding': 0.9728125, 'onehot': 0.924375, 'real': 0.99625}}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
