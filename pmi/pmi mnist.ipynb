{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Mutual Information\n",
    "\n",
    "We compute the generalized mutual information on the noisy-label MNIST problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference problem\n",
    "\n",
    "We define a very easy classification problem\n",
    "in order to calibrate the scale of MMD and Wasserstein divergences,\n",
    "since those divergences are not expressed in nats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/test_32x32.mat\n",
      "Count labels: [4948, 13861, 10585, 8497, 7458, 6882, 5727, 5595, 5045, 4659]\n",
      "Keeping min labels/class: 4659\n",
      "Data shape (46590, 3, 32, 32) Label shape (46590,)\n",
      "Count labels: [1744, 5099, 4149, 2882, 2523, 2384, 1977, 2019, 1660, 1595]\n",
      "Keeping min labels/class: 1595\n",
      "Data shape (15950, 3, 32, 32) Label shape (15950,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')\n",
    "parser.add_argument('--dataset', required=True, choices=['mnist', 'svhn'], default=False,\n",
    "                    help='For Saving the current Model')\n",
    "\n",
    "args = parser.parse_args(['--dataset', 'svhn'])\n",
    "#args = parser.parse_args(['--dataset', 'mnist'])\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "\n",
    "def corruption_to_mi_wrong(alpha):\n",
    "    return np.log(k) + np.log(1-(1-1./k)*alpha)\n",
    "\n",
    "\n",
    "def corruption_to_mi(alpha, k=10.):\n",
    "    p_y_true = (1-alpha) + alpha / k\n",
    "    p_y_false = alpha / k + 1e-8 # prevemnt 0 alpha error\n",
    "    entropy = np.log(k)\n",
    "    conditional_entropy = -(p_y_true * np.log(p_y_true) + (k-1) * p_y_false * np.log(p_y_false))\n",
    "    return entropy - conditional_entropy\n",
    "    \n",
    "#alpha = np.linspace(0, 1, 100)\n",
    "#k = 10\n",
    "#mi = corruption_to_mi(alpha)\n",
    "#plt.plot(alpha, mi)\n",
    "#plt.xlabel('Ratio of corrupted labels')\n",
    "#plt.ylabel('Mutual information (nats)')\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x.view(len(x), -1))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(len(x), -1)\n",
    "    \n",
    "    \n",
    "# Architecture loosely inspired by\n",
    "# https://www.kaggle.com/olgabelitskaya/svhn-digit-recognition\n",
    "class SvhnCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SvhnCNN, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 32x32 - 3 channels\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Conv2d(3, 20, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            # 16x16 - 20 channels\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.Conv2d(20, 32, 3, 1, 0),\n",
    "            nn.ReLU(True),            \n",
    "            # 14x14 - 32 channels\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 4, 2, 1),\n",
    "            nn.ReLU(True),           \n",
    "            # 7x7 - 32 channels\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 3, 1, 0),\n",
    "            nn.ReLU(True),  \n",
    "            # 5x5 - 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 0),\n",
    "            nn.ReLU(True),  \n",
    "            # 3x3 - 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 196, 3, 1, 0),\n",
    "            nn.ReLU(True),  \n",
    "            # 1x1 - 196,\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.Conv2d(196, 512, 1, 1, 0),\n",
    "            nn.ReLU(True),  \n",
    "            # 1x1 - 512 channels\n",
    "            FlattenLayer(),            \n",
    "            nn.Linear(512, 10),\n",
    "            nn.LogSoftmax(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SvhnLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SvhnLinear, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            FlattenLayer(),\n",
    "            nn.Linear(3*32*32, 10),\n",
    "            nn.LogSoftmax(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def do_epoch(args, mode, model, device, loader, optimizer=None, info=''):\n",
    "    if mode=='train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy.item())\n",
    "        \n",
    "        if mode == 'train':\n",
    "            #print 'training'\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('{} Batch {}/{} Accuracy {:.3f}\\tLoss: {:.3f}'.format(\n",
    "                info, batch_idx, len(loader), np.mean(accuracies), np.mean(losses)))\n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "\n",
    "# Reference problem\n",
    "class ReferenceDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if dataset == 'mnist':\n",
    "            X = torch.zeros(10, 784)\n",
    "            Y = torch.arange(10)\n",
    "            for i in xrange(10):\n",
    "                X[i, 78*i:78*(i+1)] = 1\n",
    "            X = X.view(len(X), 1, 28, 28)        \n",
    "\n",
    "            # Repeat X and Y 6000 times\n",
    "            X = np.repeat(X, 6000, 0)\n",
    "            Y = np.repeat(Y, 6000, 0)\n",
    "        elif dataset == 'svhn':\n",
    "            X = torch.zeros(10, 3*32*32)\n",
    "            Y = torch.arange(10)\n",
    "            for i in xrange(10):\n",
    "                X[i, 307*i:307*(i+1)] = 1\n",
    "            X = X.view(len(X), 3, 32, 32)        \n",
    "\n",
    "            # Repeat X and Y 6000 times\n",
    "            X = np.repeat(X, 6000, 0)\n",
    "            Y = np.repeat(Y, 6000, 0)            \n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx].item()\n",
    "    \n",
    "    \n",
    "class CorruptedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, corruption, classes=None):\n",
    "        self.dataset = dataset\n",
    "        self.corruption = corruption\n",
    "        self.corrupted = (np.random.uniform(size=len(dataset)) < corruption)\n",
    "        if classes is not None:\n",
    "            raise Exception('Uniform labels not supported anymore.')\n",
    "            print ('Corrupting dataset (uniform labels)')\n",
    "            self.labels = np.random.randint(0, classes, len(dataset))\n",
    "        else:\n",
    "            print ('Corrupting dataset (SHUFFLE labels, not always uniform)')\n",
    "            self.labels = np.asarray([y for x,y in self.dataset])\n",
    "        np.random.shuffle(self.labels)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        if self.corrupted[idx]:\n",
    "            y = self.labels[idx].item()\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class UniformSVHN(torch.utils.data.Dataset):\n",
    "    def __init__(self, svhn_dataset):\n",
    "        # Count number of each label\n",
    "        counts = []\n",
    "        for i in xrange(10):\n",
    "            mask = (svhn_dataset.labels==i)\n",
    "            counts.append(mask.sum())\n",
    "        min_count = np.min(counts)\n",
    "        print('Count labels: {}'.format(counts))\n",
    "        print('Keeping min labels/class: {}'.format(min_count))\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in xrange(10):\n",
    "            class_i_truncated = svhn_dataset.data[svhn_dataset.labels==i][:min_count]\n",
    "            X.append(class_i_truncated)\n",
    "            Y.append(np.full(min_count, i))\n",
    "        X = np.concatenate(X, 0)\n",
    "        Y = np.concatenate(Y, 0)\n",
    "\n",
    "        self.X = torch.FloatTensor(X) / 255.\n",
    "        self.Y = Y\n",
    "        print('Data shape {} Label shape {}'.format(X.shape, Y.shape))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "class MNISTWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.mnist_dataset[idx]\n",
    "        return x, y.item()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "#torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "if args.dataset == 'mnist':\n",
    "    train_dataset = MNISTWrapper(datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])))\n",
    "    test_dataset = MNISTWrapper(datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])))\n",
    "elif args.dataset == 'svhn':\n",
    "    nonuniform_train_dataset = datasets.SVHN('../data', split='train', download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))\n",
    "    nonuniform_test_dataset = datasets.SVHN('../data', split='test', download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))\n",
    "    train_dataset = UniformSVHN(nonuniform_train_dataset)\n",
    "    test_dataset = UniformSVHN(nonuniform_test_dataset)\n",
    "    \n",
    "corruptions = np.linspace(0, 1, 5)\n",
    "\n",
    "\n",
    "def make_infinite(iterator):\n",
    "    while True:\n",
    "        for x in iterator:\n",
    "            yield x\n",
    "            \n",
    "\n",
    "def compute_divergence(Net, corruptions, epochs=10, reference=False):\n",
    "\n",
    "    # Train Parametric\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    for corruption in corruptions:\n",
    "\n",
    "        print ('Corruption = {}'.format(corruption))\n",
    "\n",
    "        model = Net().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "        if not reference:\n",
    "            train_corrupted = CorruptedDataset(train_dataset, corruption)\n",
    "            test_corrupted = CorruptedDataset(test_dataset, corruption)\n",
    "        else:\n",
    "            train_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)\n",
    "            test_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)\n",
    "           \n",
    "        train_corrupted_loader = torch.utils.data.DataLoader(train_corrupted, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "        test_corrupted_loader = torch.utils.data.DataLoader(test_corrupted, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train_losses, train_accuracies = do_epoch(args, 'train', model, device, train_corrupted_loader, \n",
    "                                                      optimizer, info='Train Epoch {}'.format(epoch))\n",
    "\n",
    "            print ('Corruption: {}, MI: {:.2} bits / {:.2} nats'.format(corruption, corruption_to_mi(corruption), corruption_to_mi(corruption)/np.log(2)))\n",
    "\n",
    "            test_losses, test_accuracies = do_epoch(args, 'test', model, device, test_corrupted_loader, info='Test {}'.format(epoch))\n",
    "            print ('CORRUPTED {}'.format(np.mean(test_losses)))\n",
    "\n",
    "\n",
    "        all_train_losses.append(np.mean(train_losses))\n",
    "        all_test_losses.append(np.mean(test_losses))\n",
    "\n",
    "    return all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "if args.dataset == 'mnist':\n",
    "    all_train_losses, all_test_losses = compute_divergence(Net, corruptions)\n",
    "elif args.dataset == 'svhn':\n",
    "    all_train_losses, all_test_losses = compute_divergence(SvhnCNN, corruptions)\n",
    "    \n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_real_cnn.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption = 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/728 Accuracy 0.094\tLoss: 2.367\n",
      "Train Epoch 1 Batch 100/728 Accuracy 0.108\tLoss: 2.357\n",
      "Train Epoch 1 Batch 200/728 Accuracy 0.108\tLoss: 2.359\n",
      "Train Epoch 1 Batch 300/728 Accuracy 0.110\tLoss: 2.359\n",
      "Train Epoch 1 Batch 400/728 Accuracy 0.112\tLoss: 2.354\n",
      "Train Epoch 1 Batch 500/728 Accuracy 0.116\tLoss: 2.349\n",
      "Train Epoch 1 Batch 600/728 Accuracy 0.119\tLoss: 2.346\n",
      "Train Epoch 1 Batch 700/728 Accuracy 0.121\tLoss: 2.342\n",
      "Corruption: 0.0, MI: 2.3 bits / 3.3 nats\n",
      "Test 1 Batch 0/16 Accuracy 0.111\tLoss: 2.323\n",
      "CORRUPTED 2.31820896268\n",
      "Corruption = 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/728 Accuracy 0.078\tLoss: 2.378\n",
      "Train Epoch 1 Batch 100/728 Accuracy 0.101\tLoss: 2.367\n",
      "Train Epoch 1 Batch 200/728 Accuracy 0.104\tLoss: 2.362\n",
      "Train Epoch 1 Batch 300/728 Accuracy 0.104\tLoss: 2.360\n",
      "Train Epoch 1 Batch 400/728 Accuracy 0.105\tLoss: 2.357\n",
      "Train Epoch 1 Batch 500/728 Accuracy 0.107\tLoss: 2.356\n",
      "Train Epoch 1 Batch 600/728 Accuracy 0.109\tLoss: 2.354\n",
      "Train Epoch 1 Batch 700/728 Accuracy 0.110\tLoss: 2.352\n",
      "Corruption: 0.25, MI: 1.3 bits / 1.8 nats\n",
      "Test 1 Batch 0/16 Accuracy 0.116\tLoss: 2.429\n",
      "CORRUPTED 2.43945723772\n",
      "Corruption = 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/728 Accuracy 0.062\tLoss: 2.352\n",
      "Train Epoch 1 Batch 100/728 Accuracy 0.104\tLoss: 2.374\n",
      "Train Epoch 1 Batch 200/728 Accuracy 0.101\tLoss: 2.370\n",
      "Train Epoch 1 Batch 300/728 Accuracy 0.103\tLoss: 2.369\n",
      "Train Epoch 1 Batch 400/728 Accuracy 0.104\tLoss: 2.367\n",
      "Train Epoch 1 Batch 500/728 Accuracy 0.105\tLoss: 2.366\n",
      "Train Epoch 1 Batch 600/728 Accuracy 0.105\tLoss: 2.365\n",
      "Train Epoch 1 Batch 700/728 Accuracy 0.105\tLoss: 2.365\n",
      "Corruption: 0.5, MI: 0.63 bits / 0.9 nats\n",
      "Test 1 Batch 0/16 Accuracy 0.103\tLoss: 2.459\n",
      "CORRUPTED 2.45286087692\n",
      "Corruption = 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/728 Accuracy 0.078\tLoss: 2.314\n",
      "Train Epoch 1 Batch 100/728 Accuracy 0.101\tLoss: 2.373\n",
      "Train Epoch 1 Batch 200/728 Accuracy 0.103\tLoss: 2.372\n",
      "Train Epoch 1 Batch 300/728 Accuracy 0.103\tLoss: 2.372\n",
      "Train Epoch 1 Batch 400/728 Accuracy 0.102\tLoss: 2.372\n",
      "Train Epoch 1 Batch 500/728 Accuracy 0.102\tLoss: 2.371\n",
      "Train Epoch 1 Batch 600/728 Accuracy 0.101\tLoss: 2.371\n",
      "Train Epoch 1 Batch 700/728 Accuracy 0.101\tLoss: 2.372\n",
      "Corruption: 0.75, MI: 0.19 bits / 0.27 nats\n",
      "Test 1 Batch 0/16 Accuracy 0.095\tLoss: 2.406\n",
      "CORRUPTED 2.39928868413\n",
      "Corruption = 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/728 Accuracy 0.203\tLoss: 2.280\n",
      "Train Epoch 1 Batch 100/728 Accuracy 0.102\tLoss: 2.371\n",
      "Train Epoch 1 Batch 200/728 Accuracy 0.098\tLoss: 2.372\n",
      "Train Epoch 1 Batch 300/728 Accuracy 0.097\tLoss: 2.372\n",
      "Train Epoch 1 Batch 400/728 Accuracy 0.097\tLoss: 2.372\n",
      "Train Epoch 1 Batch 500/728 Accuracy 0.097\tLoss: 2.371\n",
      "Train Epoch 1 Batch 600/728 Accuracy 0.098\tLoss: 2.370\n",
      "Train Epoch 1 Batch 700/728 Accuracy 0.098\tLoss: 2.370\n",
      "Corruption: 1.0, MI: -1.2e-07 bits / -1.7e-07 nats\n",
      "Test 1 Batch 0/16 Accuracy 0.096\tLoss: 2.393\n",
      "CORRUPTED 2.38246124983\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == 'mnist':\n",
    "    all_train_losses, all_test_losses = compute_divergence(LinearNet, corruptions, epochs=1)\n",
    "elif args.dataset == 'svhn':\n",
    "    all_train_losses, all_test_losses = compute_divergence(SvhnLinear, corruptions, epochs=1)\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_real_logistic.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "if args.dataset == 'mnist':\n",
    "    all_train_losses, all_test_losses = compute_divergence(Net, corruptions, reference=True)\n",
    "elif args.dataset == 'svhn':\n",
    "    all_train_losses, all_test_losses = compute_divergence(SvhnCNN, corruptions, reference=True)\n",
    "    \n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_cnn.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption = 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/938 Accuracy 0.188\tLoss: 2.343\n",
      "Train Epoch 1 Batch 100/938 Accuracy 0.987\tLoss: 0.143\n",
      "Train Epoch 1 Batch 200/938 Accuracy 0.993\tLoss: 0.077\n",
      "Train Epoch 1 Batch 300/938 Accuracy 0.996\tLoss: 0.053\n",
      "Train Epoch 1 Batch 400/938 Accuracy 0.997\tLoss: 0.041\n",
      "Train Epoch 1 Batch 500/938 Accuracy 0.997\tLoss: 0.034\n",
      "Train Epoch 1 Batch 600/938 Accuracy 0.998\tLoss: 0.028\n",
      "Train Epoch 1 Batch 700/938 Accuracy 0.998\tLoss: 0.025\n",
      "Train Epoch 1 Batch 800/938 Accuracy 0.998\tLoss: 0.022\n",
      "Train Epoch 1 Batch 900/938 Accuracy 0.999\tLoss: 0.020\n",
      "Corruption: 0.0, MI: 2.3 bits / 3.3 nats\n",
      "Test 1 Batch 0/60 Accuracy 1.000\tLoss: 0.002\n",
      "CORRUPTED 0.00156493186563\n",
      "Corruption = 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/938 Accuracy 0.188\tLoss: 2.314\n",
      "Train Epoch 1 Batch 100/938 Accuracy 0.773\tLoss: 1.062\n",
      "Train Epoch 1 Batch 200/938 Accuracy 0.772\tLoss: 1.054\n",
      "Train Epoch 1 Batch 300/938 Accuracy 0.774\tLoss: 1.045\n",
      "Train Epoch 1 Batch 400/938 Accuracy 0.774\tLoss: 1.045\n",
      "Train Epoch 1 Batch 500/938 Accuracy 0.774\tLoss: 1.045\n",
      "Train Epoch 1 Batch 600/938 Accuracy 0.774\tLoss: 1.043\n",
      "Train Epoch 1 Batch 700/938 Accuracy 0.773\tLoss: 1.046\n",
      "Train Epoch 1 Batch 800/938 Accuracy 0.774\tLoss: 1.045\n",
      "Train Epoch 1 Batch 900/938 Accuracy 0.773\tLoss: 1.046\n",
      "Corruption: 0.25, MI: 1.3 bits / 1.8 nats\n",
      "Test 1 Batch 0/60 Accuracy 0.763\tLoss: 1.077\n",
      "CORRUPTED 1.03796372016\n",
      "Corruption = 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/938 Accuracy 0.047\tLoss: 2.359\n",
      "Train Epoch 1 Batch 100/938 Accuracy 0.531\tLoss: 1.736\n",
      "Train Epoch 1 Batch 200/938 Accuracy 0.544\tLoss: 1.707\n",
      "Train Epoch 1 Batch 300/938 Accuracy 0.550\tLoss: 1.693\n",
      "Train Epoch 1 Batch 400/938 Accuracy 0.550\tLoss: 1.694\n",
      "Train Epoch 1 Batch 500/938 Accuracy 0.550\tLoss: 1.694\n",
      "Train Epoch 1 Batch 600/938 Accuracy 0.549\tLoss: 1.695\n",
      "Train Epoch 1 Batch 700/938 Accuracy 0.549\tLoss: 1.695\n",
      "Train Epoch 1 Batch 800/938 Accuracy 0.548\tLoss: 1.698\n",
      "Train Epoch 1 Batch 900/938 Accuracy 0.548\tLoss: 1.699\n",
      "Corruption: 0.5, MI: 0.63 bits / 0.9 nats\n",
      "Test 1 Batch 0/60 Accuracy 0.560\tLoss: 1.682\n",
      "CORRUPTED 1.69537525773\n",
      "Corruption = 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/938 Accuracy 0.172\tLoss: 2.299\n",
      "Train Epoch 1 Batch 100/938 Accuracy 0.314\tLoss: 2.153\n",
      "Train Epoch 1 Batch 200/938 Accuracy 0.315\tLoss: 2.150\n",
      "Train Epoch 1 Batch 300/938 Accuracy 0.317\tLoss: 2.147\n",
      "Train Epoch 1 Batch 400/938 Accuracy 0.319\tLoss: 2.144\n",
      "Train Epoch 1 Batch 500/938 Accuracy 0.318\tLoss: 2.145\n",
      "Train Epoch 1 Batch 600/938 Accuracy 0.318\tLoss: 2.145\n",
      "Train Epoch 1 Batch 700/938 Accuracy 0.319\tLoss: 2.144\n",
      "Train Epoch 1 Batch 800/938 Accuracy 0.320\tLoss: 2.142\n",
      "Train Epoch 1 Batch 900/938 Accuracy 0.320\tLoss: 2.142\n",
      "Corruption: 0.75, MI: 0.19 bits / 0.27 nats\n",
      "Test 1 Batch 0/60 Accuracy 0.309\tLoss: 2.163\n",
      "CORRUPTED 2.13930016359\n",
      "Corruption = 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Train Epoch 1 Batch 0/938 Accuracy 0.125\tLoss: 2.318\n",
      "Train Epoch 1 Batch 100/938 Accuracy 0.097\tLoss: 2.325\n",
      "Train Epoch 1 Batch 200/938 Accuracy 0.095\tLoss: 2.326\n",
      "Train Epoch 1 Batch 300/938 Accuracy 0.097\tLoss: 2.325\n",
      "Train Epoch 1 Batch 400/938 Accuracy 0.097\tLoss: 2.325\n",
      "Train Epoch 1 Batch 500/938 Accuracy 0.098\tLoss: 2.326\n",
      "Train Epoch 1 Batch 600/938 Accuracy 0.097\tLoss: 2.326\n",
      "Train Epoch 1 Batch 700/938 Accuracy 0.097\tLoss: 2.326\n",
      "Train Epoch 1 Batch 800/938 Accuracy 0.098\tLoss: 2.326\n",
      "Train Epoch 1 Batch 900/938 Accuracy 0.098\tLoss: 2.326\n",
      "Corruption: 1.0, MI: -1.2e-07 bits / -1.7e-07 nats\n",
      "Test 1 Batch 0/60 Accuracy 0.107\tLoss: 2.327\n",
      "CORRUPTED 2.3208785971\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == 'mnist':\n",
    "    all_train_losses, all_test_losses = compute_divergence(LinearNet, corruptions, epochs=1, reference=True)\n",
    "elif args.dataset == 'svhn':\n",
    "    all_train_losses, all_test_losses = compute_divergence(SvhnLinear, corruptions, epochs=1, reference=True)\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_logistic.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra and Inter-class Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts [4659, 4659, 4659, 4659, 4659, 4659, 4659, 4659, 4659, 4659]\n",
      "Variance class 0: 117.85\n",
      "Variance class 1: 124.93\n",
      "Variance class 2: 120.33\n",
      "Variance class 3: 122.17\n",
      "Variance class 4: 123.28\n",
      "Variance class 5: 118.37\n",
      "Variance class 6: 124.37\n",
      "Variance class 7: 119.22\n",
      "Variance class 8: 120.97\n",
      "Variance class 9: 118.03\n",
      "Inter-class variance: 121.07\n"
     ]
    }
   ],
   "source": [
    "# Compute MNIST/SVHN intra class variance and inter-class variance\n",
    "X = []\n",
    "for i in xrange(10):\n",
    "    X.append([])\n",
    "for x, y in train_dataset:\n",
    "    X[y].append(x)\n",
    "for i in xrange(10):\n",
    "    X[i] = torch.cat(X[i]).view(len(X[i]), -1)\n",
    "counts = [len(xx) for xx in X]\n",
    "print ('Class counts', counts)\n",
    "for i in xrange(10):\n",
    "    mean = X[i].mean(0)\n",
    "    var = ((X[i]-mean)**2).sum(1).mean()\n",
    "    print ('Variance class {}: {:.2f}'.format(i, var))\n",
    "    \n",
    "all_X = torch.cat(X)\n",
    "mean = all_X.mean(0)\n",
    "var = ((all_X-mean)**2).sum(1).mean()\n",
    "print ('Inter-class variance: {:.2f}'.format(var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance class 0: 0.00\n",
      "Variance class 1: 0.00\n",
      "Variance class 2: 0.00\n",
      "Variance class 3: 0.00\n",
      "Variance class 4: 0.00\n",
      "Variance class 5: 0.00\n",
      "Variance class 6: 0.00\n",
      "Variance class 7: 0.00\n",
      "Variance class 8: 0.00\n",
      "Variance class 9: 0.00\n",
      "Inter-class variance: 276.31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Reference intra class variance and inter-class variance\n",
    "X = []\n",
    "for i in xrange(10):\n",
    "    X.append([])\n",
    "for x, y in ReferenceDataset(args.dataset):\n",
    "    X[y].append(x)\n",
    "for i in xrange(10):\n",
    "    X[i] = torch.cat(X[i]).view(len(X[i]), -1)\n",
    "    \n",
    "for i in xrange(10):\n",
    "    mean = X[i].mean(0)\n",
    "    var = ((X[i]-mean)**2).sum(1).mean()\n",
    "    print ('Variance class {}: {:.2f}'.format(i, var))\n",
    "    \n",
    "all_X = torch.cat(X)\n",
    "mean = all_X.mean(0)\n",
    "var = ((all_X-mean)**2).sum(1).mean()\n",
    "print ('Inter-class variance: {:.2f}'.format(var))\n",
    "\n",
    "if args.dataset == 'mnist':\n",
    "    plt.imshow(X[5].mean(0).view(28,28))\n",
    "elif  args.dataset == 'svhn':\n",
    "    plt.imshow(X[5].mean(0).view(3, 32, 32).numpy().transpose(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD\n",
    "class MMDKernel(object):\n",
    "    def dot(self, x1, x2):\n",
    "        raise Exception('Implement')\n",
    "\n",
    "    def h(self, X1, Y1, X2, Y2):\n",
    "        k_X1_X2 = self.dot(X1, X2)\n",
    "        k_Y1_Y2 = self.dot(Y1, Y2)\n",
    "        k_X1_Y2 = self.dot(X1, Y2)\n",
    "        k_X2_Y1 = self.dot(X2, Y1)\n",
    "        stat =  k_X1_X2 + k_Y1_Y2 - k_X1_Y2 - k_X2_Y1\n",
    "        return stat\n",
    "    \n",
    "    def split(self, X):\n",
    "        return X[0::2], X[1::2]\n",
    "    \n",
    "        \n",
    "class GaussianKernel(MMDKernel):\n",
    "    def __init__(self, sigma=1.):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def dot(self, x1, x2):\n",
    "        x1 = x1.view(len(x1), -1)  # flatten\n",
    "        x2 = x2.view(len(x2), -1)  # flatten\n",
    "        \n",
    "        delta = x1-x2\n",
    "        dot = (delta*delta).mean(1)\n",
    "        return np.exp(-dot / self.sigma**2)\n",
    "\n",
    "\n",
    "class ZeroOneKernel(MMDKernel):\n",
    "    def dot(self, y1, y2):\n",
    "        # Return 1 or less (shouldn't be zero)\n",
    "        return np.exp(-(y1 != y2).float())\n",
    "\n",
    "    \n",
    "    \n",
    "class JointKernel(MMDKernel):\n",
    "    def __init__(self, kx, ky):\n",
    "        self.kx = kx\n",
    "        self.ky = ky\n",
    "        \n",
    "    def dot(self, (x1, y1), (x2, y2)):\n",
    "        dotx = self.kx.dot(x1, x2)\n",
    "        doty = self.ky.dot(y1, y2)\n",
    "        return dotx * doty\n",
    "    \n",
    "    def split(self, (X_image, X_label)):\n",
    "        X1 = (X_image[0::2], X_label[0::2])\n",
    "        X2 = (X_image[1::2], X_label[1::2])\n",
    "        return X1, X2\n",
    "        \n",
    "# Here X~p and Y~q\n",
    "def linear_mmd(kernel, X, Y):\n",
    "    # linear time unbiased estimator\n",
    "    # h((xi,yi), (xj, yj)) = k(xi,xj)+k(yi,yj)-k(xi,yj)-k(xj,yi)\n",
    "    assert len(X)==len(Y)\n",
    "\n",
    "    X1, X2 = kernel.split(X)\n",
    "    Y1, Y2 = kernel.split(Y)\n",
    "\n",
    "    hs = kernel.h(X1, Y1, X2, Y2) \n",
    "    mmd2 = hs.mean()\n",
    "    \n",
    "    return mmd2\n",
    "    \n",
    "    \n",
    "def linear_mmd_validation(kernel, X_train, Y_train, X_val, Y_val):\n",
    "    '''\n",
    "    this should return a lower bound on MMD,\n",
    "    it takes the witness function computed on the training empirical distributions X_train, Y_train\n",
    "    and applies it on the validation empirical distributions X_val, Y_val\n",
    "    '''\n",
    "    assert len(X_train)==len(Y_train)\n",
    "    assert len(X_val)==len(Y_val)\n",
    "    assert len(X_train)==len(X_val)\n",
    "    \n",
    "    hs = kernel.h(X_train, Y_train, X_val, Y_val) \n",
    "    mmd2 = hs.mean()\n",
    "    \n",
    "    return mmd2\n",
    "    \n",
    "# Independence test\n",
    "\n",
    "def compute_mmd(corruptions, joint_kernel, epochs=1, reference=False, batch_size=1000):\n",
    "\n",
    "    all_mmds_train = []\n",
    "    all_mmds_test = []\n",
    "    #all_mmds_std = []\n",
    "    \n",
    "\n",
    "    for corruption in corruptions:\n",
    "\n",
    "        print ('Corruption {}'.format(corruption))\n",
    "\n",
    "        if not reference:\n",
    "            # IMPORTANT FOR SVHN : DO NOT SPECIFY CLASSES=10 BECAUSE\n",
    "            # LABEL DISTRIBUTION IS NON-UNIFORM\n",
    "            train_corrupted = CorruptedDataset(train_dataset, corruption)  # X partially dependent on Y\n",
    "            train_random = CorruptedDataset(train_dataset, 1)  # X totally independent Y\n",
    "            # for validaiton only\n",
    "            #test_corrupted = CorruptedDataset(test_dataset, corruption, classes=10)\n",
    "            #test_random = CorruptedDataset(test_dataset, 1, classes=10)\n",
    "        else:\n",
    "            train_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)  # X partially dependent on Y\n",
    "            train_random = CorruptedDataset(ReferenceDataset(args.dataset), 1)  # X totally independent Y\n",
    "            # for validation only\n",
    "            #test_corrupted = CorruptedDataset(ReferenceDataset(), corruption)\n",
    "            #test_random = CorruptedDataset(ReferenceDataset(), 1, classes=10)  # X totally independent Y\n",
    "            \n",
    "        train_corrupted_loader = torch.utils.data.DataLoader(train_corrupted, batch_size=batch_size, drop_last=True, shuffle=True, **kwargs)\n",
    "        train_random_loader = torch.utils.data.DataLoader(train_random, batch_size=batch_size, drop_last=True, shuffle=True, **kwargs)\n",
    "        # for validation MMD\n",
    "        #test_corrupted_loader = torch.utils.data.DataLoader(train_corrupted, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        #test_random_loader = torch.utils.data.DataLoader(test_random, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        #test_corrupted_iter = make_infinite(test_corrupted_loader)\n",
    "        #test_random_iter = make_infinite(test_random_loader)\n",
    "        \n",
    "        \n",
    "        linear_mmds_train = []\n",
    "        linear_mmds_val = []\n",
    "        for epoch in xrange(1, epochs+1):\n",
    "            # Can run many times to reduce variance\n",
    "            for batch_idx, (X, Y) in enumerate(zip(train_corrupted_loader, train_random_loader)):\n",
    "                \n",
    "                mmd_train = linear_mmd(joint_kernel, X, Y)\n",
    "                \n",
    "                # for validation only\n",
    "                #X_val, Y_val = test_corrupted_iter.next(), test_random_iter.next()\n",
    "                #mmd_val = linear_mmd_validation(joint_kernel, X, Y, X_val, Y_val)\n",
    "                               \n",
    "                \n",
    "                linear_mmds_train.append(mmd_train.item())\n",
    "                #linear_mmds_val.append(mmd_val.item())\n",
    "                \n",
    "                if batch_idx % 100 == 0 or batch_idx==len(train_corrupted_loader)-1:              \n",
    "                    print ('Epoch {}/{} Batch {}/{} MMD train {:.5} '.format(\n",
    "                        epoch, epochs, batch_idx, len(train_corrupted_loader), \n",
    "                        np.mean(linear_mmds_train)))\n",
    "        all_mmds_train.append([np.mean(linear_mmds_train)])\n",
    "        all_mmds_test.append([np.mean(linear_mmds_train)])  # THIS IS WRONG BUT VALIDATION MMD IS ANNOYING TO COMPUTE\n",
    "        #all_mmds_test.append([np.mean(linear_mmds_val)])\n",
    "        #all_mmds_std.append([np.std(linear_mmds)])\n",
    "\n",
    "    return all_mmds_train, all_mmds_test         \n",
    "\n",
    "MMD_Epochs = 100\n",
    "kernel_x = GaussianKernel()\n",
    "kernel_y = ZeroOneKernel()\n",
    "joint_kernel = JointKernel(kernel_x, kernel_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/46 MMD train -0.010551 \n",
      "Epoch 1/100 Batch 45/46 MMD train -0.0015497 \n",
      "Epoch 2/100 Batch 0/46 MMD train -0.0011117 \n",
      "Epoch 2/100 Batch 45/46 MMD train -0.00041555 \n",
      "Epoch 3/100 Batch 0/46 MMD train -0.00013558 \n",
      "Epoch 3/100 Batch 45/46 MMD train -0.00095186 \n",
      "Epoch 4/100 Batch 0/46 MMD train -0.0011353 \n",
      "Epoch 4/100 Batch 45/46 MMD train -0.0020143 \n",
      "Epoch 5/100 Batch 0/46 MMD train -0.0020543 \n",
      "Epoch 5/100 Batch 45/46 MMD train -0.0022217 \n",
      "Epoch 6/100 Batch 0/46 MMD train -0.0022813 \n",
      "Epoch 6/100 Batch 45/46 MMD train -0.001697 \n",
      "Epoch 7/100 Batch 0/46 MMD train -0.0017628 \n",
      "Epoch 7/100 Batch 45/46 MMD train -0.0015101 \n",
      "Epoch 8/100 Batch 0/46 MMD train -0.0014583 \n",
      "Epoch 8/100 Batch 45/46 MMD train -0.0014477 \n",
      "Epoch 9/100 Batch 0/46 MMD train -0.0014133 \n",
      "Epoch 9/100 Batch 45/46 MMD train -0.0013758 \n",
      "Epoch 10/100 Batch 0/46 MMD train -0.0013944 \n",
      "Epoch 10/100 Batch 45/46 MMD train -0.0011912 \n",
      "Epoch 11/100 Batch 0/46 MMD train -0.0011881 \n",
      "Epoch 11/100 Batch 45/46 MMD train -0.0012543 \n",
      "Epoch 12/100 Batch 0/46 MMD train -0.0012785 \n",
      "Epoch 12/100 Batch 45/46 MMD train -0.0010604 \n",
      "Epoch 13/100 Batch 0/46 MMD train -0.0010649 \n",
      "Epoch 13/100 Batch 45/46 MMD train -0.00062461 \n",
      "Epoch 14/100 Batch 0/46 MMD train -0.00062118 \n",
      "Epoch 14/100 Batch 45/46 MMD train -0.00052471 \n",
      "Epoch 15/100 Batch 0/46 MMD train -0.00053578 \n",
      "Epoch 15/100 Batch 45/46 MMD train -0.00054345 \n",
      "Epoch 16/100 Batch 0/46 MMD train -0.00051462 \n",
      "Epoch 16/100 Batch 45/46 MMD train -0.00027356 \n",
      "Epoch 17/100 Batch 0/46 MMD train -0.00024968 \n",
      "Epoch 17/100 Batch 45/46 MMD train -0.00034037 \n",
      "Epoch 18/100 Batch 0/46 MMD train -0.00032833 \n",
      "Epoch 18/100 Batch 45/46 MMD train -0.00024799 \n",
      "Epoch 19/100 Batch 0/46 MMD train -0.00023605 \n",
      "Epoch 19/100 Batch 45/46 MMD train -0.0002997 \n",
      "Epoch 20/100 Batch 0/46 MMD train -0.00028745 \n",
      "Epoch 20/100 Batch 45/46 MMD train -0.00031885 \n",
      "Epoch 21/100 Batch 0/46 MMD train -0.00030811 \n",
      "Epoch 21/100 Batch 45/46 MMD train -0.00016607 \n",
      "Epoch 22/100 Batch 0/46 MMD train -0.00014217 \n",
      "Epoch 22/100 Batch 45/46 MMD train -0.00020958 \n",
      "Epoch 23/100 Batch 0/46 MMD train -0.0002067 \n",
      "Epoch 23/100 Batch 45/46 MMD train -0.00034661 \n",
      "Epoch 24/100 Batch 0/46 MMD train -0.00031431 \n",
      "Epoch 24/100 Batch 45/46 MMD train -0.00045589 \n",
      "Epoch 25/100 Batch 0/46 MMD train -0.00046003 \n",
      "Epoch 25/100 Batch 45/46 MMD train -0.00021307 \n",
      "Epoch 26/100 Batch 0/46 MMD train -0.00020926 \n",
      "Epoch 26/100 Batch 45/46 MMD train -7.4611e-05 \n",
      "Epoch 27/100 Batch 0/46 MMD train -5.6474e-05 \n",
      "Epoch 27/100 Batch 45/46 MMD train 0.00013137 \n",
      "Epoch 28/100 Batch 0/46 MMD train 0.00013771 \n",
      "Epoch 28/100 Batch 45/46 MMD train 8.8251e-05 \n",
      "Epoch 29/100 Batch 0/46 MMD train 7.5288e-05 \n",
      "Epoch 29/100 Batch 45/46 MMD train 4.0489e-05 \n",
      "Epoch 30/100 Batch 0/46 MMD train 5.3356e-05 \n",
      "Epoch 30/100 Batch 45/46 MMD train -7.5516e-05 \n",
      "Epoch 31/100 Batch 0/46 MMD train -9.6717e-05 \n",
      "Epoch 31/100 Batch 45/46 MMD train -0.00018422 \n",
      "Epoch 32/100 Batch 0/46 MMD train -0.00018751 \n",
      "Epoch 32/100 Batch 45/46 MMD train 1.3332e-05 \n",
      "Epoch 33/100 Batch 0/46 MMD train 3.0417e-06 \n",
      "Epoch 33/100 Batch 45/46 MMD train -5.891e-05 \n",
      "Epoch 34/100 Batch 0/46 MMD train -7.1475e-05 \n",
      "Epoch 34/100 Batch 45/46 MMD train -0.00010949 \n",
      "Epoch 35/100 Batch 0/46 MMD train -0.00011951 \n",
      "Epoch 35/100 Batch 45/46 MMD train -5.6231e-05 \n",
      "Epoch 36/100 Batch 0/46 MMD train -6.5139e-05 \n",
      "Epoch 36/100 Batch 45/46 MMD train -9.7835e-05 \n",
      "Epoch 37/100 Batch 0/46 MMD train -9.7182e-05 \n",
      "Epoch 37/100 Batch 45/46 MMD train -0.00017066 \n",
      "Epoch 38/100 Batch 0/46 MMD train -0.00017949 \n",
      "Epoch 38/100 Batch 45/46 MMD train -0.00019285 \n",
      "Epoch 39/100 Batch 0/46 MMD train -0.00018018 \n",
      "Epoch 39/100 Batch 45/46 MMD train -0.0001944 \n",
      "Epoch 40/100 Batch 0/46 MMD train -0.00018194 \n",
      "Epoch 40/100 Batch 45/46 MMD train -0.00017993 \n",
      "Epoch 41/100 Batch 0/46 MMD train -0.00017843 \n",
      "Epoch 41/100 Batch 45/46 MMD train -0.00020958 \n",
      "Epoch 42/100 Batch 0/46 MMD train -0.00022231 \n",
      "Epoch 42/100 Batch 45/46 MMD train -0.00010304 \n",
      "Epoch 43/100 Batch 0/46 MMD train -0.00010272 \n",
      "Epoch 43/100 Batch 45/46 MMD train -0.00015787 \n",
      "Epoch 44/100 Batch 0/46 MMD train -0.00015998 \n",
      "Epoch 44/100 Batch 45/46 MMD train -8.5359e-05 \n",
      "Epoch 45/100 Batch 0/46 MMD train -8.8933e-05 \n",
      "Epoch 45/100 Batch 45/46 MMD train -8.3527e-05 \n",
      "Epoch 46/100 Batch 0/46 MMD train -8.065e-05 \n",
      "Epoch 46/100 Batch 45/46 MMD train -2.8767e-05 \n",
      "Epoch 47/100 Batch 0/46 MMD train -3.2837e-05 \n",
      "Epoch 47/100 Batch 45/46 MMD train 7.7556e-05 \n",
      "Epoch 48/100 Batch 0/46 MMD train 7.1761e-05 \n",
      "Epoch 48/100 Batch 45/46 MMD train -2.5002e-05 \n",
      "Epoch 49/100 Batch 0/46 MMD train -2.2352e-05 \n",
      "Epoch 49/100 Batch 45/46 MMD train -7.6904e-05 \n",
      "Epoch 50/100 Batch 0/46 MMD train -8.5385e-05 \n",
      "Epoch 50/100 Batch 45/46 MMD train -4.0789e-05 \n",
      "Epoch 51/100 Batch 0/46 MMD train -4.3709e-05 \n",
      "Epoch 51/100 Batch 45/46 MMD train 4.9423e-05 \n",
      "Epoch 52/100 Batch 0/46 MMD train 5.9516e-05 \n",
      "Epoch 52/100 Batch 45/46 MMD train 6.923e-05 \n",
      "Epoch 53/100 Batch 0/46 MMD train 7.5227e-05 \n",
      "Epoch 53/100 Batch 45/46 MMD train 9.1322e-05 \n",
      "Epoch 54/100 Batch 0/46 MMD train 0.00010312 \n",
      "Epoch 54/100 Batch 45/46 MMD train 0.00010727 \n",
      "Epoch 55/100 Batch 0/46 MMD train 0.00010373 \n",
      "Epoch 55/100 Batch 45/46 MMD train 8.6173e-05 \n",
      "Epoch 56/100 Batch 0/46 MMD train 7.9574e-05 \n",
      "Epoch 56/100 Batch 45/46 MMD train 7.7702e-05 \n",
      "Epoch 57/100 Batch 0/46 MMD train 6.8695e-05 \n",
      "Epoch 57/100 Batch 45/46 MMD train 3.084e-05 \n",
      "Epoch 58/100 Batch 0/46 MMD train 2.0544e-05 \n",
      "Epoch 58/100 Batch 45/46 MMD train 3.3802e-05 \n",
      "Epoch 59/100 Batch 0/46 MMD train 3.635e-05 \n",
      "Epoch 59/100 Batch 45/46 MMD train -3.55e-05 \n",
      "Epoch 60/100 Batch 0/46 MMD train -3.2488e-05 \n",
      "Epoch 60/100 Batch 45/46 MMD train -9.7816e-05 \n",
      "Epoch 61/100 Batch 0/46 MMD train -0.00010602 \n",
      "Epoch 61/100 Batch 45/46 MMD train -0.00010046 \n",
      "Epoch 62/100 Batch 0/46 MMD train -0.00010436 \n",
      "Epoch 62/100 Batch 45/46 MMD train -6.6126e-05 \n",
      "Epoch 63/100 Batch 0/46 MMD train -6.2136e-05 \n",
      "Epoch 63/100 Batch 45/46 MMD train -8.3416e-05 \n",
      "Epoch 64/100 Batch 0/46 MMD train -9.2188e-05 \n",
      "Epoch 64/100 Batch 45/46 MMD train -9.9979e-05 \n",
      "Epoch 65/100 Batch 0/46 MMD train -9.7094e-05 \n",
      "Epoch 65/100 Batch 45/46 MMD train -0.00012751 \n",
      "Epoch 66/100 Batch 0/46 MMD train -0.00012764 \n",
      "Epoch 66/100 Batch 45/46 MMD train -0.00011272 \n",
      "Epoch 67/100 Batch 0/46 MMD train -0.00010492 \n",
      "Epoch 67/100 Batch 45/46 MMD train -0.00013859 \n",
      "Epoch 68/100 Batch 0/46 MMD train -0.00013596 \n",
      "Epoch 68/100 Batch 45/46 MMD train -0.00014644 \n",
      "Epoch 69/100 Batch 0/46 MMD train -0.00014187 \n",
      "Epoch 69/100 Batch 45/46 MMD train -0.00011658 \n",
      "Epoch 70/100 Batch 0/46 MMD train -0.00011624 \n",
      "Epoch 70/100 Batch 45/46 MMD train -0.0001536 \n",
      "Epoch 71/100 Batch 0/46 MMD train -0.0001631 \n",
      "Epoch 71/100 Batch 45/46 MMD train -0.00018143 \n",
      "Epoch 72/100 Batch 0/46 MMD train -0.0001758 \n",
      "Epoch 72/100 Batch 45/46 MMD train -0.00013046 \n",
      "Epoch 73/100 Batch 0/46 MMD train -0.00013066 \n",
      "Epoch 73/100 Batch 45/46 MMD train -8.6111e-05 \n",
      "Epoch 74/100 Batch 0/46 MMD train -8.9961e-05 \n",
      "Epoch 74/100 Batch 45/46 MMD train -9.3278e-05 \n",
      "Epoch 75/100 Batch 0/46 MMD train -9.5168e-05 \n",
      "Epoch 75/100 Batch 45/46 MMD train -6.657e-05 \n",
      "Epoch 76/100 Batch 0/46 MMD train -7.1679e-05 \n",
      "Epoch 76/100 Batch 45/46 MMD train -6.2581e-05 \n",
      "Epoch 77/100 Batch 0/46 MMD train -5.8725e-05 \n",
      "Epoch 77/100 Batch 45/46 MMD train 1.8221e-05 \n",
      "Epoch 78/100 Batch 0/46 MMD train 8.6982e-06 \n",
      "Epoch 78/100 Batch 45/46 MMD train 3.8894e-05 \n",
      "Epoch 79/100 Batch 0/46 MMD train 4.0304e-05 \n",
      "Epoch 79/100 Batch 45/46 MMD train 2.2093e-05 \n",
      "Epoch 80/100 Batch 0/46 MMD train 2.2413e-05 \n",
      "Epoch 80/100 Batch 45/46 MMD train 9.0979e-05 \n",
      "Epoch 81/100 Batch 0/46 MMD train 9.4598e-05 \n",
      "Epoch 81/100 Batch 45/46 MMD train 7.7495e-05 \n",
      "Epoch 82/100 Batch 0/46 MMD train 7.853e-05 \n",
      "Epoch 82/100 Batch 45/46 MMD train 0.00011478 \n",
      "Epoch 83/100 Batch 0/46 MMD train 0.00011779 \n",
      "Epoch 83/100 Batch 45/46 MMD train 8.9136e-05 \n",
      "Epoch 84/100 Batch 0/46 MMD train 9.3723e-05 \n",
      "Epoch 84/100 Batch 45/46 MMD train 0.00010272 \n",
      "Epoch 85/100 Batch 0/46 MMD train 0.00010596 \n",
      "Epoch 85/100 Batch 45/46 MMD train 0.00010838 \n",
      "Epoch 86/100 Batch 0/46 MMD train 0.00010491 \n",
      "Epoch 86/100 Batch 45/46 MMD train 0.0001159 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 Batch 0/46 MMD train 0.00011329 \n",
      "Epoch 87/100 Batch 45/46 MMD train 0.00010503 \n",
      "Epoch 88/100 Batch 0/46 MMD train 0.00011221 \n",
      "Epoch 88/100 Batch 45/46 MMD train 9.5762e-05 \n",
      "Epoch 89/100 Batch 0/46 MMD train 9.067e-05 \n",
      "Epoch 89/100 Batch 45/46 MMD train 6.5918e-05 \n",
      "Epoch 90/100 Batch 0/46 MMD train 7.7914e-05 \n",
      "Epoch 90/100 Batch 45/46 MMD train 5.9416e-05 \n",
      "Epoch 91/100 Batch 0/46 MMD train 5.539e-05 \n",
      "Epoch 91/100 Batch 45/46 MMD train 6.7223e-05 \n",
      "Epoch 92/100 Batch 0/46 MMD train 6.6664e-05 \n",
      "Epoch 92/100 Batch 45/46 MMD train 2.982e-05 \n",
      "Epoch 93/100 Batch 0/46 MMD train 1.886e-05 \n",
      "Epoch 93/100 Batch 45/46 MMD train 4.4306e-06 \n",
      "Epoch 94/100 Batch 0/46 MMD train -1.6594e-06 \n",
      "Epoch 94/100 Batch 45/46 MMD train 4.6755e-05 \n",
      "Epoch 95/100 Batch 0/46 MMD train 4.3333e-05 \n",
      "Epoch 95/100 Batch 45/46 MMD train 3.9811e-05 \n",
      "Epoch 96/100 Batch 0/46 MMD train 3.6771e-05 \n",
      "Epoch 96/100 Batch 45/46 MMD train 4.9712e-05 \n",
      "Epoch 97/100 Batch 0/46 MMD train 5.3139e-05 \n",
      "Epoch 97/100 Batch 45/46 MMD train 3.5241e-05 \n",
      "Epoch 98/100 Batch 0/46 MMD train 3.751e-05 \n",
      "Epoch 98/100 Batch 45/46 MMD train 2.2269e-05 \n",
      "Epoch 99/100 Batch 0/46 MMD train 2.2839e-05 \n",
      "Epoch 99/100 Batch 45/46 MMD train 1.9223e-06 \n",
      "Epoch 100/100 Batch 0/46 MMD train 3.0426e-07 \n",
      "Epoch 100/100 Batch 45/46 MMD train 9.9588e-07 \n",
      "Corruption 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/46 MMD train 0.018253 \n",
      "Epoch 1/100 Batch 45/46 MMD train -0.0040519 \n",
      "Epoch 2/100 Batch 0/46 MMD train -0.0040205 \n",
      "Epoch 2/100 Batch 45/46 MMD train -0.0012973 \n",
      "Epoch 3/100 Batch 0/46 MMD train -0.0015589 \n",
      "Epoch 3/100 Batch 45/46 MMD train 6.1018e-05 \n",
      "Epoch 4/100 Batch 0/46 MMD train 0.00019654 \n",
      "Epoch 4/100 Batch 45/46 MMD train 0.00030114 \n",
      "Epoch 5/100 Batch 0/46 MMD train 0.00035866 \n",
      "Epoch 5/100 Batch 45/46 MMD train 0.00017557 \n",
      "Epoch 6/100 Batch 0/46 MMD train 0.00017685 \n",
      "Epoch 6/100 Batch 45/46 MMD train 0.00045427 \n",
      "Epoch 7/100 Batch 0/46 MMD train 0.00045574 \n",
      "Epoch 7/100 Batch 45/46 MMD train -0.00018602 \n",
      "Epoch 8/100 Batch 0/46 MMD train -0.00010725 \n",
      "Epoch 8/100 Batch 45/46 MMD train 0.00011339 \n",
      "Epoch 9/100 Batch 0/46 MMD train 9.9493e-05 \n",
      "Epoch 9/100 Batch 45/46 MMD train -0.0005766 \n",
      "Epoch 10/100 Batch 0/46 MMD train -0.00060236 \n",
      "Epoch 10/100 Batch 45/46 MMD train -0.00056074 \n",
      "Epoch 11/100 Batch 0/46 MMD train -0.00058529 \n",
      "Epoch 11/100 Batch 45/46 MMD train -0.00058062 \n",
      "Epoch 12/100 Batch 0/46 MMD train -0.00054336 \n",
      "Epoch 12/100 Batch 45/46 MMD train -0.00069682 \n",
      "Epoch 13/100 Batch 0/46 MMD train -0.00069994 \n",
      "Epoch 13/100 Batch 45/46 MMD train -0.00080276 \n",
      "Epoch 14/100 Batch 0/46 MMD train -0.0007989 \n",
      "Epoch 14/100 Batch 45/46 MMD train -0.00078325 \n",
      "Epoch 15/100 Batch 0/46 MMD train -0.00079949 \n",
      "Epoch 15/100 Batch 45/46 MMD train -0.00047532 \n",
      "Epoch 16/100 Batch 0/46 MMD train -0.00052544 \n",
      "Epoch 16/100 Batch 45/46 MMD train -0.00035218 \n",
      "Epoch 17/100 Batch 0/46 MMD train -0.00035811 \n",
      "Epoch 17/100 Batch 45/46 MMD train -0.00035555 \n",
      "Epoch 18/100 Batch 0/46 MMD train -0.00034127 \n",
      "Epoch 18/100 Batch 45/46 MMD train -0.0003662 \n",
      "Epoch 19/100 Batch 0/46 MMD train -0.00035128 \n",
      "Epoch 19/100 Batch 45/46 MMD train -0.00017285 \n",
      "Epoch 20/100 Batch 0/46 MMD train -0.00018047 \n",
      "Epoch 20/100 Batch 45/46 MMD train -0.00032465 \n",
      "Epoch 21/100 Batch 0/46 MMD train -0.00033092 \n",
      "Epoch 21/100 Batch 45/46 MMD train -0.00047343 \n",
      "Epoch 22/100 Batch 0/46 MMD train -0.00048596 \n",
      "Epoch 22/100 Batch 45/46 MMD train -0.00035402 \n",
      "Epoch 23/100 Batch 0/46 MMD train -0.00034598 \n",
      "Epoch 23/100 Batch 45/46 MMD train -0.00026067 \n",
      "Epoch 24/100 Batch 0/46 MMD train -0.00027578 \n",
      "Epoch 24/100 Batch 45/46 MMD train -0.00023329 \n",
      "Epoch 25/100 Batch 0/46 MMD train -0.00023001 \n",
      "Epoch 25/100 Batch 45/46 MMD train -0.00018152 \n",
      "Epoch 26/100 Batch 0/46 MMD train -0.00017741 \n",
      "Epoch 26/100 Batch 45/46 MMD train -0.00020593 \n",
      "Epoch 27/100 Batch 0/46 MMD train -0.0001996 \n",
      "Epoch 27/100 Batch 45/46 MMD train -0.0002272 \n",
      "Epoch 28/100 Batch 0/46 MMD train -0.00021268 \n",
      "Epoch 28/100 Batch 45/46 MMD train -0.00025841 \n",
      "Epoch 29/100 Batch 0/46 MMD train -0.00022874 \n",
      "Epoch 29/100 Batch 45/46 MMD train -0.00014559 \n",
      "Epoch 30/100 Batch 0/46 MMD train -0.00016849 \n",
      "Epoch 30/100 Batch 45/46 MMD train -8.0592e-05 \n",
      "Epoch 31/100 Batch 0/46 MMD train -7.0978e-05 \n",
      "Epoch 31/100 Batch 45/46 MMD train -9.8022e-05 \n",
      "Epoch 32/100 Batch 0/46 MMD train -0.00011353 \n",
      "Epoch 32/100 Batch 45/46 MMD train -0.00019081 \n",
      "Epoch 33/100 Batch 0/46 MMD train -0.00020495 \n",
      "Epoch 33/100 Batch 45/46 MMD train -0.00011397 \n",
      "Epoch 34/100 Batch 0/46 MMD train -0.0001272 \n",
      "Epoch 34/100 Batch 45/46 MMD train -9.6627e-05 \n",
      "Epoch 35/100 Batch 0/46 MMD train -8.4761e-05 \n",
      "Epoch 35/100 Batch 45/46 MMD train -0.00011636 \n",
      "Epoch 36/100 Batch 0/46 MMD train -0.0001128 \n",
      "Epoch 36/100 Batch 45/46 MMD train -9.2916e-05 \n",
      "Epoch 37/100 Batch 0/46 MMD train -0.00010559 \n",
      "Epoch 37/100 Batch 45/46 MMD train -6.7537e-05 \n",
      "Epoch 38/100 Batch 0/46 MMD train -7.7548e-05 \n",
      "Epoch 38/100 Batch 45/46 MMD train -4.5329e-05 \n",
      "Epoch 39/100 Batch 0/46 MMD train -4.8798e-05 \n",
      "Epoch 39/100 Batch 45/46 MMD train -5.4398e-05 \n",
      "Epoch 40/100 Batch 0/46 MMD train -5.5099e-05 \n",
      "Epoch 40/100 Batch 45/46 MMD train 2.069e-05 \n",
      "Epoch 41/100 Batch 0/46 MMD train 1.3123e-05 \n",
      "Epoch 41/100 Batch 45/46 MMD train 4.4783e-05 \n",
      "Epoch 42/100 Batch 0/46 MMD train 5.0299e-05 \n",
      "Epoch 42/100 Batch 45/46 MMD train 2.6131e-05 \n",
      "Epoch 43/100 Batch 0/46 MMD train 3.4741e-05 \n",
      "Epoch 43/100 Batch 45/46 MMD train 7.034e-05 \n",
      "Epoch 44/100 Batch 0/46 MMD train 7.0709e-05 \n",
      "Epoch 44/100 Batch 45/46 MMD train 2.6436e-05 \n",
      "Epoch 45/100 Batch 0/46 MMD train 3.0642e-05 \n",
      "Epoch 45/100 Batch 45/46 MMD train 6.8635e-05 \n",
      "Epoch 46/100 Batch 0/46 MMD train 7.2679e-05 \n",
      "Epoch 46/100 Batch 45/46 MMD train 7.2921e-05 \n",
      "Epoch 47/100 Batch 0/46 MMD train 6.9252e-05 \n",
      "Epoch 47/100 Batch 45/46 MMD train 5.2181e-05 \n",
      "Epoch 48/100 Batch 0/46 MMD train 3.9237e-05 \n",
      "Epoch 48/100 Batch 45/46 MMD train 0.00015326 \n",
      "Epoch 49/100 Batch 0/46 MMD train 0.00016646 \n",
      "Epoch 49/100 Batch 45/46 MMD train 0.00016228 \n",
      "Epoch 50/100 Batch 0/46 MMD train 0.00016879 \n",
      "Epoch 50/100 Batch 45/46 MMD train 0.00016811 \n",
      "Epoch 51/100 Batch 0/46 MMD train 0.00017849 \n",
      "Epoch 51/100 Batch 45/46 MMD train 0.00020883 \n",
      "Epoch 52/100 Batch 0/46 MMD train 0.00021303 \n",
      "Epoch 52/100 Batch 45/46 MMD train 0.0001276 \n",
      "Epoch 53/100 Batch 0/46 MMD train 0.00012398 \n",
      "Epoch 53/100 Batch 45/46 MMD train 0.00019416 \n",
      "Epoch 54/100 Batch 0/46 MMD train 0.00019819 \n",
      "Epoch 54/100 Batch 45/46 MMD train 0.00020915 \n",
      "Epoch 55/100 Batch 0/46 MMD train 0.00021555 \n",
      "Epoch 55/100 Batch 45/46 MMD train 0.00015008 \n",
      "Epoch 56/100 Batch 0/46 MMD train 0.00016128 \n",
      "Epoch 56/100 Batch 45/46 MMD train 0.00011388 \n",
      "Epoch 57/100 Batch 0/46 MMD train 0.0001137 \n",
      "Epoch 57/100 Batch 45/46 MMD train 6.1985e-05 \n",
      "Epoch 58/100 Batch 0/46 MMD train 6.0289e-05 \n",
      "Epoch 58/100 Batch 45/46 MMD train 6.298e-05 \n",
      "Epoch 59/100 Batch 0/46 MMD train 5.3847e-05 \n",
      "Epoch 59/100 Batch 45/46 MMD train 3.8059e-06 \n",
      "Epoch 60/100 Batch 0/46 MMD train -7.5049e-06 \n",
      "Epoch 60/100 Batch 45/46 MMD train 2.5931e-05 \n",
      "Epoch 61/100 Batch 0/46 MMD train 2.292e-05 \n",
      "Epoch 61/100 Batch 45/46 MMD train 2.2849e-05 \n",
      "Epoch 62/100 Batch 0/46 MMD train 3.094e-05 \n",
      "Epoch 62/100 Batch 45/46 MMD train 4.5683e-05 \n",
      "Epoch 63/100 Batch 0/46 MMD train 4.596e-05 \n",
      "Epoch 63/100 Batch 45/46 MMD train 6.2179e-05 \n",
      "Epoch 64/100 Batch 0/46 MMD train 6.7842e-05 \n",
      "Epoch 64/100 Batch 45/46 MMD train 6.0324e-05 \n",
      "Epoch 65/100 Batch 0/46 MMD train 6.1233e-05 \n",
      "Epoch 65/100 Batch 45/46 MMD train 9.13e-05 \n",
      "Epoch 66/100 Batch 0/46 MMD train 0.00010158 \n",
      "Epoch 66/100 Batch 45/46 MMD train 0.00012912 \n",
      "Epoch 67/100 Batch 0/46 MMD train 0.00013443 \n",
      "Epoch 67/100 Batch 45/46 MMD train 0.00011806 \n",
      "Epoch 68/100 Batch 0/46 MMD train 0.00011678 \n",
      "Epoch 68/100 Batch 45/46 MMD train 0.00012684 \n",
      "Epoch 69/100 Batch 0/46 MMD train 0.00012457 \n",
      "Epoch 69/100 Batch 45/46 MMD train 0.00014532 \n",
      "Epoch 70/100 Batch 0/46 MMD train 0.00014499 \n",
      "Epoch 70/100 Batch 45/46 MMD train 0.00013956 \n",
      "Epoch 71/100 Batch 0/46 MMD train 0.00015321 \n",
      "Epoch 71/100 Batch 45/46 MMD train 0.00014549 \n",
      "Epoch 72/100 Batch 0/46 MMD train 0.00014552 \n",
      "Epoch 72/100 Batch 45/46 MMD train 0.00012634 \n",
      "Epoch 73/100 Batch 0/46 MMD train 0.00012762 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 Batch 45/46 MMD train 9.0268e-05 \n",
      "Epoch 74/100 Batch 0/46 MMD train 8.6315e-05 \n",
      "Epoch 74/100 Batch 45/46 MMD train 7.5775e-05 \n",
      "Epoch 75/100 Batch 0/46 MMD train 7.1639e-05 \n",
      "Epoch 75/100 Batch 45/46 MMD train 8.6056e-05 \n",
      "Epoch 76/100 Batch 0/46 MMD train 8.5989e-05 \n",
      "Epoch 76/100 Batch 45/46 MMD train 7.3632e-05 \n",
      "Epoch 77/100 Batch 0/46 MMD train 7.3274e-05 \n",
      "Epoch 77/100 Batch 45/46 MMD train 3.6008e-05 \n",
      "Epoch 78/100 Batch 0/46 MMD train 3.3359e-05 \n",
      "Epoch 78/100 Batch 45/46 MMD train 1.7156e-05 \n",
      "Epoch 79/100 Batch 0/46 MMD train 1.5727e-05 \n",
      "Epoch 79/100 Batch 45/46 MMD train -9.0698e-06 \n",
      "Epoch 80/100 Batch 0/46 MMD train -1.5477e-05 \n",
      "Epoch 80/100 Batch 45/46 MMD train -6.6143e-05 \n",
      "Epoch 81/100 Batch 0/46 MMD train -6.1814e-05 \n",
      "Epoch 81/100 Batch 45/46 MMD train -8.0614e-05 \n",
      "Epoch 82/100 Batch 0/46 MMD train -7.5457e-05 \n",
      "Epoch 82/100 Batch 45/46 MMD train -0.00010958 \n",
      "Epoch 83/100 Batch 0/46 MMD train -0.00011265 \n",
      "Epoch 83/100 Batch 45/46 MMD train -0.00016445 \n",
      "Epoch 84/100 Batch 0/46 MMD train -0.00016623 \n",
      "Epoch 84/100 Batch 45/46 MMD train -0.00018034 \n",
      "Epoch 85/100 Batch 0/46 MMD train -0.00017779 \n",
      "Epoch 85/100 Batch 45/46 MMD train -0.00018125 \n",
      "Epoch 86/100 Batch 0/46 MMD train -0.00018283 \n",
      "Epoch 86/100 Batch 45/46 MMD train -0.00016934 \n",
      "Epoch 87/100 Batch 0/46 MMD train -0.00017173 \n",
      "Epoch 87/100 Batch 45/46 MMD train -0.00014371 \n",
      "Epoch 88/100 Batch 0/46 MMD train -0.00014824 \n",
      "Epoch 88/100 Batch 45/46 MMD train -0.00013771 \n",
      "Epoch 89/100 Batch 0/46 MMD train -0.00013603 \n",
      "Epoch 89/100 Batch 45/46 MMD train -0.00011429 \n",
      "Epoch 90/100 Batch 0/46 MMD train -0.00010735 \n",
      "Epoch 90/100 Batch 45/46 MMD train -0.00011903 \n",
      "Epoch 91/100 Batch 0/46 MMD train -0.00011906 \n",
      "Epoch 91/100 Batch 45/46 MMD train -0.00010265 \n",
      "Epoch 92/100 Batch 0/46 MMD train -0.00010982 \n",
      "Epoch 92/100 Batch 45/46 MMD train -9.26e-05 \n",
      "Epoch 93/100 Batch 0/46 MMD train -9.2097e-05 \n",
      "Epoch 93/100 Batch 45/46 MMD train -6.2182e-05 \n",
      "Epoch 94/100 Batch 0/46 MMD train -6.5008e-05 \n",
      "Epoch 94/100 Batch 45/46 MMD train -3.4387e-05 \n",
      "Epoch 95/100 Batch 0/46 MMD train -3.4751e-05 \n",
      "Epoch 95/100 Batch 45/46 MMD train -3.1637e-05 \n",
      "Epoch 96/100 Batch 0/46 MMD train -3.5028e-05 \n",
      "Epoch 96/100 Batch 45/46 MMD train -6.8098e-05 \n",
      "Epoch 97/100 Batch 0/46 MMD train -7.2508e-05 \n",
      "Epoch 97/100 Batch 45/46 MMD train -8.7324e-05 \n",
      "Epoch 98/100 Batch 0/46 MMD train -9.6435e-05 \n",
      "Epoch 98/100 Batch 45/46 MMD train -7.0758e-05 \n",
      "Epoch 99/100 Batch 0/46 MMD train -7.2055e-05 \n",
      "Epoch 99/100 Batch 45/46 MMD train -5.6832e-05 \n",
      "Epoch 100/100 Batch 0/46 MMD train -5.6689e-05 \n",
      "Epoch 100/100 Batch 45/46 MMD train -6.4212e-05 \n",
      "Corruption 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/46 MMD train 0.014089 \n",
      "Epoch 1/100 Batch 45/46 MMD train 0.0020283 \n",
      "Epoch 2/100 Batch 0/46 MMD train 0.0017615 \n",
      "Epoch 2/100 Batch 45/46 MMD train 0.0013142 \n",
      "Epoch 3/100 Batch 0/46 MMD train 0.0014063 \n",
      "Epoch 3/100 Batch 45/46 MMD train 0.0011017 \n",
      "Epoch 4/100 Batch 0/46 MMD train 0.0010795 \n",
      "Epoch 4/100 Batch 45/46 MMD train 0.0010337 \n",
      "Epoch 5/100 Batch 0/46 MMD train 0.00096464 \n",
      "Epoch 5/100 Batch 45/46 MMD train 0.00091713 \n",
      "Epoch 6/100 Batch 0/46 MMD train 0.00093766 \n",
      "Epoch 6/100 Batch 45/46 MMD train 0.00083106 \n",
      "Epoch 7/100 Batch 0/46 MMD train 0.00086701 \n",
      "Epoch 7/100 Batch 45/46 MMD train 0.00083868 \n",
      "Epoch 8/100 Batch 0/46 MMD train 0.00084288 \n",
      "Epoch 8/100 Batch 45/46 MMD train 0.0010655 \n",
      "Epoch 9/100 Batch 0/46 MMD train 0.0011738 \n",
      "Epoch 9/100 Batch 45/46 MMD train 0.0015685 \n",
      "Epoch 10/100 Batch 0/46 MMD train 0.0016234 \n",
      "Epoch 10/100 Batch 45/46 MMD train 0.0018164 \n",
      "Epoch 11/100 Batch 0/46 MMD train 0.001793 \n",
      "Epoch 11/100 Batch 45/46 MMD train 0.001555 \n",
      "Epoch 12/100 Batch 0/46 MMD train 0.0015898 \n",
      "Epoch 12/100 Batch 45/46 MMD train 0.0013652 \n",
      "Epoch 13/100 Batch 0/46 MMD train 0.0013732 \n",
      "Epoch 13/100 Batch 45/46 MMD train 0.0012314 \n",
      "Epoch 14/100 Batch 0/46 MMD train 0.0012414 \n",
      "Epoch 14/100 Batch 45/46 MMD train 0.0010546 \n",
      "Epoch 15/100 Batch 0/46 MMD train 0.0010241 \n",
      "Epoch 15/100 Batch 45/46 MMD train 0.001246 \n",
      "Epoch 16/100 Batch 0/46 MMD train 0.0012656 \n",
      "Epoch 16/100 Batch 45/46 MMD train 0.0012757 \n",
      "Epoch 17/100 Batch 0/46 MMD train 0.0012369 \n",
      "Epoch 17/100 Batch 45/46 MMD train 0.00097054 \n",
      "Epoch 18/100 Batch 0/46 MMD train 0.00095846 \n",
      "Epoch 18/100 Batch 45/46 MMD train 0.00076913 \n",
      "Epoch 19/100 Batch 0/46 MMD train 0.00078127 \n",
      "Epoch 19/100 Batch 45/46 MMD train 0.00077775 \n",
      "Epoch 20/100 Batch 0/46 MMD train 0.00076601 \n",
      "Epoch 20/100 Batch 45/46 MMD train 0.00087002 \n",
      "Epoch 21/100 Batch 0/46 MMD train 0.0008691 \n",
      "Epoch 21/100 Batch 45/46 MMD train 0.00095153 \n",
      "Epoch 22/100 Batch 0/46 MMD train 0.00095198 \n",
      "Epoch 22/100 Batch 45/46 MMD train 0.00090738 \n",
      "Epoch 23/100 Batch 0/46 MMD train 0.00089564 \n",
      "Epoch 23/100 Batch 45/46 MMD train 0.00077853 \n",
      "Epoch 24/100 Batch 0/46 MMD train 0.00077957 \n",
      "Epoch 24/100 Batch 45/46 MMD train 0.00079732 \n",
      "Epoch 25/100 Batch 0/46 MMD train 0.00081027 \n",
      "Epoch 25/100 Batch 45/46 MMD train 0.00079857 \n",
      "Epoch 26/100 Batch 0/46 MMD train 0.0008337 \n",
      "Epoch 26/100 Batch 45/46 MMD train 0.00052252 \n",
      "Epoch 27/100 Batch 0/46 MMD train 0.00051636 \n",
      "Epoch 27/100 Batch 45/46 MMD train 0.0003326 \n",
      "Epoch 28/100 Batch 0/46 MMD train 0.00031788 \n",
      "Epoch 28/100 Batch 45/46 MMD train 0.00030654 \n",
      "Epoch 29/100 Batch 0/46 MMD train 0.00029831 \n",
      "Epoch 29/100 Batch 45/46 MMD train 0.0002739 \n",
      "Epoch 30/100 Batch 0/46 MMD train 0.00026242 \n",
      "Epoch 30/100 Batch 45/46 MMD train 0.00020431 \n",
      "Epoch 31/100 Batch 0/46 MMD train 0.00018755 \n",
      "Epoch 31/100 Batch 45/46 MMD train 0.00015034 \n",
      "Epoch 32/100 Batch 0/46 MMD train 0.00015574 \n",
      "Epoch 32/100 Batch 45/46 MMD train 0.00015856 \n",
      "Epoch 33/100 Batch 0/46 MMD train 0.00015761 \n",
      "Epoch 33/100 Batch 45/46 MMD train 0.00016204 \n",
      "Epoch 34/100 Batch 0/46 MMD train 0.00015672 \n",
      "Epoch 34/100 Batch 45/46 MMD train 0.00010536 \n",
      "Epoch 35/100 Batch 0/46 MMD train 0.00010793 \n",
      "Epoch 35/100 Batch 45/46 MMD train 0.0001455 \n",
      "Epoch 36/100 Batch 0/46 MMD train 0.00016296 \n",
      "Epoch 36/100 Batch 45/46 MMD train 0.00014555 \n",
      "Epoch 37/100 Batch 0/46 MMD train 0.0001637 \n",
      "Epoch 37/100 Batch 45/46 MMD train 0.00010473 \n",
      "Epoch 38/100 Batch 0/46 MMD train 0.00011192 \n",
      "Epoch 38/100 Batch 45/46 MMD train 1.0313e-05 \n",
      "Epoch 39/100 Batch 0/46 MMD train -2.038e-07 \n",
      "Epoch 39/100 Batch 45/46 MMD train -3.1057e-05 \n",
      "Epoch 40/100 Batch 0/46 MMD train -2.2794e-05 \n",
      "Epoch 40/100 Batch 45/46 MMD train -6.3991e-05 \n",
      "Epoch 41/100 Batch 0/46 MMD train -6.0634e-05 \n",
      "Epoch 41/100 Batch 45/46 MMD train -3.0726e-05 \n",
      "Epoch 42/100 Batch 0/46 MMD train -3.0599e-05 \n",
      "Epoch 42/100 Batch 45/46 MMD train -8.3462e-05 \n",
      "Epoch 43/100 Batch 0/46 MMD train -7.2616e-05 \n",
      "Epoch 43/100 Batch 45/46 MMD train -5.2343e-05 \n",
      "Epoch 44/100 Batch 0/46 MMD train -5.0954e-05 \n",
      "Epoch 44/100 Batch 45/46 MMD train -6.0292e-05 \n",
      "Epoch 45/100 Batch 0/46 MMD train -7.8842e-05 \n",
      "Epoch 45/100 Batch 45/46 MMD train -0.00014584 \n",
      "Epoch 46/100 Batch 0/46 MMD train -0.00014712 \n",
      "Epoch 46/100 Batch 45/46 MMD train -0.00011799 \n",
      "Epoch 47/100 Batch 0/46 MMD train -0.00011679 \n",
      "Epoch 47/100 Batch 45/46 MMD train -8.225e-05 \n",
      "Epoch 48/100 Batch 0/46 MMD train -7.7134e-05 \n",
      "Epoch 48/100 Batch 45/46 MMD train -0.00018134 \n",
      "Epoch 49/100 Batch 0/46 MMD train -0.00018185 \n",
      "Epoch 49/100 Batch 45/46 MMD train -0.00018548 \n",
      "Epoch 50/100 Batch 0/46 MMD train -0.0001775 \n",
      "Epoch 50/100 Batch 45/46 MMD train -0.00020433 \n",
      "Epoch 51/100 Batch 0/46 MMD train -0.00020096 \n",
      "Epoch 51/100 Batch 45/46 MMD train -0.00020417 \n",
      "Epoch 52/100 Batch 0/46 MMD train -0.00019898 \n",
      "Epoch 52/100 Batch 45/46 MMD train -0.00026432 \n",
      "Epoch 53/100 Batch 0/46 MMD train -0.00026641 \n",
      "Epoch 53/100 Batch 45/46 MMD train -0.00020961 \n",
      "Epoch 54/100 Batch 0/46 MMD train -0.00022308 \n",
      "Epoch 54/100 Batch 45/46 MMD train -0.00022047 \n",
      "Epoch 55/100 Batch 0/46 MMD train -0.00021568 \n",
      "Epoch 55/100 Batch 45/46 MMD train -0.00023912 \n",
      "Epoch 56/100 Batch 0/46 MMD train -0.00023204 \n",
      "Epoch 56/100 Batch 45/46 MMD train -0.00017253 \n",
      "Epoch 57/100 Batch 0/46 MMD train -0.00016772 \n",
      "Epoch 57/100 Batch 45/46 MMD train -0.00016181 \n",
      "Epoch 58/100 Batch 0/46 MMD train -0.00015707 \n",
      "Epoch 58/100 Batch 45/46 MMD train -0.00016157 \n",
      "Epoch 59/100 Batch 0/46 MMD train -0.00016752 \n",
      "Epoch 59/100 Batch 45/46 MMD train -0.00015971 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 Batch 0/46 MMD train -0.00016113 \n",
      "Epoch 60/100 Batch 45/46 MMD train -0.00021144 \n",
      "Epoch 61/100 Batch 0/46 MMD train -0.0002207 \n",
      "Epoch 61/100 Batch 45/46 MMD train -0.00024987 \n",
      "Epoch 62/100 Batch 0/46 MMD train -0.00024474 \n",
      "Epoch 62/100 Batch 45/46 MMD train -0.00028016 \n",
      "Epoch 63/100 Batch 0/46 MMD train -0.00027724 \n",
      "Epoch 63/100 Batch 45/46 MMD train -0.00031927 \n",
      "Epoch 64/100 Batch 0/46 MMD train -0.00031228 \n",
      "Epoch 64/100 Batch 45/46 MMD train -0.00028258 \n",
      "Epoch 65/100 Batch 0/46 MMD train -0.00028653 \n",
      "Epoch 65/100 Batch 45/46 MMD train -0.00024881 \n",
      "Epoch 66/100 Batch 0/46 MMD train -0.00024682 \n",
      "Epoch 66/100 Batch 45/46 MMD train -0.00025314 \n",
      "Epoch 67/100 Batch 0/46 MMD train -0.00024961 \n",
      "Epoch 67/100 Batch 45/46 MMD train -0.00025654 \n",
      "Epoch 68/100 Batch 0/46 MMD train -0.0002665 \n",
      "Epoch 68/100 Batch 45/46 MMD train -0.00027586 \n",
      "Epoch 69/100 Batch 0/46 MMD train -0.00027402 \n",
      "Epoch 69/100 Batch 45/46 MMD train -0.00026295 \n",
      "Epoch 70/100 Batch 0/46 MMD train -0.00026683 \n",
      "Epoch 70/100 Batch 45/46 MMD train -0.00027829 \n",
      "Epoch 71/100 Batch 0/46 MMD train -0.00027816 \n",
      "Epoch 71/100 Batch 45/46 MMD train -0.00027908 \n",
      "Epoch 72/100 Batch 0/46 MMD train -0.00027344 \n",
      "Epoch 72/100 Batch 45/46 MMD train -0.00018712 \n",
      "Epoch 73/100 Batch 0/46 MMD train -0.0001903 \n",
      "Epoch 73/100 Batch 45/46 MMD train -0.00018732 \n",
      "Epoch 74/100 Batch 0/46 MMD train -0.00019245 \n",
      "Epoch 74/100 Batch 45/46 MMD train -0.00019985 \n",
      "Epoch 75/100 Batch 0/46 MMD train -0.00020438 \n",
      "Epoch 75/100 Batch 45/46 MMD train -0.00021497 \n",
      "Epoch 76/100 Batch 0/46 MMD train -0.00020856 \n",
      "Epoch 76/100 Batch 45/46 MMD train -0.00014145 \n",
      "Epoch 77/100 Batch 0/46 MMD train -0.00013268 \n",
      "Epoch 77/100 Batch 45/46 MMD train -0.00012804 \n",
      "Epoch 78/100 Batch 0/46 MMD train -0.00013091 \n",
      "Epoch 78/100 Batch 45/46 MMD train -0.00010691 \n",
      "Epoch 79/100 Batch 0/46 MMD train -9.6456e-05 \n",
      "Epoch 79/100 Batch 45/46 MMD train -0.00011713 \n",
      "Epoch 80/100 Batch 0/46 MMD train -0.00012492 \n",
      "Epoch 80/100 Batch 45/46 MMD train -0.00012212 \n",
      "Epoch 81/100 Batch 0/46 MMD train -0.00012793 \n",
      "Epoch 81/100 Batch 45/46 MMD train -0.00010212 \n",
      "Epoch 82/100 Batch 0/46 MMD train -9.8401e-05 \n",
      "Epoch 82/100 Batch 45/46 MMD train -0.00013971 \n",
      "Epoch 83/100 Batch 0/46 MMD train -0.00014235 \n",
      "Epoch 83/100 Batch 45/46 MMD train -0.00021642 \n",
      "Epoch 84/100 Batch 0/46 MMD train -0.00022105 \n",
      "Epoch 84/100 Batch 45/46 MMD train -0.00021981 \n",
      "Epoch 85/100 Batch 0/46 MMD train -0.00022289 \n",
      "Epoch 85/100 Batch 45/46 MMD train -0.0002196 \n",
      "Epoch 86/100 Batch 0/46 MMD train -0.00022301 \n",
      "Epoch 86/100 Batch 45/46 MMD train -0.00021934 \n",
      "Epoch 87/100 Batch 0/46 MMD train -0.00021357 \n",
      "Epoch 87/100 Batch 45/46 MMD train -0.00022983 \n",
      "Epoch 88/100 Batch 0/46 MMD train -0.00023118 \n",
      "Epoch 88/100 Batch 45/46 MMD train -0.00021152 \n",
      "Epoch 89/100 Batch 0/46 MMD train -0.00021091 \n",
      "Epoch 89/100 Batch 45/46 MMD train -0.00020021 \n",
      "Epoch 90/100 Batch 0/46 MMD train -0.00020236 \n",
      "Epoch 90/100 Batch 45/46 MMD train -0.00021335 \n",
      "Epoch 91/100 Batch 0/46 MMD train -0.00020727 \n",
      "Epoch 91/100 Batch 45/46 MMD train -0.00019031 \n",
      "Epoch 92/100 Batch 0/46 MMD train -0.00019806 \n",
      "Epoch 92/100 Batch 45/46 MMD train -0.00020173 \n",
      "Epoch 93/100 Batch 0/46 MMD train -0.00019721 \n",
      "Epoch 93/100 Batch 45/46 MMD train -0.00015701 \n",
      "Epoch 94/100 Batch 0/46 MMD train -0.00015661 \n",
      "Epoch 94/100 Batch 45/46 MMD train -0.00018324 \n",
      "Epoch 95/100 Batch 0/46 MMD train -0.00018294 \n",
      "Epoch 95/100 Batch 45/46 MMD train -0.00019561 \n",
      "Epoch 96/100 Batch 0/46 MMD train -0.00019713 \n",
      "Epoch 96/100 Batch 45/46 MMD train -0.00016736 \n",
      "Epoch 97/100 Batch 0/46 MMD train -0.00016289 \n",
      "Epoch 97/100 Batch 45/46 MMD train -0.00011019 \n",
      "Epoch 98/100 Batch 0/46 MMD train -0.00011102 \n",
      "Epoch 98/100 Batch 45/46 MMD train -0.00010534 \n",
      "Epoch 99/100 Batch 0/46 MMD train -0.0001016 \n",
      "Epoch 99/100 Batch 45/46 MMD train -9.9476e-05 \n",
      "Epoch 100/100 Batch 0/46 MMD train -9.8704e-05 \n",
      "Epoch 100/100 Batch 45/46 MMD train -0.00012406 \n",
      "Corruption 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/46 MMD train 0.0080062 \n",
      "Epoch 1/100 Batch 45/46 MMD train 5.3492e-05 \n",
      "Epoch 2/100 Batch 0/46 MMD train -0.00038568 \n",
      "Epoch 2/100 Batch 45/46 MMD train -0.0011711 \n",
      "Epoch 3/100 Batch 0/46 MMD train -0.00092839 \n",
      "Epoch 3/100 Batch 45/46 MMD train -0.0014956 \n",
      "Epoch 4/100 Batch 0/46 MMD train -0.0015675 \n",
      "Epoch 4/100 Batch 45/46 MMD train -0.00031401 \n",
      "Epoch 5/100 Batch 0/46 MMD train -0.00026781 \n",
      "Epoch 5/100 Batch 45/46 MMD train 0.00051469 \n",
      "Epoch 6/100 Batch 0/46 MMD train 0.00045777 \n",
      "Epoch 6/100 Batch 45/46 MMD train 0.00030988 \n",
      "Epoch 7/100 Batch 0/46 MMD train 0.00019104 \n",
      "Epoch 7/100 Batch 45/46 MMD train 0.00048607 \n",
      "Epoch 8/100 Batch 0/46 MMD train 0.00050974 \n",
      "Epoch 8/100 Batch 45/46 MMD train 0.00013291 \n",
      "Epoch 9/100 Batch 0/46 MMD train 0.00015003 \n",
      "Epoch 9/100 Batch 45/46 MMD train 0.00063892 \n",
      "Epoch 10/100 Batch 0/46 MMD train 0.00069205 \n",
      "Epoch 10/100 Batch 45/46 MMD train 0.00088285 \n",
      "Epoch 11/100 Batch 0/46 MMD train 0.00080662 \n",
      "Epoch 11/100 Batch 45/46 MMD train 0.001014 \n",
      "Epoch 12/100 Batch 0/46 MMD train 0.001032 \n",
      "Epoch 12/100 Batch 45/46 MMD train 0.00093695 \n",
      "Epoch 13/100 Batch 0/46 MMD train 0.00094758 \n",
      "Epoch 13/100 Batch 45/46 MMD train 0.00095662 \n",
      "Epoch 14/100 Batch 0/46 MMD train 0.00094367 \n",
      "Epoch 14/100 Batch 45/46 MMD train 0.0010811 \n",
      "Epoch 15/100 Batch 0/46 MMD train 0.0010875 \n",
      "Epoch 15/100 Batch 45/46 MMD train 0.00092032 \n",
      "Epoch 16/100 Batch 0/46 MMD train 0.00093181 \n",
      "Epoch 16/100 Batch 45/46 MMD train 0.00081679 \n",
      "Epoch 17/100 Batch 0/46 MMD train 0.00081955 \n",
      "Epoch 17/100 Batch 45/46 MMD train 0.00074247 \n",
      "Epoch 18/100 Batch 0/46 MMD train 0.00072933 \n",
      "Epoch 18/100 Batch 45/46 MMD train 0.0006377 \n",
      "Epoch 19/100 Batch 0/46 MMD train 0.00065692 \n",
      "Epoch 19/100 Batch 45/46 MMD train 0.00068554 \n",
      "Epoch 20/100 Batch 0/46 MMD train 0.00067876 \n",
      "Epoch 20/100 Batch 45/46 MMD train 0.00049199 \n",
      "Epoch 21/100 Batch 0/46 MMD train 0.00047611 \n",
      "Epoch 21/100 Batch 45/46 MMD train 0.00039305 \n",
      "Epoch 22/100 Batch 0/46 MMD train 0.0004034 \n",
      "Epoch 22/100 Batch 45/46 MMD train 0.00067637 \n",
      "Epoch 23/100 Batch 0/46 MMD train 0.00066953 \n",
      "Epoch 23/100 Batch 45/46 MMD train 0.00074415 \n",
      "Epoch 24/100 Batch 0/46 MMD train 0.00074943 \n",
      "Epoch 24/100 Batch 45/46 MMD train 0.00086194 \n",
      "Epoch 25/100 Batch 0/46 MMD train 0.00085955 \n",
      "Epoch 25/100 Batch 45/46 MMD train 0.00073075 \n",
      "Epoch 26/100 Batch 0/46 MMD train 0.0007387 \n",
      "Epoch 26/100 Batch 45/46 MMD train 0.00089981 \n",
      "Epoch 27/100 Batch 0/46 MMD train 0.00088565 \n",
      "Epoch 27/100 Batch 45/46 MMD train 0.0010401 \n",
      "Epoch 28/100 Batch 0/46 MMD train 0.0010371 \n",
      "Epoch 28/100 Batch 45/46 MMD train 0.0010263 \n",
      "Epoch 29/100 Batch 0/46 MMD train 0.0010366 \n",
      "Epoch 29/100 Batch 45/46 MMD train 0.0010403 \n",
      "Epoch 30/100 Batch 0/46 MMD train 0.0010291 \n",
      "Epoch 30/100 Batch 45/46 MMD train 0.0010004 \n",
      "Epoch 31/100 Batch 0/46 MMD train 0.00099895 \n",
      "Epoch 31/100 Batch 45/46 MMD train 0.00090691 \n",
      "Epoch 32/100 Batch 0/46 MMD train 0.00089352 \n",
      "Epoch 32/100 Batch 45/46 MMD train 0.00090954 \n",
      "Epoch 33/100 Batch 0/46 MMD train 0.00090793 \n",
      "Epoch 33/100 Batch 45/46 MMD train 0.00076539 \n",
      "Epoch 34/100 Batch 0/46 MMD train 0.00075259 \n",
      "Epoch 34/100 Batch 45/46 MMD train 0.00075714 \n",
      "Epoch 35/100 Batch 0/46 MMD train 0.00076373 \n",
      "Epoch 35/100 Batch 45/46 MMD train 0.00076618 \n",
      "Epoch 36/100 Batch 0/46 MMD train 0.00074792 \n",
      "Epoch 36/100 Batch 45/46 MMD train 0.00072892 \n",
      "Epoch 37/100 Batch 0/46 MMD train 0.00073975 \n",
      "Epoch 37/100 Batch 45/46 MMD train 0.00073391 \n",
      "Epoch 38/100 Batch 0/46 MMD train 0.00072728 \n",
      "Epoch 38/100 Batch 45/46 MMD train 0.00071811 \n",
      "Epoch 39/100 Batch 0/46 MMD train 0.00072209 \n",
      "Epoch 39/100 Batch 45/46 MMD train 0.00069408 \n",
      "Epoch 40/100 Batch 0/46 MMD train 0.00069212 \n",
      "Epoch 40/100 Batch 45/46 MMD train 0.00062834 \n",
      "Epoch 41/100 Batch 0/46 MMD train 0.0006198 \n",
      "Epoch 41/100 Batch 45/46 MMD train 0.00054266 \n",
      "Epoch 42/100 Batch 0/46 MMD train 0.00054778 \n",
      "Epoch 42/100 Batch 45/46 MMD train 0.00058112 \n",
      "Epoch 43/100 Batch 0/46 MMD train 0.00059418 \n",
      "Epoch 43/100 Batch 45/46 MMD train 0.00049889 \n",
      "Epoch 44/100 Batch 0/46 MMD train 0.00049941 \n",
      "Epoch 44/100 Batch 45/46 MMD train 0.00057475 \n",
      "Epoch 45/100 Batch 0/46 MMD train 0.00055749 \n",
      "Epoch 45/100 Batch 45/46 MMD train 0.0005379 \n",
      "Epoch 46/100 Batch 0/46 MMD train 0.00054308 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 Batch 45/46 MMD train 0.00055729 \n",
      "Epoch 47/100 Batch 0/46 MMD train 0.00055361 \n",
      "Epoch 47/100 Batch 45/46 MMD train 0.00057436 \n",
      "Epoch 48/100 Batch 0/46 MMD train 0.00056205 \n",
      "Epoch 48/100 Batch 45/46 MMD train 0.00057049 \n",
      "Epoch 49/100 Batch 0/46 MMD train 0.00056869 \n",
      "Epoch 49/100 Batch 45/46 MMD train 0.00057241 \n",
      "Epoch 50/100 Batch 0/46 MMD train 0.00057397 \n",
      "Epoch 50/100 Batch 45/46 MMD train 0.00055028 \n",
      "Epoch 51/100 Batch 0/46 MMD train 0.00054315 \n",
      "Epoch 51/100 Batch 45/46 MMD train 0.00053828 \n",
      "Epoch 52/100 Batch 0/46 MMD train 0.0005423 \n",
      "Epoch 52/100 Batch 45/46 MMD train 0.00047713 \n",
      "Epoch 53/100 Batch 0/46 MMD train 0.00048706 \n",
      "Epoch 53/100 Batch 45/46 MMD train 0.00053187 \n",
      "Epoch 54/100 Batch 0/46 MMD train 0.00054078 \n",
      "Epoch 54/100 Batch 45/46 MMD train 0.00058368 \n",
      "Epoch 55/100 Batch 0/46 MMD train 0.00059116 \n",
      "Epoch 55/100 Batch 45/46 MMD train 0.0005789 \n",
      "Epoch 56/100 Batch 0/46 MMD train 0.00058347 \n",
      "Epoch 56/100 Batch 45/46 MMD train 0.00053294 \n",
      "Epoch 57/100 Batch 0/46 MMD train 0.00054198 \n",
      "Epoch 57/100 Batch 45/46 MMD train 0.00061106 \n",
      "Epoch 58/100 Batch 0/46 MMD train 0.00060661 \n",
      "Epoch 58/100 Batch 45/46 MMD train 0.00060194 \n",
      "Epoch 59/100 Batch 0/46 MMD train 0.00060542 \n",
      "Epoch 59/100 Batch 45/46 MMD train 0.00055594 \n",
      "Epoch 60/100 Batch 0/46 MMD train 0.00055729 \n",
      "Epoch 60/100 Batch 45/46 MMD train 0.00054329 \n",
      "Epoch 61/100 Batch 0/46 MMD train 0.0005457 \n",
      "Epoch 61/100 Batch 45/46 MMD train 0.00056158 \n",
      "Epoch 62/100 Batch 0/46 MMD train 0.00056301 \n",
      "Epoch 62/100 Batch 45/46 MMD train 0.00050767 \n",
      "Epoch 63/100 Batch 0/46 MMD train 0.00051039 \n",
      "Epoch 63/100 Batch 45/46 MMD train 0.00053799 \n",
      "Epoch 64/100 Batch 0/46 MMD train 0.0005421 \n",
      "Epoch 64/100 Batch 45/46 MMD train 0.00062321 \n",
      "Epoch 65/100 Batch 0/46 MMD train 0.0006222 \n",
      "Epoch 65/100 Batch 45/46 MMD train 0.00055058 \n",
      "Epoch 66/100 Batch 0/46 MMD train 0.00056111 \n",
      "Epoch 66/100 Batch 45/46 MMD train 0.00060881 \n",
      "Epoch 67/100 Batch 0/46 MMD train 0.00060382 \n",
      "Epoch 67/100 Batch 45/46 MMD train 0.00062412 \n",
      "Epoch 68/100 Batch 0/46 MMD train 0.00062561 \n",
      "Epoch 68/100 Batch 45/46 MMD train 0.00061768 \n",
      "Epoch 69/100 Batch 0/46 MMD train 0.00062063 \n",
      "Epoch 69/100 Batch 45/46 MMD train 0.00059897 \n",
      "Epoch 70/100 Batch 0/46 MMD train 0.00060378 \n",
      "Epoch 70/100 Batch 45/46 MMD train 0.00057205 \n",
      "Epoch 71/100 Batch 0/46 MMD train 0.00056951 \n",
      "Epoch 71/100 Batch 45/46 MMD train 0.00055823 \n",
      "Epoch 72/100 Batch 0/46 MMD train 0.00056148 \n",
      "Epoch 72/100 Batch 45/46 MMD train 0.00058758 \n",
      "Epoch 73/100 Batch 0/46 MMD train 0.00058783 \n",
      "Epoch 73/100 Batch 45/46 MMD train 0.0005752 \n",
      "Epoch 74/100 Batch 0/46 MMD train 0.000576 \n",
      "Epoch 74/100 Batch 45/46 MMD train 0.00061393 \n",
      "Epoch 75/100 Batch 0/46 MMD train 0.00061422 \n",
      "Epoch 75/100 Batch 45/46 MMD train 0.00052018 \n",
      "Epoch 76/100 Batch 0/46 MMD train 0.00051124 \n",
      "Epoch 76/100 Batch 45/46 MMD train 0.00048876 \n",
      "Epoch 77/100 Batch 0/46 MMD train 0.00049278 \n",
      "Epoch 77/100 Batch 45/46 MMD train 0.00048457 \n",
      "Epoch 78/100 Batch 0/46 MMD train 0.00047604 \n",
      "Epoch 78/100 Batch 45/46 MMD train 0.00048304 \n",
      "Epoch 79/100 Batch 0/46 MMD train 0.00048404 \n",
      "Epoch 79/100 Batch 45/46 MMD train 0.00045415 \n",
      "Epoch 80/100 Batch 0/46 MMD train 0.00046086 \n",
      "Epoch 80/100 Batch 45/46 MMD train 0.00044147 \n",
      "Epoch 81/100 Batch 0/46 MMD train 0.00044439 \n",
      "Epoch 81/100 Batch 45/46 MMD train 0.00048318 \n",
      "Epoch 82/100 Batch 0/46 MMD train 0.00048456 \n",
      "Epoch 82/100 Batch 45/46 MMD train 0.00050823 \n",
      "Epoch 83/100 Batch 0/46 MMD train 0.0005148 \n",
      "Epoch 83/100 Batch 45/46 MMD train 0.00047256 \n",
      "Epoch 84/100 Batch 0/46 MMD train 0.00047329 \n",
      "Epoch 84/100 Batch 45/46 MMD train 0.00044812 \n",
      "Epoch 85/100 Batch 0/46 MMD train 0.00045067 \n",
      "Epoch 85/100 Batch 45/46 MMD train 0.00044904 \n",
      "Epoch 86/100 Batch 0/46 MMD train 0.00045097 \n",
      "Epoch 86/100 Batch 45/46 MMD train 0.00039313 \n",
      "Epoch 87/100 Batch 0/46 MMD train 0.00039297 \n",
      "Epoch 87/100 Batch 45/46 MMD train 0.00040977 \n",
      "Epoch 88/100 Batch 0/46 MMD train 0.00041413 \n",
      "Epoch 88/100 Batch 45/46 MMD train 0.00046232 \n",
      "Epoch 89/100 Batch 0/46 MMD train 0.0004542 \n",
      "Epoch 89/100 Batch 45/46 MMD train 0.0004481 \n",
      "Epoch 90/100 Batch 0/46 MMD train 0.00044835 \n",
      "Epoch 90/100 Batch 45/46 MMD train 0.00050714 \n",
      "Epoch 91/100 Batch 0/46 MMD train 0.00050445 \n",
      "Epoch 91/100 Batch 45/46 MMD train 0.00050559 \n",
      "Epoch 92/100 Batch 0/46 MMD train 0.00050618 \n",
      "Epoch 92/100 Batch 45/46 MMD train 0.00051894 \n",
      "Epoch 93/100 Batch 0/46 MMD train 0.00051518 \n",
      "Epoch 93/100 Batch 45/46 MMD train 0.00052264 \n",
      "Epoch 94/100 Batch 0/46 MMD train 0.00052038 \n",
      "Epoch 94/100 Batch 45/46 MMD train 0.00047778 \n",
      "Epoch 95/100 Batch 0/46 MMD train 0.00047862 \n",
      "Epoch 95/100 Batch 45/46 MMD train 0.00045195 \n",
      "Epoch 96/100 Batch 0/46 MMD train 0.0004588 \n",
      "Epoch 96/100 Batch 45/46 MMD train 0.00044271 \n",
      "Epoch 97/100 Batch 0/46 MMD train 0.00044431 \n",
      "Epoch 97/100 Batch 45/46 MMD train 0.00045768 \n",
      "Epoch 98/100 Batch 0/46 MMD train 0.00045286 \n",
      "Epoch 98/100 Batch 45/46 MMD train 0.00045854 \n",
      "Epoch 99/100 Batch 0/46 MMD train 0.0004634 \n",
      "Epoch 99/100 Batch 45/46 MMD train 0.00048012 \n",
      "Epoch 100/100 Batch 0/46 MMD train 0.00047497 \n",
      "Epoch 100/100 Batch 45/46 MMD train 0.00050112 \n",
      "Corruption 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/46 MMD train 0.0055508 \n",
      "Epoch 1/100 Batch 45/46 MMD train 0.00042 \n",
      "Epoch 2/100 Batch 0/46 MMD train 0.0003663 \n",
      "Epoch 2/100 Batch 45/46 MMD train -0.00026237 \n",
      "Epoch 3/100 Batch 0/46 MMD train -0.00027757 \n",
      "Epoch 3/100 Batch 45/46 MMD train -0.00041417 \n",
      "Epoch 4/100 Batch 0/46 MMD train -0.00047003 \n",
      "Epoch 4/100 Batch 45/46 MMD train -0.00017469 \n",
      "Epoch 5/100 Batch 0/46 MMD train -0.00016129 \n",
      "Epoch 5/100 Batch 45/46 MMD train 0.00064803 \n",
      "Epoch 6/100 Batch 0/46 MMD train 0.0005938 \n",
      "Epoch 6/100 Batch 45/46 MMD train 0.00030472 \n",
      "Epoch 7/100 Batch 0/46 MMD train 0.00026891 \n",
      "Epoch 7/100 Batch 45/46 MMD train 0.00051263 \n",
      "Epoch 8/100 Batch 0/46 MMD train 0.00051805 \n",
      "Epoch 8/100 Batch 45/46 MMD train 0.00055806 \n",
      "Epoch 9/100 Batch 0/46 MMD train 0.00062011 \n",
      "Epoch 9/100 Batch 45/46 MMD train 0.0010106 \n",
      "Epoch 10/100 Batch 0/46 MMD train 0.0010587 \n",
      "Epoch 10/100 Batch 45/46 MMD train 0.00064228 \n",
      "Epoch 11/100 Batch 0/46 MMD train 0.00056499 \n",
      "Epoch 11/100 Batch 45/46 MMD train 0.00085748 \n",
      "Epoch 12/100 Batch 0/46 MMD train 0.0008541 \n",
      "Epoch 12/100 Batch 45/46 MMD train 0.00067077 \n",
      "Epoch 13/100 Batch 0/46 MMD train 0.0006793 \n",
      "Epoch 13/100 Batch 45/46 MMD train 0.00042344 \n",
      "Epoch 14/100 Batch 0/46 MMD train 0.00041411 \n",
      "Epoch 14/100 Batch 45/46 MMD train 0.00066409 \n",
      "Epoch 15/100 Batch 0/46 MMD train 0.00063891 \n",
      "Epoch 15/100 Batch 45/46 MMD train 0.00065048 \n",
      "Epoch 16/100 Batch 0/46 MMD train 0.00067299 \n",
      "Epoch 16/100 Batch 45/46 MMD train 0.00040225 \n",
      "Epoch 17/100 Batch 0/46 MMD train 0.00041148 \n",
      "Epoch 17/100 Batch 45/46 MMD train 0.00041096 \n",
      "Epoch 18/100 Batch 0/46 MMD train 0.00042013 \n",
      "Epoch 18/100 Batch 45/46 MMD train 0.00043604 \n",
      "Epoch 19/100 Batch 0/46 MMD train 0.00042518 \n",
      "Epoch 19/100 Batch 45/46 MMD train 0.00025337 \n",
      "Epoch 20/100 Batch 0/46 MMD train 0.00026357 \n",
      "Epoch 20/100 Batch 45/46 MMD train 0.00017403 \n",
      "Epoch 21/100 Batch 0/46 MMD train 0.00019549 \n",
      "Epoch 21/100 Batch 45/46 MMD train 0.00025474 \n",
      "Epoch 22/100 Batch 0/46 MMD train 0.0002614 \n",
      "Epoch 22/100 Batch 45/46 MMD train 0.00036981 \n",
      "Epoch 23/100 Batch 0/46 MMD train 0.0003708 \n",
      "Epoch 23/100 Batch 45/46 MMD train 0.00042216 \n",
      "Epoch 24/100 Batch 0/46 MMD train 0.00042999 \n",
      "Epoch 24/100 Batch 45/46 MMD train 0.0003599 \n",
      "Epoch 25/100 Batch 0/46 MMD train 0.00038578 \n",
      "Epoch 25/100 Batch 45/46 MMD train 0.00043894 \n",
      "Epoch 26/100 Batch 0/46 MMD train 0.00042835 \n",
      "Epoch 26/100 Batch 45/46 MMD train 0.00043564 \n",
      "Epoch 27/100 Batch 0/46 MMD train 0.00042269 \n",
      "Epoch 27/100 Batch 45/46 MMD train 0.00041889 \n",
      "Epoch 28/100 Batch 0/46 MMD train 0.00041519 \n",
      "Epoch 28/100 Batch 45/46 MMD train 0.00034469 \n",
      "Epoch 29/100 Batch 0/46 MMD train 0.00034919 \n",
      "Epoch 29/100 Batch 45/46 MMD train 0.00042689 \n",
      "Epoch 30/100 Batch 0/46 MMD train 0.00044474 \n",
      "Epoch 30/100 Batch 45/46 MMD train 0.00033463 \n",
      "Epoch 31/100 Batch 0/46 MMD train 0.00033579 \n",
      "Epoch 31/100 Batch 45/46 MMD train 0.0002926 \n",
      "Epoch 32/100 Batch 0/46 MMD train 0.00030029 \n",
      "Epoch 32/100 Batch 45/46 MMD train 0.00028817 \n",
      "Epoch 33/100 Batch 0/46 MMD train 0.00028475 \n",
      "Epoch 33/100 Batch 45/46 MMD train 0.00012633 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 Batch 0/46 MMD train 0.0001215 \n",
      "Epoch 34/100 Batch 45/46 MMD train 0.00016335 \n",
      "Epoch 35/100 Batch 0/46 MMD train 0.00014843 \n",
      "Epoch 35/100 Batch 45/46 MMD train 0.00014202 \n",
      "Epoch 36/100 Batch 0/46 MMD train 0.00015392 \n",
      "Epoch 36/100 Batch 45/46 MMD train 0.00012386 \n",
      "Epoch 37/100 Batch 0/46 MMD train 0.00013605 \n",
      "Epoch 37/100 Batch 45/46 MMD train 0.00015541 \n",
      "Epoch 38/100 Batch 0/46 MMD train 0.00015721 \n",
      "Epoch 38/100 Batch 45/46 MMD train 3.5272e-05 \n",
      "Epoch 39/100 Batch 0/46 MMD train 4.8048e-05 \n",
      "Epoch 39/100 Batch 45/46 MMD train -4.4736e-05 \n",
      "Epoch 40/100 Batch 0/46 MMD train -3.9e-05 \n",
      "Epoch 40/100 Batch 45/46 MMD train -4.5075e-05 \n",
      "Epoch 41/100 Batch 0/46 MMD train -4.0956e-05 \n",
      "Epoch 41/100 Batch 45/46 MMD train -5.1635e-05 \n",
      "Epoch 42/100 Batch 0/46 MMD train -6.4162e-05 \n",
      "Epoch 42/100 Batch 45/46 MMD train -0.00011555 \n",
      "Epoch 43/100 Batch 0/46 MMD train -0.00010975 \n",
      "Epoch 43/100 Batch 45/46 MMD train -9.9679e-05 \n",
      "Epoch 44/100 Batch 0/46 MMD train -9.132e-05 \n",
      "Epoch 44/100 Batch 45/46 MMD train -7.7513e-05 \n",
      "Epoch 45/100 Batch 0/46 MMD train -8.1307e-05 \n",
      "Epoch 45/100 Batch 45/46 MMD train -5.9834e-05 \n",
      "Epoch 46/100 Batch 0/46 MMD train -5.7571e-05 \n",
      "Epoch 46/100 Batch 45/46 MMD train -0.00016478 \n",
      "Epoch 47/100 Batch 0/46 MMD train -0.00017282 \n",
      "Epoch 47/100 Batch 45/46 MMD train -0.00023684 \n",
      "Epoch 48/100 Batch 0/46 MMD train -0.00022893 \n",
      "Epoch 48/100 Batch 45/46 MMD train -0.00020889 \n",
      "Epoch 49/100 Batch 0/46 MMD train -0.00019919 \n",
      "Epoch 49/100 Batch 45/46 MMD train -0.00021495 \n",
      "Epoch 50/100 Batch 0/46 MMD train -0.00021127 \n",
      "Epoch 50/100 Batch 45/46 MMD train -0.00027317 \n",
      "Epoch 51/100 Batch 0/46 MMD train -0.00028072 \n",
      "Epoch 51/100 Batch 45/46 MMD train -0.00029058 \n",
      "Epoch 52/100 Batch 0/46 MMD train -0.00029938 \n",
      "Epoch 52/100 Batch 45/46 MMD train -0.00033381 \n",
      "Epoch 53/100 Batch 0/46 MMD train -0.00034142 \n",
      "Epoch 53/100 Batch 45/46 MMD train -0.0001959 \n",
      "Epoch 54/100 Batch 0/46 MMD train -0.00019339 \n",
      "Epoch 54/100 Batch 45/46 MMD train -0.00020789 \n",
      "Epoch 55/100 Batch 0/46 MMD train -0.00019488 \n",
      "Epoch 55/100 Batch 45/46 MMD train -0.00018153 \n",
      "Epoch 56/100 Batch 0/46 MMD train -0.00017439 \n",
      "Epoch 56/100 Batch 45/46 MMD train -0.00018782 \n",
      "Epoch 57/100 Batch 0/46 MMD train -0.00017846 \n",
      "Epoch 57/100 Batch 45/46 MMD train -0.00010331 \n",
      "Epoch 58/100 Batch 0/46 MMD train -0.00010419 \n",
      "Epoch 58/100 Batch 45/46 MMD train -0.00013328 \n",
      "Epoch 59/100 Batch 0/46 MMD train -0.00013018 \n",
      "Epoch 59/100 Batch 45/46 MMD train -9.4977e-05 \n",
      "Epoch 60/100 Batch 0/46 MMD train -0.00010081 \n",
      "Epoch 60/100 Batch 45/46 MMD train -9.566e-05 \n",
      "Epoch 61/100 Batch 0/46 MMD train -9.0257e-05 \n",
      "Epoch 61/100 Batch 45/46 MMD train -0.00019674 \n",
      "Epoch 62/100 Batch 0/46 MMD train -0.0001931 \n",
      "Epoch 62/100 Batch 45/46 MMD train -0.000205 \n",
      "Epoch 63/100 Batch 0/46 MMD train -0.00021082 \n",
      "Epoch 63/100 Batch 45/46 MMD train -0.00017242 \n",
      "Epoch 64/100 Batch 0/46 MMD train -0.00017319 \n",
      "Epoch 64/100 Batch 45/46 MMD train -0.00013267 \n",
      "Epoch 65/100 Batch 0/46 MMD train -0.00013423 \n",
      "Epoch 65/100 Batch 45/46 MMD train -0.0001467 \n",
      "Epoch 66/100 Batch 0/46 MMD train -0.00014597 \n",
      "Epoch 66/100 Batch 45/46 MMD train -0.00016052 \n",
      "Epoch 67/100 Batch 0/46 MMD train -0.00015439 \n",
      "Epoch 67/100 Batch 45/46 MMD train -5.8948e-05 \n",
      "Epoch 68/100 Batch 0/46 MMD train -5.6191e-05 \n",
      "Epoch 68/100 Batch 45/46 MMD train -6.7183e-05 \n",
      "Epoch 69/100 Batch 0/46 MMD train -6.3135e-05 \n",
      "Epoch 69/100 Batch 45/46 MMD train -6.9206e-05 \n",
      "Epoch 70/100 Batch 0/46 MMD train -7.0579e-05 \n",
      "Epoch 70/100 Batch 45/46 MMD train -8.8612e-05 \n",
      "Epoch 71/100 Batch 0/46 MMD train -8.9572e-05 \n",
      "Epoch 71/100 Batch 45/46 MMD train -8.3051e-05 \n",
      "Epoch 72/100 Batch 0/46 MMD train -7.8907e-05 \n",
      "Epoch 72/100 Batch 45/46 MMD train -9.7201e-05 \n",
      "Epoch 73/100 Batch 0/46 MMD train -9.455e-05 \n",
      "Epoch 73/100 Batch 45/46 MMD train -6.4819e-05 \n",
      "Epoch 74/100 Batch 0/46 MMD train -6.523e-05 \n",
      "Epoch 74/100 Batch 45/46 MMD train -3.4063e-05 \n",
      "Epoch 75/100 Batch 0/46 MMD train -3.3504e-05 \n",
      "Epoch 75/100 Batch 45/46 MMD train -1.2797e-05 \n",
      "Epoch 76/100 Batch 0/46 MMD train -1.1146e-05 \n",
      "Epoch 76/100 Batch 45/46 MMD train -2.4288e-05 \n",
      "Epoch 77/100 Batch 0/46 MMD train -3.0137e-05 \n",
      "Epoch 77/100 Batch 45/46 MMD train -3.908e-05 \n",
      "Epoch 78/100 Batch 0/46 MMD train -4.1648e-05 \n",
      "Epoch 78/100 Batch 45/46 MMD train -3.478e-05 \n",
      "Epoch 79/100 Batch 0/46 MMD train -3.2938e-05 \n",
      "Epoch 79/100 Batch 45/46 MMD train 7.405e-05 \n",
      "Epoch 80/100 Batch 0/46 MMD train 8.8213e-05 \n",
      "Epoch 80/100 Batch 45/46 MMD train 9.0622e-05 \n",
      "Epoch 81/100 Batch 0/46 MMD train 9.8728e-05 \n",
      "Epoch 81/100 Batch 45/46 MMD train 8.6657e-05 \n",
      "Epoch 82/100 Batch 0/46 MMD train 8.4745e-05 \n",
      "Epoch 82/100 Batch 45/46 MMD train 9.0709e-05 \n",
      "Epoch 83/100 Batch 0/46 MMD train 9.1085e-05 \n",
      "Epoch 83/100 Batch 45/46 MMD train 5.0833e-05 \n",
      "Epoch 84/100 Batch 0/46 MMD train 4.4037e-05 \n",
      "Epoch 84/100 Batch 45/46 MMD train 2.1158e-05 \n",
      "Epoch 85/100 Batch 0/46 MMD train 2.2114e-05 \n",
      "Epoch 85/100 Batch 45/46 MMD train -1.5497e-05 \n",
      "Epoch 86/100 Batch 0/46 MMD train -1.9863e-05 \n",
      "Epoch 86/100 Batch 45/46 MMD train 4.7256e-06 \n",
      "Epoch 87/100 Batch 0/46 MMD train 3.492e-07 \n",
      "Epoch 87/100 Batch 45/46 MMD train 5.2907e-06 \n",
      "Epoch 88/100 Batch 0/46 MMD train 5.4695e-06 \n",
      "Epoch 88/100 Batch 45/46 MMD train -2.9288e-05 \n",
      "Epoch 89/100 Batch 0/46 MMD train -3.0987e-05 \n",
      "Epoch 89/100 Batch 45/46 MMD train 1.0977e-05 \n",
      "Epoch 90/100 Batch 0/46 MMD train 1.5719e-05 \n",
      "Epoch 90/100 Batch 45/46 MMD train 5.8018e-06 \n",
      "Epoch 91/100 Batch 0/46 MMD train 3.6281e-06 \n",
      "Epoch 91/100 Batch 45/46 MMD train 2.2586e-05 \n",
      "Epoch 92/100 Batch 0/46 MMD train 2.0376e-05 \n",
      "Epoch 92/100 Batch 45/46 MMD train 3.5009e-05 \n",
      "Epoch 93/100 Batch 0/46 MMD train 3.3132e-05 \n",
      "Epoch 93/100 Batch 45/46 MMD train 4.5906e-05 \n",
      "Epoch 94/100 Batch 0/46 MMD train 4.7213e-05 \n",
      "Epoch 94/100 Batch 45/46 MMD train 4.2923e-05 \n",
      "Epoch 95/100 Batch 0/46 MMD train 4.5193e-05 \n",
      "Epoch 95/100 Batch 45/46 MMD train 5.6542e-05 \n",
      "Epoch 96/100 Batch 0/46 MMD train 5.4887e-05 \n",
      "Epoch 96/100 Batch 45/46 MMD train 3.3489e-05 \n",
      "Epoch 97/100 Batch 0/46 MMD train 3.6711e-05 \n",
      "Epoch 97/100 Batch 45/46 MMD train 6.1838e-05 \n",
      "Epoch 98/100 Batch 0/46 MMD train 5.928e-05 \n",
      "Epoch 98/100 Batch 45/46 MMD train 6.04e-05 \n",
      "Epoch 99/100 Batch 0/46 MMD train 6.4515e-05 \n",
      "Epoch 99/100 Batch 45/46 MMD train 8.3783e-05 \n",
      "Epoch 100/100 Batch 0/46 MMD train 8.5071e-05 \n",
      "Epoch 100/100 Batch 45/46 MMD train 0.00011573 \n"
     ]
    }
   ],
   "source": [
    "all_mmds_train, all_mmds_test = compute_mmd(corruptions, joint_kernel, epochs=MMD_Epochs, reference=False)\n",
    "data = {'train': all_mmds_train,\n",
    "       'test': all_mmds_test,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_real_mmd.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/60 MMD train 0.023272 \n",
      "Epoch 1/100 Batch 59/60 MMD train 0.0083118 \n",
      "Epoch 2/100 Batch 0/60 MMD train 0.0084755 \n",
      "Epoch 2/100 Batch 59/60 MMD train 0.0095841 \n",
      "Epoch 3/100 Batch 0/60 MMD train 0.0095954 \n",
      "Epoch 3/100 Batch 59/60 MMD train 0.010367 \n",
      "Epoch 4/100 Batch 0/60 MMD train 0.010405 \n",
      "Epoch 4/100 Batch 59/60 MMD train 0.0098786 \n",
      "Epoch 5/100 Batch 0/60 MMD train 0.0098191 \n",
      "Epoch 5/100 Batch 59/60 MMD train 0.0095885 \n",
      "Epoch 6/100 Batch 0/60 MMD train 0.0095647 \n",
      "Epoch 6/100 Batch 59/60 MMD train 0.0097082 \n",
      "Epoch 7/100 Batch 0/60 MMD train 0.0096997 \n",
      "Epoch 7/100 Batch 59/60 MMD train 0.0099827 \n",
      "Epoch 8/100 Batch 0/60 MMD train 0.010008 \n",
      "Epoch 8/100 Batch 59/60 MMD train 0.01025 \n",
      "Epoch 9/100 Batch 0/60 MMD train 0.010262 \n",
      "Epoch 9/100 Batch 59/60 MMD train 0.010196 \n",
      "Epoch 10/100 Batch 0/60 MMD train 0.010225 \n",
      "Epoch 10/100 Batch 59/60 MMD train 0.0098868 \n",
      "Epoch 11/100 Batch 0/60 MMD train 0.0098717 \n",
      "Epoch 11/100 Batch 59/60 MMD train 0.0099268 \n",
      "Epoch 12/100 Batch 0/60 MMD train 0.0099227 \n",
      "Epoch 12/100 Batch 59/60 MMD train 0.0098748 \n",
      "Epoch 13/100 Batch 0/60 MMD train 0.009857 \n",
      "Epoch 13/100 Batch 59/60 MMD train 0.0095265 \n",
      "Epoch 14/100 Batch 0/60 MMD train 0.00952 \n",
      "Epoch 14/100 Batch 59/60 MMD train 0.0094348 \n",
      "Epoch 15/100 Batch 0/60 MMD train 0.0094312 \n",
      "Epoch 15/100 Batch 59/60 MMD train 0.0094962 \n",
      "Epoch 16/100 Batch 0/60 MMD train 0.0095331 \n",
      "Epoch 16/100 Batch 59/60 MMD train 0.0096079 \n",
      "Epoch 17/100 Batch 0/60 MMD train 0.0096041 \n",
      "Epoch 17/100 Batch 59/60 MMD train 0.0096385 \n",
      "Epoch 18/100 Batch 0/60 MMD train 0.0096356 \n",
      "Epoch 18/100 Batch 59/60 MMD train 0.0097396 \n",
      "Epoch 19/100 Batch 0/60 MMD train 0.0097277 \n",
      "Epoch 19/100 Batch 59/60 MMD train 0.0096782 \n",
      "Epoch 20/100 Batch 0/60 MMD train 0.0096914 \n",
      "Epoch 20/100 Batch 59/60 MMD train 0.0097226 \n",
      "Epoch 21/100 Batch 0/60 MMD train 0.0097287 \n",
      "Epoch 21/100 Batch 59/60 MMD train 0.0099031 \n",
      "Epoch 22/100 Batch 0/60 MMD train 0.0099032 \n",
      "Epoch 22/100 Batch 59/60 MMD train 0.009771 \n",
      "Epoch 23/100 Batch 0/60 MMD train 0.0097769 \n",
      "Epoch 23/100 Batch 59/60 MMD train 0.0097652 \n",
      "Epoch 24/100 Batch 0/60 MMD train 0.0097773 \n",
      "Epoch 24/100 Batch 59/60 MMD train 0.0098723 \n",
      "Epoch 25/100 Batch 0/60 MMD train 0.0098741 \n",
      "Epoch 25/100 Batch 59/60 MMD train 0.009917 \n",
      "Epoch 26/100 Batch 0/60 MMD train 0.0099124 \n",
      "Epoch 26/100 Batch 59/60 MMD train 0.0099495 \n",
      "Epoch 27/100 Batch 0/60 MMD train 0.0099656 \n",
      "Epoch 27/100 Batch 59/60 MMD train 0.0099641 \n",
      "Epoch 28/100 Batch 0/60 MMD train 0.0099656 \n",
      "Epoch 28/100 Batch 59/60 MMD train 0.0099194 \n",
      "Epoch 29/100 Batch 0/60 MMD train 0.0099401 \n",
      "Epoch 29/100 Batch 59/60 MMD train 0.0099082 \n",
      "Epoch 30/100 Batch 0/60 MMD train 0.0099116 \n",
      "Epoch 30/100 Batch 59/60 MMD train 0.0098563 \n",
      "Epoch 31/100 Batch 0/60 MMD train 0.0098568 \n",
      "Epoch 31/100 Batch 59/60 MMD train 0.0099578 \n",
      "Epoch 32/100 Batch 0/60 MMD train 0.0099595 \n",
      "Epoch 32/100 Batch 59/60 MMD train 0.0099633 \n",
      "Epoch 33/100 Batch 0/60 MMD train 0.0099786 \n",
      "Epoch 33/100 Batch 59/60 MMD train 0.010016 \n",
      "Epoch 34/100 Batch 0/60 MMD train 0.010013 \n",
      "Epoch 34/100 Batch 59/60 MMD train 0.010046 \n",
      "Epoch 35/100 Batch 0/60 MMD train 0.010048 \n",
      "Epoch 35/100 Batch 59/60 MMD train 0.010037 \n",
      "Epoch 36/100 Batch 0/60 MMD train 0.010034 \n",
      "Epoch 36/100 Batch 59/60 MMD train 0.01 \n",
      "Epoch 37/100 Batch 0/60 MMD train 0.010003 \n",
      "Epoch 37/100 Batch 59/60 MMD train 0.010051 \n",
      "Epoch 38/100 Batch 0/60 MMD train 0.010057 \n",
      "Epoch 38/100 Batch 59/60 MMD train 0.010004 \n",
      "Epoch 39/100 Batch 0/60 MMD train 0.010002 \n",
      "Epoch 39/100 Batch 59/60 MMD train 0.010004 \n",
      "Epoch 40/100 Batch 0/60 MMD train 0.010008 \n",
      "Epoch 40/100 Batch 59/60 MMD train 0.00997 \n",
      "Epoch 41/100 Batch 0/60 MMD train 0.0099636 \n",
      "Epoch 41/100 Batch 59/60 MMD train 0.0099688 \n",
      "Epoch 42/100 Batch 0/60 MMD train 0.0099661 \n",
      "Epoch 42/100 Batch 59/60 MMD train 0.0099197 \n",
      "Epoch 43/100 Batch 0/60 MMD train 0.0099086 \n",
      "Epoch 43/100 Batch 59/60 MMD train 0.0098962 \n",
      "Epoch 44/100 Batch 0/60 MMD train 0.0098882 \n",
      "Epoch 44/100 Batch 59/60 MMD train 0.0098936 \n",
      "Epoch 45/100 Batch 0/60 MMD train 0.0098872 \n",
      "Epoch 45/100 Batch 59/60 MMD train 0.0098894 \n",
      "Epoch 46/100 Batch 0/60 MMD train 0.0098846 \n",
      "Epoch 46/100 Batch 59/60 MMD train 0.0099464 \n",
      "Epoch 47/100 Batch 0/60 MMD train 0.0099493 \n",
      "Epoch 47/100 Batch 59/60 MMD train 0.0098736 \n",
      "Epoch 48/100 Batch 0/60 MMD train 0.0098797 \n",
      "Epoch 48/100 Batch 59/60 MMD train 0.0098979 \n",
      "Epoch 49/100 Batch 0/60 MMD train 0.0098947 \n",
      "Epoch 49/100 Batch 59/60 MMD train 0.0099531 \n",
      "Epoch 50/100 Batch 0/60 MMD train 0.0099556 \n",
      "Epoch 50/100 Batch 59/60 MMD train 0.0099632 \n",
      "Epoch 51/100 Batch 0/60 MMD train 0.0099645 \n",
      "Epoch 51/100 Batch 59/60 MMD train 0.010008 \n",
      "Epoch 52/100 Batch 0/60 MMD train 0.01001 \n",
      "Epoch 52/100 Batch 59/60 MMD train 0.010007 \n",
      "Epoch 53/100 Batch 0/60 MMD train 0.010004 \n",
      "Epoch 53/100 Batch 59/60 MMD train 0.010067 \n",
      "Epoch 54/100 Batch 0/60 MMD train 0.010074 \n",
      "Epoch 54/100 Batch 59/60 MMD train 0.01012 \n",
      "Epoch 55/100 Batch 0/60 MMD train 0.010121 \n",
      "Epoch 55/100 Batch 59/60 MMD train 0.010166 \n",
      "Epoch 56/100 Batch 0/60 MMD train 0.010166 \n",
      "Epoch 56/100 Batch 59/60 MMD train 0.01016 \n",
      "Epoch 57/100 Batch 0/60 MMD train 0.01016 \n",
      "Epoch 57/100 Batch 59/60 MMD train 0.010175 \n",
      "Epoch 58/100 Batch 0/60 MMD train 0.01018 \n",
      "Epoch 58/100 Batch 59/60 MMD train 0.010177 \n",
      "Epoch 59/100 Batch 0/60 MMD train 0.010179 \n",
      "Epoch 59/100 Batch 59/60 MMD train 0.010209 \n",
      "Epoch 60/100 Batch 0/60 MMD train 0.010205 \n",
      "Epoch 60/100 Batch 59/60 MMD train 0.010203 \n",
      "Epoch 61/100 Batch 0/60 MMD train 0.010195 \n",
      "Epoch 61/100 Batch 59/60 MMD train 0.010142 \n",
      "Epoch 62/100 Batch 0/60 MMD train 0.010139 \n",
      "Epoch 62/100 Batch 59/60 MMD train 0.010079 \n",
      "Epoch 63/100 Batch 0/60 MMD train 0.010078 \n",
      "Epoch 63/100 Batch 59/60 MMD train 0.010067 \n",
      "Epoch 64/100 Batch 0/60 MMD train 0.010074 \n",
      "Epoch 64/100 Batch 59/60 MMD train 0.010099 \n",
      "Epoch 65/100 Batch 0/60 MMD train 0.010101 \n",
      "Epoch 65/100 Batch 59/60 MMD train 0.010124 \n",
      "Epoch 66/100 Batch 0/60 MMD train 0.01012 \n",
      "Epoch 66/100 Batch 59/60 MMD train 0.010095 \n",
      "Epoch 67/100 Batch 0/60 MMD train 0.010094 \n",
      "Epoch 67/100 Batch 59/60 MMD train 0.010056 \n",
      "Epoch 68/100 Batch 0/60 MMD train 0.010053 \n",
      "Epoch 68/100 Batch 59/60 MMD train 0.010045 \n",
      "Epoch 69/100 Batch 0/60 MMD train 0.010047 \n",
      "Epoch 69/100 Batch 59/60 MMD train 0.010054 \n",
      "Epoch 70/100 Batch 0/60 MMD train 0.010053 \n",
      "Epoch 70/100 Batch 59/60 MMD train 0.01008 \n",
      "Epoch 71/100 Batch 0/60 MMD train 0.010081 \n",
      "Epoch 71/100 Batch 59/60 MMD train 0.010024 \n",
      "Epoch 72/100 Batch 0/60 MMD train 0.010026 \n",
      "Epoch 72/100 Batch 59/60 MMD train 0.010052 \n",
      "Epoch 73/100 Batch 0/60 MMD train 0.010054 \n",
      "Epoch 73/100 Batch 59/60 MMD train 0.01005 \n",
      "Epoch 74/100 Batch 0/60 MMD train 0.010053 \n",
      "Epoch 74/100 Batch 59/60 MMD train 0.010056 \n",
      "Epoch 75/100 Batch 0/60 MMD train 0.010058 \n",
      "Epoch 75/100 Batch 59/60 MMD train 0.010051 \n",
      "Epoch 76/100 Batch 0/60 MMD train 0.010051 \n",
      "Epoch 76/100 Batch 59/60 MMD train 0.0099822 \n",
      "Epoch 77/100 Batch 0/60 MMD train 0.009983 \n",
      "Epoch 77/100 Batch 59/60 MMD train 0.009968 \n",
      "Epoch 78/100 Batch 0/60 MMD train 0.0099652 \n",
      "Epoch 78/100 Batch 59/60 MMD train 0.0099955 \n",
      "Epoch 79/100 Batch 0/60 MMD train 0.0099915 \n",
      "Epoch 79/100 Batch 59/60 MMD train 0.0099694 \n",
      "Epoch 80/100 Batch 0/60 MMD train 0.0099678 \n",
      "Epoch 80/100 Batch 59/60 MMD train 0.0099725 \n",
      "Epoch 81/100 Batch 0/60 MMD train 0.0099744 \n",
      "Epoch 81/100 Batch 59/60 MMD train 0.0099767 \n",
      "Epoch 82/100 Batch 0/60 MMD train 0.0099727 \n",
      "Epoch 82/100 Batch 59/60 MMD train 0.0099811 \n",
      "Epoch 83/100 Batch 0/60 MMD train 0.0099763 \n",
      "Epoch 83/100 Batch 59/60 MMD train 0.010003 \n",
      "Epoch 84/100 Batch 0/60 MMD train 0.0099995 \n",
      "Epoch 84/100 Batch 59/60 MMD train 0.009988 \n",
      "Epoch 85/100 Batch 0/60 MMD train 0.0099917 \n",
      "Epoch 85/100 Batch 59/60 MMD train 0.0099883 \n",
      "Epoch 86/100 Batch 0/60 MMD train 0.0099888 \n",
      "Epoch 86/100 Batch 59/60 MMD train 0.010004 \n",
      "Epoch 87/100 Batch 0/60 MMD train 0.010007 \n",
      "Epoch 87/100 Batch 59/60 MMD train 0.0099951 \n",
      "Epoch 88/100 Batch 0/60 MMD train 0.0099895 \n",
      "Epoch 88/100 Batch 59/60 MMD train 0.010014 \n",
      "Epoch 89/100 Batch 0/60 MMD train 0.010015 \n",
      "Epoch 89/100 Batch 59/60 MMD train 0.010042 \n",
      "Epoch 90/100 Batch 0/60 MMD train 0.010042 \n",
      "Epoch 90/100 Batch 59/60 MMD train 0.010027 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 Batch 0/60 MMD train 0.01003 \n",
      "Epoch 91/100 Batch 59/60 MMD train 0.010041 \n",
      "Epoch 92/100 Batch 0/60 MMD train 0.010037 \n",
      "Epoch 92/100 Batch 59/60 MMD train 0.010034 \n",
      "Epoch 93/100 Batch 0/60 MMD train 0.010032 \n",
      "Epoch 93/100 Batch 59/60 MMD train 0.010044 \n",
      "Epoch 94/100 Batch 0/60 MMD train 0.010042 \n",
      "Epoch 94/100 Batch 59/60 MMD train 0.010036 \n",
      "Epoch 95/100 Batch 0/60 MMD train 0.010034 \n",
      "Epoch 95/100 Batch 59/60 MMD train 0.010024 \n",
      "Epoch 96/100 Batch 0/60 MMD train 0.01002 \n",
      "Epoch 96/100 Batch 59/60 MMD train 0.01 \n",
      "Epoch 97/100 Batch 0/60 MMD train 0.010001 \n",
      "Epoch 97/100 Batch 59/60 MMD train 0.0099744 \n",
      "Epoch 98/100 Batch 0/60 MMD train 0.0099705 \n",
      "Epoch 98/100 Batch 59/60 MMD train 0.0099876 \n",
      "Epoch 99/100 Batch 0/60 MMD train 0.0099877 \n",
      "Epoch 99/100 Batch 59/60 MMD train 0.0099644 \n",
      "Epoch 100/100 Batch 0/60 MMD train 0.0099632 \n",
      "Epoch 100/100 Batch 59/60 MMD train 0.009944 \n",
      "Corruption 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/60 MMD train 0.005079 \n",
      "Epoch 1/100 Batch 59/60 MMD train 0.0080873 \n",
      "Epoch 2/100 Batch 0/60 MMD train 0.0084526 \n",
      "Epoch 2/100 Batch 59/60 MMD train 0.0059946 \n",
      "Epoch 3/100 Batch 0/60 MMD train 0.0059222 \n",
      "Epoch 3/100 Batch 59/60 MMD train 0.0061768 \n",
      "Epoch 4/100 Batch 0/60 MMD train 0.0060993 \n",
      "Epoch 4/100 Batch 59/60 MMD train 0.0059621 \n",
      "Epoch 5/100 Batch 0/60 MMD train 0.0060067 \n",
      "Epoch 5/100 Batch 59/60 MMD train 0.005226 \n",
      "Epoch 6/100 Batch 0/60 MMD train 0.0052493 \n",
      "Epoch 6/100 Batch 59/60 MMD train 0.0052913 \n",
      "Epoch 7/100 Batch 0/60 MMD train 0.0053286 \n",
      "Epoch 7/100 Batch 59/60 MMD train 0.005651 \n",
      "Epoch 8/100 Batch 0/60 MMD train 0.0056398 \n",
      "Epoch 8/100 Batch 59/60 MMD train 0.0058332 \n",
      "Epoch 9/100 Batch 0/60 MMD train 0.0057883 \n",
      "Epoch 9/100 Batch 59/60 MMD train 0.0060084 \n",
      "Epoch 10/100 Batch 0/60 MMD train 0.0059913 \n",
      "Epoch 10/100 Batch 59/60 MMD train 0.0059823 \n",
      "Epoch 11/100 Batch 0/60 MMD train 0.0059992 \n",
      "Epoch 11/100 Batch 59/60 MMD train 0.0060129 \n",
      "Epoch 12/100 Batch 0/60 MMD train 0.0059609 \n",
      "Epoch 12/100 Batch 59/60 MMD train 0.0059409 \n",
      "Epoch 13/100 Batch 0/60 MMD train 0.0059518 \n",
      "Epoch 13/100 Batch 59/60 MMD train 0.0060083 \n",
      "Epoch 14/100 Batch 0/60 MMD train 0.0060019 \n",
      "Epoch 14/100 Batch 59/60 MMD train 0.0057841 \n",
      "Epoch 15/100 Batch 0/60 MMD train 0.0058186 \n",
      "Epoch 15/100 Batch 59/60 MMD train 0.0056075 \n",
      "Epoch 16/100 Batch 0/60 MMD train 0.0056519 \n",
      "Epoch 16/100 Batch 59/60 MMD train 0.005824 \n",
      "Epoch 17/100 Batch 0/60 MMD train 0.0058148 \n",
      "Epoch 17/100 Batch 59/60 MMD train 0.0058626 \n",
      "Epoch 18/100 Batch 0/60 MMD train 0.0058749 \n",
      "Epoch 18/100 Batch 59/60 MMD train 0.0058328 \n",
      "Epoch 19/100 Batch 0/60 MMD train 0.0058313 \n",
      "Epoch 19/100 Batch 59/60 MMD train 0.0058238 \n",
      "Epoch 20/100 Batch 0/60 MMD train 0.0058186 \n",
      "Epoch 20/100 Batch 59/60 MMD train 0.0058232 \n",
      "Epoch 21/100 Batch 0/60 MMD train 0.0058309 \n",
      "Epoch 21/100 Batch 59/60 MMD train 0.005803 \n",
      "Epoch 22/100 Batch 0/60 MMD train 0.0057869 \n",
      "Epoch 22/100 Batch 59/60 MMD train 0.005784 \n",
      "Epoch 23/100 Batch 0/60 MMD train 0.0057881 \n",
      "Epoch 23/100 Batch 59/60 MMD train 0.0056899 \n",
      "Epoch 24/100 Batch 0/60 MMD train 0.0056902 \n",
      "Epoch 24/100 Batch 59/60 MMD train 0.0057289 \n",
      "Epoch 25/100 Batch 0/60 MMD train 0.0057382 \n",
      "Epoch 25/100 Batch 59/60 MMD train 0.0057214 \n",
      "Epoch 26/100 Batch 0/60 MMD train 0.0057232 \n",
      "Epoch 26/100 Batch 59/60 MMD train 0.0058149 \n",
      "Epoch 27/100 Batch 0/60 MMD train 0.0058036 \n",
      "Epoch 27/100 Batch 59/60 MMD train 0.0057982 \n",
      "Epoch 28/100 Batch 0/60 MMD train 0.0057988 \n",
      "Epoch 28/100 Batch 59/60 MMD train 0.0058329 \n",
      "Epoch 29/100 Batch 0/60 MMD train 0.0058207 \n",
      "Epoch 29/100 Batch 59/60 MMD train 0.0059189 \n",
      "Epoch 30/100 Batch 0/60 MMD train 0.005904 \n",
      "Epoch 30/100 Batch 59/60 MMD train 0.0058948 \n",
      "Epoch 31/100 Batch 0/60 MMD train 0.0059004 \n",
      "Epoch 31/100 Batch 59/60 MMD train 0.0058571 \n",
      "Epoch 32/100 Batch 0/60 MMD train 0.0058622 \n",
      "Epoch 32/100 Batch 59/60 MMD train 0.0058859 \n",
      "Epoch 33/100 Batch 0/60 MMD train 0.0058898 \n",
      "Epoch 33/100 Batch 59/60 MMD train 0.0059186 \n",
      "Epoch 34/100 Batch 0/60 MMD train 0.0059269 \n",
      "Epoch 34/100 Batch 59/60 MMD train 0.0060241 \n",
      "Epoch 35/100 Batch 0/60 MMD train 0.0060186 \n",
      "Epoch 35/100 Batch 59/60 MMD train 0.0061267 \n",
      "Epoch 36/100 Batch 0/60 MMD train 0.0061306 \n",
      "Epoch 36/100 Batch 59/60 MMD train 0.0061304 \n",
      "Epoch 37/100 Batch 0/60 MMD train 0.0061264 \n",
      "Epoch 37/100 Batch 59/60 MMD train 0.0060816 \n",
      "Epoch 38/100 Batch 0/60 MMD train 0.0060783 \n",
      "Epoch 38/100 Batch 59/60 MMD train 0.006106 \n",
      "Epoch 39/100 Batch 0/60 MMD train 0.0060973 \n",
      "Epoch 39/100 Batch 59/60 MMD train 0.0060896 \n",
      "Epoch 40/100 Batch 0/60 MMD train 0.0060921 \n",
      "Epoch 40/100 Batch 59/60 MMD train 0.0060807 \n",
      "Epoch 41/100 Batch 0/60 MMD train 0.0060834 \n",
      "Epoch 41/100 Batch 59/60 MMD train 0.0060616 \n",
      "Epoch 42/100 Batch 0/60 MMD train 0.006066 \n",
      "Epoch 42/100 Batch 59/60 MMD train 0.0061079 \n",
      "Epoch 43/100 Batch 0/60 MMD train 0.0061149 \n",
      "Epoch 43/100 Batch 59/60 MMD train 0.0061171 \n",
      "Epoch 44/100 Batch 0/60 MMD train 0.0061183 \n",
      "Epoch 44/100 Batch 59/60 MMD train 0.0060829 \n",
      "Epoch 45/100 Batch 0/60 MMD train 0.0060876 \n",
      "Epoch 45/100 Batch 59/60 MMD train 0.0060399 \n",
      "Epoch 46/100 Batch 0/60 MMD train 0.0060474 \n",
      "Epoch 46/100 Batch 59/60 MMD train 0.0060478 \n",
      "Epoch 47/100 Batch 0/60 MMD train 0.0060522 \n",
      "Epoch 47/100 Batch 59/60 MMD train 0.005981 \n",
      "Epoch 48/100 Batch 0/60 MMD train 0.0059803 \n",
      "Epoch 48/100 Batch 59/60 MMD train 0.0059574 \n",
      "Epoch 49/100 Batch 0/60 MMD train 0.0059608 \n",
      "Epoch 49/100 Batch 59/60 MMD train 0.0059921 \n",
      "Epoch 50/100 Batch 0/60 MMD train 0.0059985 \n",
      "Epoch 50/100 Batch 59/60 MMD train 0.0059312 \n",
      "Epoch 51/100 Batch 0/60 MMD train 0.0059334 \n",
      "Epoch 51/100 Batch 59/60 MMD train 0.0059074 \n",
      "Epoch 52/100 Batch 0/60 MMD train 0.0059067 \n",
      "Epoch 52/100 Batch 59/60 MMD train 0.005939 \n",
      "Epoch 53/100 Batch 0/60 MMD train 0.005937 \n",
      "Epoch 53/100 Batch 59/60 MMD train 0.0058884 \n",
      "Epoch 54/100 Batch 0/60 MMD train 0.0058808 \n",
      "Epoch 54/100 Batch 59/60 MMD train 0.0058889 \n",
      "Epoch 55/100 Batch 0/60 MMD train 0.0058918 \n",
      "Epoch 55/100 Batch 59/60 MMD train 0.0059246 \n",
      "Epoch 56/100 Batch 0/60 MMD train 0.0059263 \n",
      "Epoch 56/100 Batch 59/60 MMD train 0.0059295 \n",
      "Epoch 57/100 Batch 0/60 MMD train 0.0059314 \n",
      "Epoch 57/100 Batch 59/60 MMD train 0.005943 \n",
      "Epoch 58/100 Batch 0/60 MMD train 0.0059506 \n",
      "Epoch 58/100 Batch 59/60 MMD train 0.0059667 \n",
      "Epoch 59/100 Batch 0/60 MMD train 0.0059586 \n",
      "Epoch 59/100 Batch 59/60 MMD train 0.0059486 \n",
      "Epoch 60/100 Batch 0/60 MMD train 0.0059428 \n",
      "Epoch 60/100 Batch 59/60 MMD train 0.0059677 \n",
      "Epoch 61/100 Batch 0/60 MMD train 0.0059714 \n",
      "Epoch 61/100 Batch 59/60 MMD train 0.0060051 \n",
      "Epoch 62/100 Batch 0/60 MMD train 0.0060063 \n",
      "Epoch 62/100 Batch 59/60 MMD train 0.005989 \n",
      "Epoch 63/100 Batch 0/60 MMD train 0.0059825 \n",
      "Epoch 63/100 Batch 59/60 MMD train 0.0058941 \n",
      "Epoch 64/100 Batch 0/60 MMD train 0.0058963 \n",
      "Epoch 64/100 Batch 59/60 MMD train 0.0058691 \n",
      "Epoch 65/100 Batch 0/60 MMD train 0.0058681 \n",
      "Epoch 65/100 Batch 59/60 MMD train 0.0058275 \n",
      "Epoch 66/100 Batch 0/60 MMD train 0.0058237 \n",
      "Epoch 66/100 Batch 59/60 MMD train 0.0058231 \n",
      "Epoch 67/100 Batch 0/60 MMD train 0.0058228 \n",
      "Epoch 67/100 Batch 59/60 MMD train 0.005828 \n",
      "Epoch 68/100 Batch 0/60 MMD train 0.0058296 \n",
      "Epoch 68/100 Batch 59/60 MMD train 0.0057966 \n",
      "Epoch 69/100 Batch 0/60 MMD train 0.0058005 \n",
      "Epoch 69/100 Batch 59/60 MMD train 0.0058019 \n",
      "Epoch 70/100 Batch 0/60 MMD train 0.0057991 \n",
      "Epoch 70/100 Batch 59/60 MMD train 0.0058285 \n",
      "Epoch 71/100 Batch 0/60 MMD train 0.0058343 \n",
      "Epoch 71/100 Batch 59/60 MMD train 0.0057972 \n",
      "Epoch 72/100 Batch 0/60 MMD train 0.0058023 \n",
      "Epoch 72/100 Batch 59/60 MMD train 0.0058269 \n",
      "Epoch 73/100 Batch 0/60 MMD train 0.0058214 \n",
      "Epoch 73/100 Batch 59/60 MMD train 0.0058654 \n",
      "Epoch 74/100 Batch 0/60 MMD train 0.0058662 \n",
      "Epoch 74/100 Batch 59/60 MMD train 0.0058648 \n",
      "Epoch 75/100 Batch 0/60 MMD train 0.0058641 \n",
      "Epoch 75/100 Batch 59/60 MMD train 0.0058819 \n",
      "Epoch 76/100 Batch 0/60 MMD train 0.0058901 \n",
      "Epoch 76/100 Batch 59/60 MMD train 0.0058698 \n",
      "Epoch 77/100 Batch 0/60 MMD train 0.0058688 \n",
      "Epoch 77/100 Batch 59/60 MMD train 0.0058417 \n",
      "Epoch 78/100 Batch 0/60 MMD train 0.0058443 \n",
      "Epoch 78/100 Batch 59/60 MMD train 0.0058367 \n",
      "Epoch 79/100 Batch 0/60 MMD train 0.0058419 \n",
      "Epoch 79/100 Batch 59/60 MMD train 0.005843 \n",
      "Epoch 80/100 Batch 0/60 MMD train 0.0058425 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 Batch 59/60 MMD train 0.0058564 \n",
      "Epoch 81/100 Batch 0/60 MMD train 0.0058574 \n",
      "Epoch 81/100 Batch 59/60 MMD train 0.0058511 \n",
      "Epoch 82/100 Batch 0/60 MMD train 0.0058524 \n",
      "Epoch 82/100 Batch 59/60 MMD train 0.0058387 \n",
      "Epoch 83/100 Batch 0/60 MMD train 0.005839 \n",
      "Epoch 83/100 Batch 59/60 MMD train 0.0058393 \n",
      "Epoch 84/100 Batch 0/60 MMD train 0.005841 \n",
      "Epoch 84/100 Batch 59/60 MMD train 0.0058501 \n",
      "Epoch 85/100 Batch 0/60 MMD train 0.0058472 \n",
      "Epoch 85/100 Batch 59/60 MMD train 0.0058276 \n",
      "Epoch 86/100 Batch 0/60 MMD train 0.0058262 \n",
      "Epoch 86/100 Batch 59/60 MMD train 0.0058591 \n",
      "Epoch 87/100 Batch 0/60 MMD train 0.0058621 \n",
      "Epoch 87/100 Batch 59/60 MMD train 0.0058322 \n",
      "Epoch 88/100 Batch 0/60 MMD train 0.0058313 \n",
      "Epoch 88/100 Batch 59/60 MMD train 0.0058113 \n",
      "Epoch 89/100 Batch 0/60 MMD train 0.0058159 \n",
      "Epoch 89/100 Batch 59/60 MMD train 0.0058113 \n",
      "Epoch 90/100 Batch 0/60 MMD train 0.0058115 \n",
      "Epoch 90/100 Batch 59/60 MMD train 0.005838 \n",
      "Epoch 91/100 Batch 0/60 MMD train 0.0058399 \n",
      "Epoch 91/100 Batch 59/60 MMD train 0.0058306 \n",
      "Epoch 92/100 Batch 0/60 MMD train 0.0058253 \n",
      "Epoch 92/100 Batch 59/60 MMD train 0.0058164 \n",
      "Epoch 93/100 Batch 0/60 MMD train 0.0058142 \n",
      "Epoch 93/100 Batch 59/60 MMD train 0.0058824 \n",
      "Epoch 94/100 Batch 0/60 MMD train 0.0058831 \n",
      "Epoch 94/100 Batch 59/60 MMD train 0.0059058 \n",
      "Epoch 95/100 Batch 0/60 MMD train 0.0059066 \n",
      "Epoch 95/100 Batch 59/60 MMD train 0.0059204 \n",
      "Epoch 96/100 Batch 0/60 MMD train 0.005926 \n",
      "Epoch 96/100 Batch 59/60 MMD train 0.0058986 \n",
      "Epoch 97/100 Batch 0/60 MMD train 0.0058971 \n",
      "Epoch 97/100 Batch 59/60 MMD train 0.0059055 \n",
      "Epoch 98/100 Batch 0/60 MMD train 0.0059024 \n",
      "Epoch 98/100 Batch 59/60 MMD train 0.0058829 \n",
      "Epoch 99/100 Batch 0/60 MMD train 0.0058864 \n",
      "Epoch 99/100 Batch 59/60 MMD train 0.0059084 \n",
      "Epoch 100/100 Batch 0/60 MMD train 0.0059042 \n",
      "Epoch 100/100 Batch 59/60 MMD train 0.0059036 \n",
      "Corruption 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/60 MMD train -0.012155 \n",
      "Epoch 1/100 Batch 59/60 MMD train 0.0021627 \n",
      "Epoch 2/100 Batch 0/60 MMD train 0.0022907 \n",
      "Epoch 2/100 Batch 59/60 MMD train 0.0024926 \n",
      "Epoch 3/100 Batch 0/60 MMD train 0.0024482 \n",
      "Epoch 3/100 Batch 59/60 MMD train 0.0020308 \n",
      "Epoch 4/100 Batch 0/60 MMD train 0.0020922 \n",
      "Epoch 4/100 Batch 59/60 MMD train 0.00144 \n",
      "Epoch 5/100 Batch 0/60 MMD train 0.001465 \n",
      "Epoch 5/100 Batch 59/60 MMD train 0.0021244 \n",
      "Epoch 6/100 Batch 0/60 MMD train 0.0020838 \n",
      "Epoch 6/100 Batch 59/60 MMD train 0.0019696 \n",
      "Epoch 7/100 Batch 0/60 MMD train 0.0019597 \n",
      "Epoch 7/100 Batch 59/60 MMD train 0.0019897 \n",
      "Epoch 8/100 Batch 0/60 MMD train 0.0020205 \n",
      "Epoch 8/100 Batch 59/60 MMD train 0.0020142 \n",
      "Epoch 9/100 Batch 0/60 MMD train 0.0020229 \n",
      "Epoch 9/100 Batch 59/60 MMD train 0.002016 \n",
      "Epoch 10/100 Batch 0/60 MMD train 0.002035 \n",
      "Epoch 10/100 Batch 59/60 MMD train 0.0021775 \n",
      "Epoch 11/100 Batch 0/60 MMD train 0.0022001 \n",
      "Epoch 11/100 Batch 59/60 MMD train 0.0018326 \n",
      "Epoch 12/100 Batch 0/60 MMD train 0.001837 \n",
      "Epoch 12/100 Batch 59/60 MMD train 0.0021758 \n",
      "Epoch 13/100 Batch 0/60 MMD train 0.0021843 \n",
      "Epoch 13/100 Batch 59/60 MMD train 0.0021387 \n",
      "Epoch 14/100 Batch 0/60 MMD train 0.0021409 \n",
      "Epoch 14/100 Batch 59/60 MMD train 0.0022291 \n",
      "Epoch 15/100 Batch 0/60 MMD train 0.0022276 \n",
      "Epoch 15/100 Batch 59/60 MMD train 0.002249 \n",
      "Epoch 16/100 Batch 0/60 MMD train 0.0022582 \n",
      "Epoch 16/100 Batch 59/60 MMD train 0.0024291 \n",
      "Epoch 17/100 Batch 0/60 MMD train 0.0024325 \n",
      "Epoch 17/100 Batch 59/60 MMD train 0.0025188 \n",
      "Epoch 18/100 Batch 0/60 MMD train 0.002533 \n",
      "Epoch 18/100 Batch 59/60 MMD train 0.0025453 \n",
      "Epoch 19/100 Batch 0/60 MMD train 0.0025123 \n",
      "Epoch 19/100 Batch 59/60 MMD train 0.0024269 \n",
      "Epoch 20/100 Batch 0/60 MMD train 0.0024327 \n",
      "Epoch 20/100 Batch 59/60 MMD train 0.0025136 \n",
      "Epoch 21/100 Batch 0/60 MMD train 0.0025273 \n",
      "Epoch 21/100 Batch 59/60 MMD train 0.0026418 \n",
      "Epoch 22/100 Batch 0/60 MMD train 0.0026442 \n",
      "Epoch 22/100 Batch 59/60 MMD train 0.0026512 \n",
      "Epoch 23/100 Batch 0/60 MMD train 0.0026448 \n",
      "Epoch 23/100 Batch 59/60 MMD train 0.0026366 \n",
      "Epoch 24/100 Batch 0/60 MMD train 0.002649 \n",
      "Epoch 24/100 Batch 59/60 MMD train 0.0026551 \n",
      "Epoch 25/100 Batch 0/60 MMD train 0.002636 \n",
      "Epoch 25/100 Batch 59/60 MMD train 0.0026397 \n",
      "Epoch 26/100 Batch 0/60 MMD train 0.0026495 \n",
      "Epoch 26/100 Batch 59/60 MMD train 0.002737 \n",
      "Epoch 27/100 Batch 0/60 MMD train 0.0027381 \n",
      "Epoch 27/100 Batch 59/60 MMD train 0.0027422 \n",
      "Epoch 28/100 Batch 0/60 MMD train 0.0027461 \n",
      "Epoch 28/100 Batch 59/60 MMD train 0.0026429 \n",
      "Epoch 29/100 Batch 0/60 MMD train 0.0026518 \n",
      "Epoch 29/100 Batch 59/60 MMD train 0.0025277 \n",
      "Epoch 30/100 Batch 0/60 MMD train 0.0025462 \n",
      "Epoch 30/100 Batch 59/60 MMD train 0.0025857 \n",
      "Epoch 31/100 Batch 0/60 MMD train 0.0025839 \n",
      "Epoch 31/100 Batch 59/60 MMD train 0.0025869 \n",
      "Epoch 32/100 Batch 0/60 MMD train 0.002578 \n",
      "Epoch 32/100 Batch 59/60 MMD train 0.0025891 \n",
      "Epoch 33/100 Batch 0/60 MMD train 0.0026021 \n",
      "Epoch 33/100 Batch 59/60 MMD train 0.0025162 \n",
      "Epoch 34/100 Batch 0/60 MMD train 0.0025192 \n",
      "Epoch 34/100 Batch 59/60 MMD train 0.0024887 \n",
      "Epoch 35/100 Batch 0/60 MMD train 0.0024916 \n",
      "Epoch 35/100 Batch 59/60 MMD train 0.002578 \n",
      "Epoch 36/100 Batch 0/60 MMD train 0.0025699 \n",
      "Epoch 36/100 Batch 59/60 MMD train 0.0025604 \n",
      "Epoch 37/100 Batch 0/60 MMD train 0.0025532 \n",
      "Epoch 37/100 Batch 59/60 MMD train 0.0026237 \n",
      "Epoch 38/100 Batch 0/60 MMD train 0.0026187 \n",
      "Epoch 38/100 Batch 59/60 MMD train 0.0025788 \n",
      "Epoch 39/100 Batch 0/60 MMD train 0.0025849 \n",
      "Epoch 39/100 Batch 59/60 MMD train 0.0025679 \n",
      "Epoch 40/100 Batch 0/60 MMD train 0.0025717 \n",
      "Epoch 40/100 Batch 59/60 MMD train 0.002555 \n",
      "Epoch 41/100 Batch 0/60 MMD train 0.0025426 \n",
      "Epoch 41/100 Batch 59/60 MMD train 0.0025334 \n",
      "Epoch 42/100 Batch 0/60 MMD train 0.0025551 \n",
      "Epoch 42/100 Batch 59/60 MMD train 0.0025636 \n",
      "Epoch 43/100 Batch 0/60 MMD train 0.0025696 \n",
      "Epoch 43/100 Batch 59/60 MMD train 0.0025236 \n",
      "Epoch 44/100 Batch 0/60 MMD train 0.0025329 \n",
      "Epoch 44/100 Batch 59/60 MMD train 0.0025726 \n",
      "Epoch 45/100 Batch 0/60 MMD train 0.0025728 \n",
      "Epoch 45/100 Batch 59/60 MMD train 0.0025908 \n",
      "Epoch 46/100 Batch 0/60 MMD train 0.0025857 \n",
      "Epoch 46/100 Batch 59/60 MMD train 0.0026177 \n",
      "Epoch 47/100 Batch 0/60 MMD train 0.0026186 \n",
      "Epoch 47/100 Batch 59/60 MMD train 0.0026726 \n",
      "Epoch 48/100 Batch 0/60 MMD train 0.002669 \n",
      "Epoch 48/100 Batch 59/60 MMD train 0.0025717 \n",
      "Epoch 49/100 Batch 0/60 MMD train 0.0025731 \n",
      "Epoch 49/100 Batch 59/60 MMD train 0.0026005 \n",
      "Epoch 50/100 Batch 0/60 MMD train 0.0026003 \n",
      "Epoch 50/100 Batch 59/60 MMD train 0.0024973 \n",
      "Epoch 51/100 Batch 0/60 MMD train 0.0024973 \n",
      "Epoch 51/100 Batch 59/60 MMD train 0.0024387 \n",
      "Epoch 52/100 Batch 0/60 MMD train 0.0024307 \n",
      "Epoch 52/100 Batch 59/60 MMD train 0.0024204 \n",
      "Epoch 53/100 Batch 0/60 MMD train 0.0024177 \n",
      "Epoch 53/100 Batch 59/60 MMD train 0.0024092 \n",
      "Epoch 54/100 Batch 0/60 MMD train 0.0024106 \n",
      "Epoch 54/100 Batch 59/60 MMD train 0.0023242 \n",
      "Epoch 55/100 Batch 0/60 MMD train 0.0023297 \n",
      "Epoch 55/100 Batch 59/60 MMD train 0.0023003 \n",
      "Epoch 56/100 Batch 0/60 MMD train 0.0022953 \n",
      "Epoch 56/100 Batch 59/60 MMD train 0.0022905 \n",
      "Epoch 57/100 Batch 0/60 MMD train 0.002286 \n",
      "Epoch 57/100 Batch 59/60 MMD train 0.0023013 \n",
      "Epoch 58/100 Batch 0/60 MMD train 0.0023007 \n",
      "Epoch 58/100 Batch 59/60 MMD train 0.0023245 \n",
      "Epoch 59/100 Batch 0/60 MMD train 0.0023271 \n",
      "Epoch 59/100 Batch 59/60 MMD train 0.002349 \n",
      "Epoch 60/100 Batch 0/60 MMD train 0.0023492 \n",
      "Epoch 60/100 Batch 59/60 MMD train 0.0023666 \n",
      "Epoch 61/100 Batch 0/60 MMD train 0.0023681 \n",
      "Epoch 61/100 Batch 59/60 MMD train 0.002357 \n",
      "Epoch 62/100 Batch 0/60 MMD train 0.0023568 \n",
      "Epoch 62/100 Batch 59/60 MMD train 0.0023536 \n",
      "Epoch 63/100 Batch 0/60 MMD train 0.0023535 \n",
      "Epoch 63/100 Batch 59/60 MMD train 0.0023528 \n",
      "Epoch 64/100 Batch 0/60 MMD train 0.0023509 \n",
      "Epoch 64/100 Batch 59/60 MMD train 0.0023261 \n",
      "Epoch 65/100 Batch 0/60 MMD train 0.0023305 \n",
      "Epoch 65/100 Batch 59/60 MMD train 0.0023132 \n",
      "Epoch 66/100 Batch 0/60 MMD train 0.0023158 \n",
      "Epoch 66/100 Batch 59/60 MMD train 0.0023212 \n",
      "Epoch 67/100 Batch 0/60 MMD train 0.0023258 \n",
      "Epoch 67/100 Batch 59/60 MMD train 0.002344 \n",
      "Epoch 68/100 Batch 0/60 MMD train 0.0023437 \n",
      "Epoch 68/100 Batch 59/60 MMD train 0.002362 \n",
      "Epoch 69/100 Batch 0/60 MMD train 0.002368 \n",
      "Epoch 69/100 Batch 59/60 MMD train 0.0023816 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 Batch 0/60 MMD train 0.0023814 \n",
      "Epoch 70/100 Batch 59/60 MMD train 0.0023325 \n",
      "Epoch 71/100 Batch 0/60 MMD train 0.002328 \n",
      "Epoch 71/100 Batch 59/60 MMD train 0.0023405 \n",
      "Epoch 72/100 Batch 0/60 MMD train 0.0023411 \n",
      "Epoch 72/100 Batch 59/60 MMD train 0.0023571 \n",
      "Epoch 73/100 Batch 0/60 MMD train 0.0023593 \n",
      "Epoch 73/100 Batch 59/60 MMD train 0.0023512 \n",
      "Epoch 74/100 Batch 0/60 MMD train 0.0023593 \n",
      "Epoch 74/100 Batch 59/60 MMD train 0.0023906 \n",
      "Epoch 75/100 Batch 0/60 MMD train 0.0023929 \n",
      "Epoch 75/100 Batch 59/60 MMD train 0.0023412 \n",
      "Epoch 76/100 Batch 0/60 MMD train 0.0023395 \n",
      "Epoch 76/100 Batch 59/60 MMD train 0.002355 \n",
      "Epoch 77/100 Batch 0/60 MMD train 0.0023602 \n",
      "Epoch 77/100 Batch 59/60 MMD train 0.0023793 \n",
      "Epoch 78/100 Batch 0/60 MMD train 0.0023747 \n",
      "Epoch 78/100 Batch 59/60 MMD train 0.0023916 \n",
      "Epoch 79/100 Batch 0/60 MMD train 0.0023925 \n",
      "Epoch 79/100 Batch 59/60 MMD train 0.0024144 \n",
      "Epoch 80/100 Batch 0/60 MMD train 0.0024087 \n",
      "Epoch 80/100 Batch 59/60 MMD train 0.0024036 \n",
      "Epoch 81/100 Batch 0/60 MMD train 0.0024053 \n",
      "Epoch 81/100 Batch 59/60 MMD train 0.0024249 \n",
      "Epoch 82/100 Batch 0/60 MMD train 0.0024282 \n",
      "Epoch 82/100 Batch 59/60 MMD train 0.0024256 \n",
      "Epoch 83/100 Batch 0/60 MMD train 0.0024214 \n",
      "Epoch 83/100 Batch 59/60 MMD train 0.0023886 \n",
      "Epoch 84/100 Batch 0/60 MMD train 0.0023916 \n",
      "Epoch 84/100 Batch 59/60 MMD train 0.0024059 \n",
      "Epoch 85/100 Batch 0/60 MMD train 0.002408 \n",
      "Epoch 85/100 Batch 59/60 MMD train 0.0024013 \n",
      "Epoch 86/100 Batch 0/60 MMD train 0.0023981 \n",
      "Epoch 86/100 Batch 59/60 MMD train 0.0024 \n",
      "Epoch 87/100 Batch 0/60 MMD train 0.0024063 \n",
      "Epoch 87/100 Batch 59/60 MMD train 0.0023905 \n",
      "Epoch 88/100 Batch 0/60 MMD train 0.0023898 \n",
      "Epoch 88/100 Batch 59/60 MMD train 0.0024093 \n",
      "Epoch 89/100 Batch 0/60 MMD train 0.0024092 \n",
      "Epoch 89/100 Batch 59/60 MMD train 0.0024193 \n",
      "Epoch 90/100 Batch 0/60 MMD train 0.0024168 \n",
      "Epoch 90/100 Batch 59/60 MMD train 0.0024573 \n",
      "Epoch 91/100 Batch 0/60 MMD train 0.0024568 \n",
      "Epoch 91/100 Batch 59/60 MMD train 0.0024628 \n",
      "Epoch 92/100 Batch 0/60 MMD train 0.0024668 \n",
      "Epoch 92/100 Batch 59/60 MMD train 0.0024796 \n",
      "Epoch 93/100 Batch 0/60 MMD train 0.0024771 \n",
      "Epoch 93/100 Batch 59/60 MMD train 0.0024758 \n",
      "Epoch 94/100 Batch 0/60 MMD train 0.0024749 \n",
      "Epoch 94/100 Batch 59/60 MMD train 0.0024825 \n",
      "Epoch 95/100 Batch 0/60 MMD train 0.0024829 \n",
      "Epoch 95/100 Batch 59/60 MMD train 0.0024647 \n",
      "Epoch 96/100 Batch 0/60 MMD train 0.0024674 \n",
      "Epoch 96/100 Batch 59/60 MMD train 0.0025069 \n",
      "Epoch 97/100 Batch 0/60 MMD train 0.0025075 \n",
      "Epoch 97/100 Batch 59/60 MMD train 0.0025025 \n",
      "Epoch 98/100 Batch 0/60 MMD train 0.0025015 \n",
      "Epoch 98/100 Batch 59/60 MMD train 0.0025456 \n",
      "Epoch 99/100 Batch 0/60 MMD train 0.0025466 \n",
      "Epoch 99/100 Batch 59/60 MMD train 0.0025255 \n",
      "Epoch 100/100 Batch 0/60 MMD train 0.0025265 \n",
      "Epoch 100/100 Batch 59/60 MMD train 0.0025167 \n",
      "Corruption 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/60 MMD train -0.015669 \n",
      "Epoch 1/100 Batch 59/60 MMD train -0.00106 \n",
      "Epoch 2/100 Batch 0/60 MMD train -0.0014951 \n",
      "Epoch 2/100 Batch 59/60 MMD train -0.0013125 \n",
      "Epoch 3/100 Batch 0/60 MMD train -0.0012912 \n",
      "Epoch 3/100 Batch 59/60 MMD train -0.0012423 \n",
      "Epoch 4/100 Batch 0/60 MMD train -0.0012362 \n",
      "Epoch 4/100 Batch 59/60 MMD train -0.00068627 \n",
      "Epoch 5/100 Batch 0/60 MMD train -0.00062792 \n",
      "Epoch 5/100 Batch 59/60 MMD train -0.0002648 \n",
      "Epoch 6/100 Batch 0/60 MMD train -0.00033834 \n",
      "Epoch 6/100 Batch 59/60 MMD train -0.00015258 \n",
      "Epoch 7/100 Batch 0/60 MMD train -0.00015875 \n",
      "Epoch 7/100 Batch 59/60 MMD train 0.00045504 \n",
      "Epoch 8/100 Batch 0/60 MMD train 0.00049292 \n",
      "Epoch 8/100 Batch 59/60 MMD train 0.000832 \n",
      "Epoch 9/100 Batch 0/60 MMD train 0.00085396 \n",
      "Epoch 9/100 Batch 59/60 MMD train 0.00069649 \n",
      "Epoch 10/100 Batch 0/60 MMD train 0.00073487 \n",
      "Epoch 10/100 Batch 59/60 MMD train 0.00068706 \n",
      "Epoch 11/100 Batch 0/60 MMD train 0.00066957 \n",
      "Epoch 11/100 Batch 59/60 MMD train 0.00057893 \n",
      "Epoch 12/100 Batch 0/60 MMD train 0.0005887 \n",
      "Epoch 12/100 Batch 59/60 MMD train 0.00055739 \n",
      "Epoch 13/100 Batch 0/60 MMD train 0.00056844 \n",
      "Epoch 13/100 Batch 59/60 MMD train 0.00056849 \n",
      "Epoch 14/100 Batch 0/60 MMD train 0.00059405 \n",
      "Epoch 14/100 Batch 59/60 MMD train 0.00039086 \n",
      "Epoch 15/100 Batch 0/60 MMD train 0.00038218 \n",
      "Epoch 15/100 Batch 59/60 MMD train 0.00051165 \n",
      "Epoch 16/100 Batch 0/60 MMD train 0.00052587 \n",
      "Epoch 16/100 Batch 59/60 MMD train 0.00060593 \n",
      "Epoch 17/100 Batch 0/60 MMD train 0.00062501 \n",
      "Epoch 17/100 Batch 59/60 MMD train 0.0006419 \n",
      "Epoch 18/100 Batch 0/60 MMD train 0.00064917 \n",
      "Epoch 18/100 Batch 59/60 MMD train 0.00054937 \n",
      "Epoch 19/100 Batch 0/60 MMD train 0.00053507 \n",
      "Epoch 19/100 Batch 59/60 MMD train 0.00059518 \n",
      "Epoch 20/100 Batch 0/60 MMD train 0.00061512 \n",
      "Epoch 20/100 Batch 59/60 MMD train 0.00061838 \n",
      "Epoch 21/100 Batch 0/60 MMD train 0.00058906 \n",
      "Epoch 21/100 Batch 59/60 MMD train 0.00061177 \n",
      "Epoch 22/100 Batch 0/60 MMD train 0.00062363 \n",
      "Epoch 22/100 Batch 59/60 MMD train 0.00057956 \n",
      "Epoch 23/100 Batch 0/60 MMD train 0.00060673 \n",
      "Epoch 23/100 Batch 59/60 MMD train 0.00054549 \n",
      "Epoch 24/100 Batch 0/60 MMD train 0.00055558 \n",
      "Epoch 24/100 Batch 59/60 MMD train 0.00064609 \n",
      "Epoch 25/100 Batch 0/60 MMD train 0.00063938 \n",
      "Epoch 25/100 Batch 59/60 MMD train 0.0006427 \n",
      "Epoch 26/100 Batch 0/60 MMD train 0.00063664 \n",
      "Epoch 26/100 Batch 59/60 MMD train 0.00058638 \n",
      "Epoch 27/100 Batch 0/60 MMD train 0.00059186 \n",
      "Epoch 27/100 Batch 59/60 MMD train 0.00049511 \n",
      "Epoch 28/100 Batch 0/60 MMD train 0.00050435 \n",
      "Epoch 28/100 Batch 59/60 MMD train 0.00046992 \n",
      "Epoch 29/100 Batch 0/60 MMD train 0.00045359 \n",
      "Epoch 29/100 Batch 59/60 MMD train 0.00036857 \n",
      "Epoch 30/100 Batch 0/60 MMD train 0.00037294 \n",
      "Epoch 30/100 Batch 59/60 MMD train 0.00041183 \n",
      "Epoch 31/100 Batch 0/60 MMD train 0.00041774 \n",
      "Epoch 31/100 Batch 59/60 MMD train 0.00037651 \n",
      "Epoch 32/100 Batch 0/60 MMD train 0.00037482 \n",
      "Epoch 32/100 Batch 59/60 MMD train 0.0002374 \n",
      "Epoch 33/100 Batch 0/60 MMD train 0.00024328 \n",
      "Epoch 33/100 Batch 59/60 MMD train 0.00028299 \n",
      "Epoch 34/100 Batch 0/60 MMD train 0.00027597 \n",
      "Epoch 34/100 Batch 59/60 MMD train 0.0002973 \n",
      "Epoch 35/100 Batch 0/60 MMD train 0.0002896 \n",
      "Epoch 35/100 Batch 59/60 MMD train 0.00025874 \n",
      "Epoch 36/100 Batch 0/60 MMD train 0.00025595 \n",
      "Epoch 36/100 Batch 59/60 MMD train 0.00024423 \n",
      "Epoch 37/100 Batch 0/60 MMD train 0.00023373 \n",
      "Epoch 37/100 Batch 59/60 MMD train 0.00028309 \n",
      "Epoch 38/100 Batch 0/60 MMD train 0.00029191 \n",
      "Epoch 38/100 Batch 59/60 MMD train 0.0003344 \n",
      "Epoch 39/100 Batch 0/60 MMD train 0.00034525 \n",
      "Epoch 39/100 Batch 59/60 MMD train 0.00035282 \n",
      "Epoch 40/100 Batch 0/60 MMD train 0.0003552 \n",
      "Epoch 40/100 Batch 59/60 MMD train 0.00028482 \n",
      "Epoch 41/100 Batch 0/60 MMD train 0.00028875 \n",
      "Epoch 41/100 Batch 59/60 MMD train 0.00021227 \n",
      "Epoch 42/100 Batch 0/60 MMD train 0.00020521 \n",
      "Epoch 42/100 Batch 59/60 MMD train 0.00024083 \n",
      "Epoch 43/100 Batch 0/60 MMD train 0.00024741 \n",
      "Epoch 43/100 Batch 59/60 MMD train 0.00025014 \n",
      "Epoch 44/100 Batch 0/60 MMD train 0.0002488 \n",
      "Epoch 44/100 Batch 59/60 MMD train 0.00024635 \n",
      "Epoch 45/100 Batch 0/60 MMD train 0.00025572 \n",
      "Epoch 45/100 Batch 59/60 MMD train 0.0003375 \n",
      "Epoch 46/100 Batch 0/60 MMD train 0.00033411 \n",
      "Epoch 46/100 Batch 59/60 MMD train 0.00032546 \n",
      "Epoch 47/100 Batch 0/60 MMD train 0.00032526 \n",
      "Epoch 47/100 Batch 59/60 MMD train 0.00033335 \n",
      "Epoch 48/100 Batch 0/60 MMD train 0.00033963 \n",
      "Epoch 48/100 Batch 59/60 MMD train 0.00026344 \n",
      "Epoch 49/100 Batch 0/60 MMD train 0.00026647 \n",
      "Epoch 49/100 Batch 59/60 MMD train 0.00025514 \n",
      "Epoch 50/100 Batch 0/60 MMD train 0.00026002 \n",
      "Epoch 50/100 Batch 59/60 MMD train 0.00027565 \n",
      "Epoch 51/100 Batch 0/60 MMD train 0.00027516 \n",
      "Epoch 51/100 Batch 59/60 MMD train 0.00023747 \n",
      "Epoch 52/100 Batch 0/60 MMD train 0.00023538 \n",
      "Epoch 52/100 Batch 59/60 MMD train 0.00027742 \n",
      "Epoch 53/100 Batch 0/60 MMD train 0.00027941 \n",
      "Epoch 53/100 Batch 59/60 MMD train 0.00026979 \n",
      "Epoch 54/100 Batch 0/60 MMD train 0.00027088 \n",
      "Epoch 54/100 Batch 59/60 MMD train 0.00024735 \n",
      "Epoch 55/100 Batch 0/60 MMD train 0.00025075 \n",
      "Epoch 55/100 Batch 59/60 MMD train 0.00025521 \n",
      "Epoch 56/100 Batch 0/60 MMD train 0.00025783 \n",
      "Epoch 56/100 Batch 59/60 MMD train 0.00025555 \n",
      "Epoch 57/100 Batch 0/60 MMD train 0.00025308 \n",
      "Epoch 57/100 Batch 59/60 MMD train 0.00023536 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 Batch 0/60 MMD train 0.00023797 \n",
      "Epoch 58/100 Batch 59/60 MMD train 0.00025316 \n",
      "Epoch 59/100 Batch 0/60 MMD train 0.00025158 \n",
      "Epoch 59/100 Batch 59/60 MMD train 0.00025215 \n",
      "Epoch 60/100 Batch 0/60 MMD train 0.00025419 \n",
      "Epoch 60/100 Batch 59/60 MMD train 0.00024409 \n",
      "Epoch 61/100 Batch 0/60 MMD train 0.00024915 \n",
      "Epoch 61/100 Batch 59/60 MMD train 0.00027624 \n",
      "Epoch 62/100 Batch 0/60 MMD train 0.00027352 \n",
      "Epoch 62/100 Batch 59/60 MMD train 0.00031383 \n",
      "Epoch 63/100 Batch 0/60 MMD train 0.00031187 \n",
      "Epoch 63/100 Batch 59/60 MMD train 0.00032274 \n",
      "Epoch 64/100 Batch 0/60 MMD train 0.00032508 \n",
      "Epoch 64/100 Batch 59/60 MMD train 0.0003221 \n",
      "Epoch 65/100 Batch 0/60 MMD train 0.00031745 \n",
      "Epoch 65/100 Batch 59/60 MMD train 0.00033205 \n",
      "Epoch 66/100 Batch 0/60 MMD train 0.00033104 \n",
      "Epoch 66/100 Batch 59/60 MMD train 0.00035145 \n",
      "Epoch 67/100 Batch 0/60 MMD train 0.00035112 \n",
      "Epoch 67/100 Batch 59/60 MMD train 0.00033318 \n",
      "Epoch 68/100 Batch 0/60 MMD train 0.00032952 \n",
      "Epoch 68/100 Batch 59/60 MMD train 0.00025327 \n",
      "Epoch 69/100 Batch 0/60 MMD train 0.00025535 \n",
      "Epoch 69/100 Batch 59/60 MMD train 0.0002574 \n",
      "Epoch 70/100 Batch 0/60 MMD train 0.00025785 \n",
      "Epoch 70/100 Batch 59/60 MMD train 0.00025468 \n",
      "Epoch 71/100 Batch 0/60 MMD train 0.00025337 \n",
      "Epoch 71/100 Batch 59/60 MMD train 0.00024816 \n",
      "Epoch 72/100 Batch 0/60 MMD train 0.000247 \n",
      "Epoch 72/100 Batch 59/60 MMD train 0.00030885 \n",
      "Epoch 73/100 Batch 0/60 MMD train 0.00030726 \n",
      "Epoch 73/100 Batch 59/60 MMD train 0.00032427 \n",
      "Epoch 74/100 Batch 0/60 MMD train 0.00032648 \n",
      "Epoch 74/100 Batch 59/60 MMD train 0.00027667 \n",
      "Epoch 75/100 Batch 0/60 MMD train 0.00027948 \n",
      "Epoch 75/100 Batch 59/60 MMD train 0.00030417 \n",
      "Epoch 76/100 Batch 0/60 MMD train 0.00030775 \n",
      "Epoch 76/100 Batch 59/60 MMD train 0.00031442 \n",
      "Epoch 77/100 Batch 0/60 MMD train 0.00031212 \n",
      "Epoch 77/100 Batch 59/60 MMD train 0.00028807 \n",
      "Epoch 78/100 Batch 0/60 MMD train 0.00029069 \n",
      "Epoch 78/100 Batch 59/60 MMD train 0.00029736 \n",
      "Epoch 79/100 Batch 0/60 MMD train 0.00029623 \n",
      "Epoch 79/100 Batch 59/60 MMD train 0.00030955 \n",
      "Epoch 80/100 Batch 0/60 MMD train 0.00030822 \n",
      "Epoch 80/100 Batch 59/60 MMD train 0.00028731 \n",
      "Epoch 81/100 Batch 0/60 MMD train 0.00028676 \n",
      "Epoch 81/100 Batch 59/60 MMD train 0.00031839 \n",
      "Epoch 82/100 Batch 0/60 MMD train 0.00031603 \n",
      "Epoch 82/100 Batch 59/60 MMD train 0.00031565 \n",
      "Epoch 83/100 Batch 0/60 MMD train 0.00031534 \n",
      "Epoch 83/100 Batch 59/60 MMD train 0.00029582 \n",
      "Epoch 84/100 Batch 0/60 MMD train 0.0002944 \n",
      "Epoch 84/100 Batch 59/60 MMD train 0.00032507 \n",
      "Epoch 85/100 Batch 0/60 MMD train 0.0003245 \n",
      "Epoch 85/100 Batch 59/60 MMD train 0.00035425 \n",
      "Epoch 86/100 Batch 0/60 MMD train 0.00035473 \n",
      "Epoch 86/100 Batch 59/60 MMD train 0.00037204 \n",
      "Epoch 87/100 Batch 0/60 MMD train 0.00037019 \n",
      "Epoch 87/100 Batch 59/60 MMD train 0.00032502 \n",
      "Epoch 88/100 Batch 0/60 MMD train 0.00032372 \n",
      "Epoch 88/100 Batch 59/60 MMD train 0.00033213 \n",
      "Epoch 89/100 Batch 0/60 MMD train 0.00033073 \n",
      "Epoch 89/100 Batch 59/60 MMD train 0.00034759 \n",
      "Epoch 90/100 Batch 0/60 MMD train 0.00034875 \n",
      "Epoch 90/100 Batch 59/60 MMD train 0.00034179 \n",
      "Epoch 91/100 Batch 0/60 MMD train 0.0003412 \n",
      "Epoch 91/100 Batch 59/60 MMD train 0.00034456 \n",
      "Epoch 92/100 Batch 0/60 MMD train 0.00033852 \n",
      "Epoch 92/100 Batch 59/60 MMD train 0.00038757 \n",
      "Epoch 93/100 Batch 0/60 MMD train 0.00038978 \n",
      "Epoch 93/100 Batch 59/60 MMD train 0.00037942 \n",
      "Epoch 94/100 Batch 0/60 MMD train 0.0003782 \n",
      "Epoch 94/100 Batch 59/60 MMD train 0.00041178 \n",
      "Epoch 95/100 Batch 0/60 MMD train 0.00040961 \n",
      "Epoch 95/100 Batch 59/60 MMD train 0.00042515 \n",
      "Epoch 96/100 Batch 0/60 MMD train 0.00042555 \n",
      "Epoch 96/100 Batch 59/60 MMD train 0.00040498 \n",
      "Epoch 97/100 Batch 0/60 MMD train 0.00040516 \n",
      "Epoch 97/100 Batch 59/60 MMD train 0.00042452 \n",
      "Epoch 98/100 Batch 0/60 MMD train 0.00043056 \n",
      "Epoch 98/100 Batch 59/60 MMD train 0.00043372 \n",
      "Epoch 99/100 Batch 0/60 MMD train 0.00043105 \n",
      "Epoch 99/100 Batch 59/60 MMD train 0.00043536 \n",
      "Epoch 100/100 Batch 0/60 MMD train 0.00043425 \n",
      "Epoch 100/100 Batch 59/60 MMD train 0.00043798 \n",
      "Corruption 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Epoch 1/100 Batch 0/60 MMD train 0.030861 \n",
      "Epoch 1/100 Batch 59/60 MMD train 0.0027176 \n",
      "Epoch 2/100 Batch 0/60 MMD train 0.0026088 \n",
      "Epoch 2/100 Batch 59/60 MMD train 0.0012944 \n",
      "Epoch 3/100 Batch 0/60 MMD train 0.0015306 \n",
      "Epoch 3/100 Batch 59/60 MMD train 0.00037907 \n",
      "Epoch 4/100 Batch 0/60 MMD train 0.00047007 \n",
      "Epoch 4/100 Batch 59/60 MMD train 0.00081373 \n",
      "Epoch 5/100 Batch 0/60 MMD train 0.00074817 \n",
      "Epoch 5/100 Batch 59/60 MMD train 0.00057801 \n",
      "Epoch 6/100 Batch 0/60 MMD train 0.00055704 \n",
      "Epoch 6/100 Batch 59/60 MMD train 0.00074553 \n",
      "Epoch 7/100 Batch 0/60 MMD train 0.00069679 \n",
      "Epoch 7/100 Batch 59/60 MMD train 0.00054785 \n",
      "Epoch 8/100 Batch 0/60 MMD train 0.0005143 \n",
      "Epoch 8/100 Batch 59/60 MMD train 0.00055845 \n",
      "Epoch 9/100 Batch 0/60 MMD train 0.00052267 \n",
      "Epoch 9/100 Batch 59/60 MMD train 0.00061061 \n",
      "Epoch 10/100 Batch 0/60 MMD train 0.00058354 \n",
      "Epoch 10/100 Batch 59/60 MMD train 0.00050081 \n",
      "Epoch 11/100 Batch 0/60 MMD train 0.00050272 \n",
      "Epoch 11/100 Batch 59/60 MMD train 0.00021856 \n",
      "Epoch 12/100 Batch 0/60 MMD train 0.0002456 \n",
      "Epoch 12/100 Batch 59/60 MMD train 0.00015725 \n",
      "Epoch 13/100 Batch 0/60 MMD train 0.00018406 \n",
      "Epoch 13/100 Batch 59/60 MMD train 0.00019309 \n",
      "Epoch 14/100 Batch 0/60 MMD train 0.00022295 \n",
      "Epoch 14/100 Batch 59/60 MMD train -4.0821e-05 \n",
      "Epoch 15/100 Batch 0/60 MMD train -5.6051e-05 \n",
      "Epoch 15/100 Batch 59/60 MMD train -0.00017155 \n",
      "Epoch 16/100 Batch 0/60 MMD train -0.00018199 \n",
      "Epoch 16/100 Batch 59/60 MMD train -0.00024042 \n",
      "Epoch 17/100 Batch 0/60 MMD train -0.00027243 \n",
      "Epoch 17/100 Batch 59/60 MMD train -0.00020904 \n",
      "Epoch 18/100 Batch 0/60 MMD train -0.0001923 \n",
      "Epoch 18/100 Batch 59/60 MMD train -0.0002111 \n",
      "Epoch 19/100 Batch 0/60 MMD train -0.0002175 \n",
      "Epoch 19/100 Batch 59/60 MMD train -0.00014241 \n",
      "Epoch 20/100 Batch 0/60 MMD train -0.00013296 \n",
      "Epoch 20/100 Batch 59/60 MMD train -0.00013712 \n",
      "Epoch 21/100 Batch 0/60 MMD train -0.00014302 \n",
      "Epoch 21/100 Batch 59/60 MMD train -3.6761e-05 \n",
      "Epoch 22/100 Batch 0/60 MMD train -2.889e-05 \n",
      "Epoch 22/100 Batch 59/60 MMD train -9.0584e-05 \n",
      "Epoch 23/100 Batch 0/60 MMD train -0.00010492 \n",
      "Epoch 23/100 Batch 59/60 MMD train -0.00015878 \n",
      "Epoch 24/100 Batch 0/60 MMD train -0.00014007 \n",
      "Epoch 24/100 Batch 59/60 MMD train -0.00014247 \n",
      "Epoch 25/100 Batch 0/60 MMD train -0.00013134 \n",
      "Epoch 25/100 Batch 59/60 MMD train -0.00012524 \n",
      "Epoch 26/100 Batch 0/60 MMD train -0.00012142 \n",
      "Epoch 26/100 Batch 59/60 MMD train -0.00018585 \n",
      "Epoch 27/100 Batch 0/60 MMD train -0.00021061 \n",
      "Epoch 27/100 Batch 59/60 MMD train -6.0798e-05 \n",
      "Epoch 28/100 Batch 0/60 MMD train -6.6893e-05 \n",
      "Epoch 28/100 Batch 59/60 MMD train -2.8218e-05 \n",
      "Epoch 29/100 Batch 0/60 MMD train -1.5839e-05 \n",
      "Epoch 29/100 Batch 59/60 MMD train -0.00017408 \n",
      "Epoch 30/100 Batch 0/60 MMD train -0.00017868 \n",
      "Epoch 30/100 Batch 59/60 MMD train -0.00015584 \n",
      "Epoch 31/100 Batch 0/60 MMD train -0.00015984 \n",
      "Epoch 31/100 Batch 59/60 MMD train -0.00011656 \n",
      "Epoch 32/100 Batch 0/60 MMD train -0.00011174 \n",
      "Epoch 32/100 Batch 59/60 MMD train -8.3643e-06 \n",
      "Epoch 33/100 Batch 0/60 MMD train -2.3574e-05 \n",
      "Epoch 33/100 Batch 59/60 MMD train -1.5216e-05 \n",
      "Epoch 34/100 Batch 0/60 MMD train -1.8099e-05 \n",
      "Epoch 34/100 Batch 59/60 MMD train -5.1372e-05 \n",
      "Epoch 35/100 Batch 0/60 MMD train -3.8514e-05 \n",
      "Epoch 35/100 Batch 59/60 MMD train -7.3406e-05 \n",
      "Epoch 36/100 Batch 0/60 MMD train -6.6009e-05 \n",
      "Epoch 36/100 Batch 59/60 MMD train -0.00011704 \n",
      "Epoch 37/100 Batch 0/60 MMD train -0.00011939 \n",
      "Epoch 37/100 Batch 59/60 MMD train -0.00016466 \n",
      "Epoch 38/100 Batch 0/60 MMD train -0.00015685 \n",
      "Epoch 38/100 Batch 59/60 MMD train -0.00013683 \n",
      "Epoch 39/100 Batch 0/60 MMD train -0.00014248 \n",
      "Epoch 39/100 Batch 59/60 MMD train -0.00011316 \n",
      "Epoch 40/100 Batch 0/60 MMD train -0.00010412 \n",
      "Epoch 40/100 Batch 59/60 MMD train -6.2205e-06 \n",
      "Epoch 41/100 Batch 0/60 MMD train -6.245e-06 \n",
      "Epoch 41/100 Batch 59/60 MMD train -8.4577e-06 \n",
      "Epoch 42/100 Batch 0/60 MMD train -6.4441e-06 \n",
      "Epoch 42/100 Batch 59/60 MMD train 1.6762e-05 \n",
      "Epoch 43/100 Batch 0/60 MMD train 1.1614e-05 \n",
      "Epoch 43/100 Batch 59/60 MMD train 4.8118e-05 \n",
      "Epoch 44/100 Batch 0/60 MMD train 4.9084e-05 \n",
      "Epoch 44/100 Batch 59/60 MMD train 3.1653e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 Batch 0/60 MMD train 3.6849e-05 \n",
      "Epoch 45/100 Batch 59/60 MMD train -1.2838e-05 \n",
      "Epoch 46/100 Batch 0/60 MMD train -9.4392e-06 \n",
      "Epoch 46/100 Batch 59/60 MMD train 4.7784e-05 \n",
      "Epoch 47/100 Batch 0/60 MMD train 4.2129e-05 \n",
      "Epoch 47/100 Batch 59/60 MMD train 4.4607e-05 \n",
      "Epoch 48/100 Batch 0/60 MMD train 4.8812e-05 \n",
      "Epoch 48/100 Batch 59/60 MMD train 4.9882e-05 \n",
      "Epoch 49/100 Batch 0/60 MMD train 5.2741e-05 \n",
      "Epoch 49/100 Batch 59/60 MMD train -2.1453e-05 \n",
      "Epoch 50/100 Batch 0/60 MMD train -1.6598e-05 \n",
      "Epoch 50/100 Batch 59/60 MMD train -4.904e-05 \n",
      "Epoch 51/100 Batch 0/60 MMD train -6.359e-05 \n",
      "Epoch 51/100 Batch 59/60 MMD train -9.2654e-05 \n",
      "Epoch 52/100 Batch 0/60 MMD train -9.1724e-05 \n",
      "Epoch 52/100 Batch 59/60 MMD train -8.2581e-05 \n",
      "Epoch 53/100 Batch 0/60 MMD train -8.4563e-05 \n",
      "Epoch 53/100 Batch 59/60 MMD train -8.5058e-05 \n",
      "Epoch 54/100 Batch 0/60 MMD train -8.7978e-05 \n",
      "Epoch 54/100 Batch 59/60 MMD train -8.2524e-05 \n",
      "Epoch 55/100 Batch 0/60 MMD train -8.639e-05 \n",
      "Epoch 55/100 Batch 59/60 MMD train -0.00010042 \n",
      "Epoch 56/100 Batch 0/60 MMD train -9.7867e-05 \n",
      "Epoch 56/100 Batch 59/60 MMD train -9.4115e-05 \n",
      "Epoch 57/100 Batch 0/60 MMD train -9.4408e-05 \n",
      "Epoch 57/100 Batch 59/60 MMD train -5.9729e-05 \n",
      "Epoch 58/100 Batch 0/60 MMD train -5.8561e-05 \n",
      "Epoch 58/100 Batch 59/60 MMD train -2.6678e-05 \n",
      "Epoch 59/100 Batch 0/60 MMD train -2.7641e-05 \n",
      "Epoch 59/100 Batch 59/60 MMD train -1.6653e-05 \n",
      "Epoch 60/100 Batch 0/60 MMD train -1.669e-05 \n",
      "Epoch 60/100 Batch 59/60 MMD train -6.8256e-05 \n",
      "Epoch 61/100 Batch 0/60 MMD train -6.4955e-05 \n",
      "Epoch 61/100 Batch 59/60 MMD train -5.0996e-05 \n",
      "Epoch 62/100 Batch 0/60 MMD train -4.9681e-05 \n",
      "Epoch 62/100 Batch 59/60 MMD train -4.1562e-05 \n",
      "Epoch 63/100 Batch 0/60 MMD train -4.2379e-05 \n",
      "Epoch 63/100 Batch 59/60 MMD train -2.7572e-05 \n",
      "Epoch 64/100 Batch 0/60 MMD train -2.5067e-05 \n",
      "Epoch 64/100 Batch 59/60 MMD train -1.1499e-05 \n",
      "Epoch 65/100 Batch 0/60 MMD train -9.1266e-06 \n",
      "Epoch 65/100 Batch 59/60 MMD train 6.9052e-06 \n",
      "Epoch 66/100 Batch 0/60 MMD train 5.4661e-06 \n",
      "Epoch 66/100 Batch 59/60 MMD train 5.4334e-05 \n",
      "Epoch 67/100 Batch 0/60 MMD train 4.5909e-05 \n",
      "Epoch 67/100 Batch 59/60 MMD train -3.0359e-06 \n",
      "Epoch 68/100 Batch 0/60 MMD train -5.724e-07 \n",
      "Epoch 68/100 Batch 59/60 MMD train -3.6753e-05 \n",
      "Epoch 69/100 Batch 0/60 MMD train -4.2306e-05 \n",
      "Epoch 69/100 Batch 59/60 MMD train 1.1489e-05 \n",
      "Epoch 70/100 Batch 0/60 MMD train 1.2859e-05 \n",
      "Epoch 70/100 Batch 59/60 MMD train 2.5433e-05 \n",
      "Epoch 71/100 Batch 0/60 MMD train 2.3825e-05 \n",
      "Epoch 71/100 Batch 59/60 MMD train -2.6376e-05 \n",
      "Epoch 72/100 Batch 0/60 MMD train -2.3425e-05 \n",
      "Epoch 72/100 Batch 59/60 MMD train 8.2117e-06 \n",
      "Epoch 73/100 Batch 0/60 MMD train 1.0848e-05 \n",
      "Epoch 73/100 Batch 59/60 MMD train -9.2532e-06 \n",
      "Epoch 74/100 Batch 0/60 MMD train -7.9456e-06 \n",
      "Epoch 74/100 Batch 59/60 MMD train 7.1184e-06 \n",
      "Epoch 75/100 Batch 0/60 MMD train 5.6637e-06 \n",
      "Epoch 75/100 Batch 59/60 MMD train -1.7628e-05 \n",
      "Epoch 76/100 Batch 0/60 MMD train -1.4338e-05 \n",
      "Epoch 76/100 Batch 59/60 MMD train 4.2226e-05 \n",
      "Epoch 77/100 Batch 0/60 MMD train 4.0903e-05 \n",
      "Epoch 77/100 Batch 59/60 MMD train 1.3569e-05 \n",
      "Epoch 78/100 Batch 0/60 MMD train 9.6595e-06 \n",
      "Epoch 78/100 Batch 59/60 MMD train 2.5996e-05 \n",
      "Epoch 79/100 Batch 0/60 MMD train 2.5011e-05 \n",
      "Epoch 79/100 Batch 59/60 MMD train 1.8606e-05 \n",
      "Epoch 80/100 Batch 0/60 MMD train 1.7762e-05 \n",
      "Epoch 80/100 Batch 59/60 MMD train 1.8807e-05 \n",
      "Epoch 81/100 Batch 0/60 MMD train 1.6836e-05 \n",
      "Epoch 81/100 Batch 59/60 MMD train 1.1402e-05 \n",
      "Epoch 82/100 Batch 0/60 MMD train 1.1529e-05 \n",
      "Epoch 82/100 Batch 59/60 MMD train 1.8722e-05 \n",
      "Epoch 83/100 Batch 0/60 MMD train 1.6826e-05 \n",
      "Epoch 83/100 Batch 59/60 MMD train 3.752e-05 \n",
      "Epoch 84/100 Batch 0/60 MMD train 3.2233e-05 \n",
      "Epoch 84/100 Batch 59/60 MMD train 5.1212e-05 \n",
      "Epoch 85/100 Batch 0/60 MMD train 4.9206e-05 \n",
      "Epoch 85/100 Batch 59/60 MMD train 8.3259e-05 \n",
      "Epoch 86/100 Batch 0/60 MMD train 7.7756e-05 \n",
      "Epoch 86/100 Batch 59/60 MMD train 7.2658e-05 \n",
      "Epoch 87/100 Batch 0/60 MMD train 7.6287e-05 \n",
      "Epoch 87/100 Batch 59/60 MMD train 4.9482e-05 \n",
      "Epoch 88/100 Batch 0/60 MMD train 4.5153e-05 \n",
      "Epoch 88/100 Batch 59/60 MMD train 7.4024e-05 \n",
      "Epoch 89/100 Batch 0/60 MMD train 7.1435e-05 \n",
      "Epoch 89/100 Batch 59/60 MMD train 9.246e-05 \n",
      "Epoch 90/100 Batch 0/60 MMD train 9.1919e-05 \n",
      "Epoch 90/100 Batch 59/60 MMD train 9.2882e-05 \n",
      "Epoch 91/100 Batch 0/60 MMD train 9.5162e-05 \n",
      "Epoch 91/100 Batch 59/60 MMD train 7.6541e-05 \n",
      "Epoch 92/100 Batch 0/60 MMD train 7.7604e-05 \n",
      "Epoch 92/100 Batch 59/60 MMD train 6.6861e-05 \n",
      "Epoch 93/100 Batch 0/60 MMD train 6.6083e-05 \n",
      "Epoch 93/100 Batch 59/60 MMD train 6.1643e-05 \n",
      "Epoch 94/100 Batch 0/60 MMD train 5.6958e-05 \n",
      "Epoch 94/100 Batch 59/60 MMD train 8.2712e-05 \n",
      "Epoch 95/100 Batch 0/60 MMD train 8.2719e-05 \n",
      "Epoch 95/100 Batch 59/60 MMD train 7.8757e-05 \n",
      "Epoch 96/100 Batch 0/60 MMD train 7.5695e-05 \n",
      "Epoch 96/100 Batch 59/60 MMD train 8.6991e-05 \n",
      "Epoch 97/100 Batch 0/60 MMD train 8.6878e-05 \n",
      "Epoch 97/100 Batch 59/60 MMD train 8.5349e-05 \n",
      "Epoch 98/100 Batch 0/60 MMD train 8.4839e-05 \n",
      "Epoch 98/100 Batch 59/60 MMD train 4.7325e-05 \n",
      "Epoch 99/100 Batch 0/60 MMD train 5.038e-05 \n",
      "Epoch 99/100 Batch 59/60 MMD train 7.2994e-05 \n",
      "Epoch 100/100 Batch 0/60 MMD train 7.1326e-05 \n",
      "Epoch 100/100 Batch 59/60 MMD train 0.0001034 \n"
     ]
    }
   ],
   "source": [
    "all_mmds_train, all_mmds_test = compute_mmd(corruptions, joint_kernel, epochs=MMD_Epochs, reference=True)\n",
    "data = {'train': all_mmds_train,\n",
    "       'test': all_mmds_test,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_mmd.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_mmds_train, all_mmds_test = compute_mmd(corruptions, joint_kernel, epochs=10, reference=True)\n",
    "data = {'train': all_mmds_train,\n",
    "       'test': all_mmds_test,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_mmd.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_mmds_train, all_mmds_test = compute_mmd(corruptions, joint_kernel, epochs=1, reference=False)\n",
    "data = {'train': all_mmds_train,\n",
    "       'test': all_mmds_test,\n",
    "       'corruptions': corruptions}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy(dataset):    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, (x, y) in enumerate(dataset):\n",
    "        X.append(x.numpy().flatten())\n",
    "        if not isinstance(y, int):\n",
    "            y = y.item()\n",
    "        Y.append(y)\n",
    "    \n",
    "        if i % 10000 ==0 :\n",
    "            print ('{}/{}'.format(i, len(dataset)))\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    # shuffle\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    # make contiguous\n",
    "    X = X[idx].copy()\n",
    "    Y = Y[idx].copy()\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_regularized_logloss(Y_prob, Y_test, regularize=0.01):\n",
    "    uniform = np.ones((len(Y_prob), 10)) / 10.\n",
    "    Y_regularized = regularize*uniform+(1-regularize)*Y_prob\n",
    "    logloss = -np.log(Y_regularized)[range(len(Y_test)), Y_test].mean()\n",
    "    #print (Y_regularized.sum(1))\n",
    "    return logloss\n",
    "\n",
    "    \n",
    "def compute_rf(corruptions, reference=False):\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    for corruption in corruptions:\n",
    "\n",
    "        print ('Corruption {}'.format(corruption))\n",
    "\n",
    "        if not reference:\n",
    "            train_corrupted = CorruptedDataset(train_dataset, corruption)  # X partially dependent on Y\n",
    "            test_corrupted = CorruptedDataset(test_dataset, corruption)\n",
    "        else:\n",
    "            train_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)  # X partially dependent on Y\n",
    "            test_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)\n",
    "        \n",
    "        X_train, Y_train = dataset_to_numpy(train_corrupted)\n",
    "        X_test, Y_test = dataset_to_numpy(test_corrupted)\n",
    "\n",
    "        print ('Fitting random forest')\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        Y_pred = clf.predict(X_test)\n",
    "        print ('accuracy', (Y_pred == Y_test).mean())\n",
    "\n",
    "        # Log loss train\n",
    "        Y_prob_train = clf.predict_proba(X_train)\n",
    "        logloss_train = get_regularized_logloss(Y_prob_train, Y_train)\n",
    "\n",
    "        # Log loss test\n",
    "        Y_prob_test = clf.predict_proba(X_test)\n",
    "        logloss_test = get_regularized_logloss(Y_prob_test, Y_test)\n",
    "\n",
    "        all_test_losses.append(logloss_test)\n",
    "        all_train_losses.append(logloss_train)\n",
    "\n",
    "        print ('Log loss Train {}'.format(logloss_train))\n",
    "        print ('Log loss Test {}'.format(logloss_test))\n",
    "\n",
    "    return all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/15950\n",
      "10000/15950\n",
      "Fitting random forest\n",
      "accuracy 0.49454545454545457\n",
      "Log loss Train 0.313459950678\n",
      "Log loss Test 1.92962339132\n",
      "Corruption 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/15950\n",
      "10000/15950\n",
      "Fitting random forest\n",
      "accuracy 0.3220689655172414\n",
      "Log loss Train 0.372374851264\n",
      "Log loss Test 2.70305580786\n",
      "Corruption 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/15950\n",
      "10000/15950\n",
      "Fitting random forest\n",
      "accuracy 0.19247648902821315\n",
      "Log loss Train 0.41349973893\n",
      "Log loss Test 3.24708106856\n",
      "Corruption 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/15950\n",
      "10000/15950\n",
      "Fitting random forest\n",
      "accuracy 0.11786833855799372\n",
      "Log loss Train 0.432764708943\n",
      "Log loss Test 3.61319149901\n",
      "Corruption 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/15950\n",
      "10000/15950\n",
      "Fitting random forest\n",
      "accuracy 0.10056426332288401\n",
      "Log loss Train 0.438417166089\n",
      "Log loss Test 3.69635091171\n"
     ]
    }
   ],
   "source": [
    "all_train_losses, all_test_losses = compute_rf(corruptions, reference=False)\n",
    "\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_real_rf.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "Fitting random forest\n",
      "accuracy 1.0\n",
      "Log loss Train 0.00904074465215\n",
      "Log loss Test 0.00904074465215\n",
      "Corruption 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "Fitting random forest\n",
      "accuracy 0.7762333333333333\n",
      "Log loss Train 1.01787518281\n",
      "Log loss Test 1.02411239671\n",
      "Corruption 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "Fitting random forest\n",
      "accuracy 0.5476666666666666\n",
      "Log loss Train 1.67875444355\n",
      "Log loss Test 1.68302501484\n",
      "Corruption 0.75\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "Fitting random forest\n",
      "accuracy 0.33015\n",
      "Log loss Train 2.11194652128\n",
      "Log loss Test 2.10694463385\n",
      "Corruption 1.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "0/60000\n",
      "10000/60000\n",
      "20000/60000\n",
      "30000/60000\n",
      "40000/60000\n",
      "50000/60000\n",
      "Fitting random forest\n",
      "accuracy 0.09983333333333333\n",
      "Log loss Train 2.30193214802\n",
      "Log loss Test 2.30349361231\n"
     ]
    }
   ],
   "source": [
    "all_train_losses, all_test_losses = compute_rf(corruptions, reference=True)\n",
    "\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_test_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_rf.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import squareform\n",
    "import scipy\n",
    "\n",
    "\n",
    "def log_sum_exp(u, dim):\n",
    "    # Reduce log sum exp along axis\n",
    "    u_max, __ = u.max(dim=dim, keepdim=True)\n",
    "    log_sum_exp_u = torch.log(torch.exp(u - u_max).sum(dim)) + u_max.sum(dim)\n",
    "    return log_sum_exp_u\n",
    "\n",
    "\n",
    "def compute_sinkhorn_stable(m, r=None, c=None, log_v=None, regularization=100., iterations=40):\n",
    "    # If no distributions are given, consider two uniform histograms\n",
    "    if r is None:\n",
    "        r = torch.ones(m.size()[0]).to(m.device) / m.size()[0]\n",
    "    if c is None:\n",
    "        c = torch.ones(m.size()[1]).to(m.device) / m.size()[1]\n",
    "    log_r = torch.log(r)\n",
    "    log_c = torch.log(c)\n",
    "\n",
    "    # Initialize dual variable v (u is implicitly defined in the loop)\n",
    "    if log_v is None:\n",
    "        log_v = torch.zeros(m.size()[1]).to(m.device)  # ==torch.log(torch.ones(m.size()[1]))\n",
    "\n",
    "    # Exponentiate the pairwise distance matrix\n",
    "    log_K = -regularization * m\n",
    "\n",
    "    # Main loop\n",
    "    for i in xrange(iterations):\n",
    "        # Match r marginals\n",
    "        log_u = log_r - log_sum_exp(log_K + log_v[None, :], dim=1)\n",
    "\n",
    "        # Match c marginals\n",
    "        log_v = log_c - log_sum_exp(log_u[:, None] + log_K, dim=0)\n",
    "\n",
    "    # Compute optimal plan, cost, return everything\n",
    "    log_P = log_u[:, None] + log_K + log_v[None, :]\n",
    "    P = torch.exp(log_P)  # transport plan\n",
    "    dst = (P * m).sum()\n",
    "\n",
    "    return dst, P, log_P, log_u, log_v\n",
    "\n",
    "\n",
    "def compute_wasserstein(corruptions, reference=False, N=1000):\n",
    "\n",
    "    all_train_losses = []\n",
    "\n",
    "    for corruption in corruptions:\n",
    "        \n",
    "        print ('Corruption {}'.format(corruption))\n",
    "        \n",
    "        if not reference:\n",
    "            \n",
    "            # IMPORTANT FOR SVHN : DO NOT SPECIFY CLASSES=10 BECAUSE\n",
    "            # LABEL DISTRIBUTION IS NON-UNIFORM            \n",
    "            train_corrupted = CorruptedDataset(train_dataset, corruption)  # X partially dependent on Y\n",
    "            train_random = CorruptedDataset(train_dataset, 1)  # X totally independent Y\n",
    "        else:\n",
    "            train_corrupted = CorruptedDataset(ReferenceDataset(args.dataset), corruption)  # X partially dependent on Y\n",
    "            train_random = CorruptedDataset(ReferenceDataset(args.dataset), 1)  # X totally independent Y            \n",
    "            \n",
    "        # Weight matrix\n",
    "        X_train, Y_train = dataset_to_numpy(train_corrupted)\n",
    "        X_random, Y_random = dataset_to_numpy(train_random)\n",
    "        #X_test, Y_test = dataset_to_numpy(test_corrupted)\n",
    "\n",
    "        # limit the number of points\n",
    "        X_train = X_train[:N]\n",
    "        X_random = X_random[:N]\n",
    "        Y_train = Y_train[:N]\n",
    "        Y_random = Y_random[:N]\n",
    "\n",
    "        xdst = cdist(X_train, X_random) / 28.\n",
    "        ydst = (Y_train[:, None] != Y_random[None, :]).astype(float)\n",
    "        totaldst = xdst + ydst\n",
    "\n",
    "        print ('Computing Sinkhorn')\n",
    "        \n",
    "        totaldst = torch.from_numpy(totaldst).float()\n",
    "        dst, P, log_P, log_u, log_v = compute_sinkhorn_stable(totaldst, regularization=100.)\n",
    "        \n",
    "        all_train_losses.append(dst.item())\n",
    "        \n",
    "        print ('dst {:.3f}'.format(dst.item()))\n",
    "        \n",
    "    return all_train_losses\n",
    "\n",
    "\n",
    "Wasserstein_N = 6000 # limit to those examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption 0.0\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "Computing Sinkhorn\n",
      "dst 0.267\n",
      "Corruption 0.25\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "Computing Sinkhorn\n",
      "dst 0.264\n",
      "Corruption 0.5\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "Corrupting dataset (SHUFFLE labels, not always uniform)\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n",
      "0/46590\n",
      "10000/46590\n",
      "20000/46590\n",
      "30000/46590\n",
      "40000/46590\n"
     ]
    }
   ],
   "source": [
    "all_train_losses = compute_wasserstein(corruptions, reference=False, N=Wasserstein_N)\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_train_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_real_wasserstein.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_train_losses = compute_wasserstein(corruptions, reference=True, N=Wasserstein_N)\n",
    "data = {'train': all_train_losses,\n",
    "       'test': all_train_losses,\n",
    "       'corruptions': corruptions}\n",
    "torch.save(data, '{}_reference_wasserstein.pt'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot All MNIST/SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, label, color, marker=None, **kwargs):\n",
    "    train_mi = np.log(10) - np.asarray(data['train'])\n",
    "    test_mi = np.log(10) - np.asarray(data['test'])\n",
    "    plt.plot(data['corruptions'], np.maximum(train_mi, 0),\n",
    "             label='{} (train)'.format(label), color=color, \n",
    "             alpha=0.2, linewidth=3, markersize=15, marker='v', **kwargs)\n",
    "    plt.plot(data['corruptions'], np.maximum(test_mi, 0), \n",
    "             label='{} (test)'.format(label), color=color, \n",
    "             alpha=0.5, linewidth=3, markersize=15, marker='^', **kwargs)\n",
    "    \n",
    "def plot_data2(data, label, color, scale=1., marker=None, **kwargs):\n",
    "    plt.plot(data['corruptions'], scale*np.maximum(data['train'], 0),\n",
    "             label='{} (train)'.format(label), markersize=15, color=color, \n",
    "             marker='o', alpha=0.5, linewidth=3, **kwargs)#, ls='dashed')    \n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "MMD_max = np.max(torch.load('{}_reference_mmd.pt'.format(args.dataset))['train'])\n",
    "MMD_scaling = np.log(10) / MMD_max\n",
    "Wasserstein_max = np.max(torch.load('{}_reference_wasserstein.pt'.format(args.dataset))['train'])\n",
    "Wasserstein_scaling = np.log(10) / Wasserstein_max\n",
    "    \n",
    "plt.subplot(121)\n",
    "    \n",
    "\n",
    "# Reference population and empirical\n",
    "alpha = np.linspace(0, 1, 100)\n",
    "k = 10\n",
    "true_mi = corruption_to_mi(alpha)\n",
    "empirical_mi = np.ones_like(alpha)*np.log(10)\n",
    "plt.plot(alpha, true_mi, label='True MI', color='xkcd:black', linewidth=3)\n",
    "plt.plot(alpha,empirical_mi, label='Empirical MI', color='xkcd:red', linewidth=3)\n",
    "    \n",
    "plot_data(torch.load('{}_reference_logistic.pt'.format(args.dataset)), 'Logistic', 'xkcd:green', marker='s')\n",
    "plot_data(torch.load('{}_reference_cnn.pt'.format(args.dataset)), 'CNN', 'xkcd:orange', marker=10)\n",
    "plot_data(torch.load('{}_reference_rf.pt'.format(args.dataset)), 'Random Forest', 'xkcd:blue', marker=11)\n",
    "plot_data2(torch.load('{}_reference_mmd.pt'.format(args.dataset)), 'MMD', 'xkcd:purple', MMD_scaling, marker='o')\n",
    "plot_data2(torch.load('{}_reference_wasserstein.pt'.format(args.dataset)), 'Wasserstein', 'xkcd:hot pink', Wasserstein_scaling, marker='o')\n",
    "    \n",
    "    \n",
    "plt.title('Reference Easy problem')\n",
    "plt.xlabel('Ratio of corrupted labels')\n",
    "plt.ylabel('Mutual information (nats)')\n",
    "plt.ylim(-0.2, 2.5)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "\n",
    "# Reference population and empirical\n",
    "alpha = np.linspace(0, 1, 100)\n",
    "k = 10\n",
    "true_mi = corruption_to_mi(alpha)\n",
    "empirical_mi = np.ones_like(alpha)*np.log(10)\n",
    "plt.plot(alpha, true_mi, label='True MI', color='xkcd:black', linewidth=3)\n",
    "plt.plot(alpha,empirical_mi, label='Empirical MI', color='xkcd:red', linewidth=3)\n",
    "    \n",
    "plot_data(torch.load('{}_real_logistic.pt'.format(args.dataset)), 'Logistic', 'xkcd:green', marker='s')\n",
    "plot_data(torch.load('{}_real_cnn.pt'.format(args.dataset)), 'CNN', 'xkcd:orange', marker=10)\n",
    "plot_data(torch.load('{}_real_rf.pt'.format(args.dataset)), 'Random Forest', 'xkcd:blue', marker=11)\n",
    "plot_data2(torch.load('{}_real_mmd.pt'.format(args.dataset)), 'MMD', 'xkcd:purple', MMD_scaling, marker='o')\n",
    "plot_data2(torch.load('{}_real_wasserstein.pt'.format(args.dataset)), 'Wasserstein', 'xkcd:hot pink', Wasserstein_scaling, marker='o')\n",
    "    \n",
    "\n",
    "plt.title('Noisy-Label {}'.format(args.dataset))\n",
    "plt.xlabel('Ratio of corrupted labels')\n",
    "plt.ylabel('Mutual information (nats)')\n",
    "plt.ylim(-0.2, 2.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('pmi_{}.pdf'.format(args.dataset))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
